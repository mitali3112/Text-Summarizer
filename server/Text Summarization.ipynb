{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Using OCR and Attention Networks\n",
    "## Final Year Project (2018-2019)\n",
    "### Guide: Mrs. Sneha H R\n",
    "### Mentor: Dr. Prasanta Gogoi\n",
    "### Members: Dhwani D Shah,Mitali Sandip Sheth,Nitisha Yadav, Pooja Pandit\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "\n",
    "import Summarizer\n",
    "import summarizer_data_utils\n",
    "import summarizer_model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "The data we will use here is the 'all-the-news'-dataset from Kaggle. It contains about 200000 news articles and the headlines of those articles.The articles are from several big news corporations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/mitali/toi.csv',\n",
    "                   encoding='utf-8')\n",
    "data1 = pd.read_csv('/home/mitali/articles2.csv',\n",
    "                    encoding='utf-8')\n",
    "data2 = pd.read_csv('/home/mitali/articles3.csv',\n",
    "                    encoding='utf-8')\n",
    "data3 = pd.read_csv('/home/mitali/articles1.csv',\n",
    "                    encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(143846, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, data1, data2,data3])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNEW DELHI: In a move to aid the real estate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nGST Council meet: GST council approves trans...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNEW DELHI: Infosys Ltd, India's second-bigge...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nABN Amro: Infosys to buy 75% stake in ABN AM...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNEW DELHI: Lyft Inc on Wednesday raised the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nLyft raises IPO price target as investor fre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNEW DELHI: On Friday, diamantaire Nirav Modi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n‘A golden visa can save you from going to ja...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNEW DELHI: \\n\\n  Jet Airways\\n + has started...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nJet Airways News: Planes grounded, Jet Airwa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 author                                            content date  \\\n",
       "0         NaN    NaN  \\nNEW DELHI: In a move to aid the real estate ...  NaN   \n",
       "1         NaN    NaN  \\nNEW DELHI: Infosys Ltd, India's second-bigge...  NaN   \n",
       "2         NaN    NaN  \\nNEW DELHI: Lyft Inc on Wednesday raised the ...  NaN   \n",
       "3         NaN    NaN  \\nNEW DELHI: On Friday, diamantaire Nirav Modi...  NaN   \n",
       "4         NaN    NaN  \\nNEW DELHI: \\n\\n  Jet Airways\\n + has started...  NaN   \n",
       "\n",
       "   id  month publication                                              title  \\\n",
       "0 NaN    NaN         NaN  \\nGST Council meet: GST council approves trans...   \n",
       "1 NaN    NaN         NaN  \\nABN Amro: Infosys to buy 75% stake in ABN AM...   \n",
       "2 NaN    NaN         NaN  \\nLyft raises IPO price target as investor fre...   \n",
       "3 NaN    NaN         NaN  \\n‘A golden visa can save you from going to ja...   \n",
       "4 NaN    NaN         NaN  \\nJet Airways News: Planes grounded, Jet Airwa...   \n",
       "\n",
       "   url  year  \n",
       "0  NaN   NaN  \n",
       "1  NaN   NaN  \n",
       "2  NaN   NaN  \n",
       "3  NaN   NaN  \n",
       "4  NaN   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Atlantic': 7179,\n",
       "         'Breitbart': 23781,\n",
       "         'Business Insider': 6757,\n",
       "         'Buzzfeed News': 4854,\n",
       "         'CNN': 11488,\n",
       "         'Fox News': 4354,\n",
       "         'Guardian': 8681,\n",
       "         'NPR': 11992,\n",
       "         'National Review': 6203,\n",
       "         'New York Post': 17493,\n",
       "         'New York Times': 7803,\n",
       "         'Reuters': 10710,\n",
       "         'Talking Points Memo': 5214,\n",
       "         'Vox': 4947,\n",
       "         'Washington Post': 11114,\n",
       "         nan: 1276})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data.publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.publication != 'Breitbart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      1276\n",
       "author         17152\n",
       "content          119\n",
       "date            3917\n",
       "id              1276\n",
       "month           3917\n",
       "publication     1276\n",
       "title              2\n",
       "url            34506\n",
       "year            3917\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120065"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \\nGST Council meet: GST council approves trans...\n",
       "0    Patriots Day Is Best When It Digs Past the Her...\n",
       "0    Alton Sterling’s son: ’Everyone needs to prote...\n",
       "0    House Republicans Fret About Winning Their Hea...\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['title'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(index = str, columns = {'title':'Headline', 'content':'Story'}, inplace = True)\n",
    "data = data[['Headline', 'Story']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GST Council meet: GST council approves transition plan for new tax rates for housing sector - Times of India\n",
      "\n",
      "\n",
      "\n",
      "ABN Amro: Infosys to buy 75% stake in ABN AMRO's mortgage services arm - Times of India\n",
      "\n",
      "\n",
      "\n",
      "Lyft raises IPO price target as investor fret over missing out - Times of India\n",
      "\n",
      "\n",
      "\n",
      "‘A golden visa can save you from going to jail at home’ - Times of India\n",
      "\n",
      "\n",
      "\n",
      "Jet Airways News: Planes grounded, Jet Airways sends expat pilots on leave without pay \n",
      "\n",
      "\n",
      "\n",
      "Asian central banks look to cut rates after Fed pause - Times of India\n",
      "\n",
      "\n",
      "\n",
      "Heineken buys 74L shares of UBHL from ED - Times of India\n",
      "\n",
      "\n",
      "\n",
      "Tata Group: Tatas, GIC & SSG to buy stake in GMR Airports by investing Rs 8,000 crore \n",
      "\n",
      "\n",
      "\n",
      "Uber Careem Merger: Uber to buy Mideast rival Careem for $3.1 billion - Times of India\n",
      "\n",
      "\n",
      "\n",
      "Talegaon - The Green Heaven for your Dream Home - Times of India\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in data.Headline[:10]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_headline = [len(headline) for i, headline in enumerate(data.Headline)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_story=[]\n",
    "for i,story in enumerate(data.Story):\n",
    "    if isinstance(story,str):\n",
    "        l=len(story)\n",
    "        len_story.append((i,l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119944, 120063)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(len_story),len(len_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120063, 119944, 119)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = set(i for i in range(len(len_story)))\n",
    "indexh= set(i for i in range(len(len_headline)))\n",
    "indexd=indexh-index\n",
    "len(indexh),len(index),len(indexd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([111, 90, 82, 75, 90, 73, 60, 89, 89, 67],\n",
       " [(0, 2663),\n",
       "  (1, 783),\n",
       "  (2, 2826),\n",
       "  (3, 5730),\n",
       "  (4, 3105),\n",
       "  (5, 1898),\n",
       "  (6, 1266),\n",
       "  (7, 3640),\n",
       "  (8, 1426),\n",
       "  (9, 4080)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_headline[:10], len_story[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_headline= [val for ind, val in enumerate(len_headline) if ind not in indexd]\n",
    "len_story= [val for ind, val in enumerate(len_story) if ind not in indexd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119944, 119944)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(len_headline),len(len_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([111, 90, 82, 75, 90, 73, 60, 89, 89, 67],\n",
       " [(0, 2663),\n",
       "  (1, 783),\n",
       "  (2, 2826),\n",
       "  (3, 5730),\n",
       "  (4, 3105),\n",
       "  (5, 1898),\n",
       "  (6, 1266),\n",
       "  (7, 3640),\n",
       "  (8, 1426),\n",
       "  (9, 4080)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_headline[:10], len_story[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(64, 2894),\n",
       "  (63, 2893),\n",
       "  (60, 2890),\n",
       "  (59, 2864),\n",
       "  (62, 2847),\n",
       "  (58, 2838),\n",
       "  (61, 2789),\n",
       "  (65, 2784),\n",
       "  (66, 2673),\n",
       "  (67, 2651)],\n",
       " [((0, 2663), 1),\n",
       "  ((1, 783), 1),\n",
       "  ((2, 2826), 1),\n",
       "  ((3, 5730), 1),\n",
       "  ((4, 3105), 1),\n",
       "  ((5, 1898), 1),\n",
       "  ((6, 1266), 1),\n",
       "  ((7, 3640), 1),\n",
       "  ((8, 1426), 1),\n",
       "  ((9, 4080), 1)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_headline_counted = Counter(len_headline).most_common()\n",
    "len_story_counted = Counter(len_story).most_common()\n",
    "len_data= len(data.Headline)\n",
    "len_headline_counted[:10], len_story_counted[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dataset\n",
    "Taking the articles whose content length is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [ind for ind, val in len_story if 50 < val < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 11, 21, 45, 47, 48, 70, 75, 76, 77], 7736)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:10],len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=set(i for i in range(len_data))\n",
    "l2=set(indices)\n",
    "u_id=list(l1-l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_unprocessed= [text for ind, text in enumerate(data.Story) if ind not in u_id]\n",
    "summaries_unprocessed= [text for ind, text in enumerate(data.Headline) if ind not in u_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7736, 112327, 7736)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts_unprocessed),len(u_id),len(summaries_unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "to_remove = ['- The New York Times', '- Breitbart','- The Times Of India']\n",
    "\n",
    "summaries_unprocessed_clean = []\n",
    "texts_unprocessed_clean = []\n",
    "\n",
    "removed = 0\n",
    "append = True\n",
    "for sentence in summaries_unprocessed:\n",
    "    append = True\n",
    "    for r in to_remove:\n",
    "        if sentence.endswith(r):\n",
    "            sentence = sentence.replace(r, '.')\n",
    "            summaries_unprocessed_clean.append(sentence.replace(r, '.'))\n",
    "            removed+=1\n",
    "            append = False\n",
    "            break\n",
    "            \n",
    "    if append:\n",
    "        summaries_unprocessed_clean.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7736, 7736)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries_unprocessed_clean), len(texts_unprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Time:  11.464321374893188\n"
     ]
    }
   ],
   "source": [
    "processed_texts, processed_summaries, words_counted = summarizer_data_utils.preprocess_texts_and_summaries(\n",
    "    texts_unprocessed,\n",
    "    summaries_unprocessed_clean,\n",
    "    keep_most=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts_clean = []\n",
    "processed_summaries_clean = []\n",
    "\n",
    "for t, s in zip(processed_texts, processed_summaries):\n",
    "    if t != [] and s != []:\n",
    "        processed_texts_clean.append(t)\n",
    "        processed_summaries_clean.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create lookup dictionaries\n",
    "We cannot feed our network actual words, but numbers. So we first have to create our lookup dictionaries, where each word gets an int value (high or low, depending on its frequency in our corpus). Those help us to later convert the texts into numbers.\n",
    "\n",
    "We also add special tokens. EndOfSentence(EOS) and StartOfSentence(SOS) are crucial for the Seq2Seq model we later use. Pad token(PAD), because all summaries and texts in a batch need to have the same length, pad token helps us do that.\n",
    "\n",
    "Unknown Token(UNK) refers to certain words that might be proper nouns and are not frequently used.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23168 23168 14685\n"
     ]
    }
   ],
   "source": [
    "specials = [\"<EOS>\", \"<SOS>\",\"<PAD>\",\"<UNK>\"]\n",
    "word2ind, ind2word,  missing_words = summarizer_data_utils.create_word_inds_dicts(words_counted,\n",
    "                                                                                  specials = specials,\n",
    "                                                                                  min_occurences = 2)\n",
    "print(len(word2ind), len(ind2word), len(missing_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Embeddings\n",
    "We can use pretrained word embeddings as they have proved to increase training speed and accuracy. Here I have used the embeddings from tf_hub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0402 15:18:29.574964 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0402 15:18:29.782527 140079934850880 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed = hub.Module(\"https://tfhub.dev/google/Wiki-words-250/1\")\n",
    "emb = embed([key for key in word2ind.keys()])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    embedding = sess.run(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23168, 250)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./tf_hub_embedding_headlinesA.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Text and Summaries\n",
    "As we cannot feed the words directly to our network, we have to convert them to numbers first. So we use the function to convert them to integer values. And we also append the SOS and EOS tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts words in texts and summaries to indices\n",
    "converted_texts, unknown_words_in_texts = summarizer_data_utils.convert_to_inds(processed_texts_clean,\n",
    "                                                                                word2ind,\n",
    "                                                                                eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_summaries, unknown_words_in_summaries = summarizer_data_utils.convert_to_inds(processed_summaries_clean,\n",
    "                                                                                        word2ind,\n",
    "                                                                                        eos = True,\n",
    "                                                                                        sos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'delhi', 'infosys', 'ltd', 'india', 's', 'second', 'biggest', 'it', 'services', 'company', 'said', 'on', 'thursday', 'it', 'would', 'buy', 'a', '75', 'per', 'cent', 'stake', 'in', 'abn', 'amro', 'group', 'nv', 's', 'mortgage', 'administration', 'services', 'unit', 'for', '127', '5', 'million', 'euros', '143', '53', 'million', 'infosys', 'will', 'acquire', 'the', 'stake', 'in', 'stater', 'nv', 'through', 'unit', 'infosys', 'consulting', 'pvt', 'ltd', 'and', 'the', 'transaction', 'is', 'expected', 'to', 'close', 'in', 'the', 'first', 'quarter', 'of', 'fiscal', '2020', 'this', 'is', 'in', 'line', 'with', 'infosys', 'strategy', 'to', 'strengthen', 'its', 'mortgage', 'servicing', 'capabilities', 'in', 'continental', 'europe', 'the', 'software', 'services', 'company', 'said', 'in', 'a', 'statement', 'abn', 'amro', 'will', 'continue', 'to', 'hold', 'the', 'remaining', '25', 'per', 'cent', 'stake', 'in', 'stater', 'which', 'operates', 'in', 'the', 'netherlands', 'belgium', 'and', 'germany', 'infosys', 'had', 'gained', '1', '6', 'per', 'cent', 'by', '12', '58', 'pm', 'while', 'the', 'broader', 'mumbai', 'market', 'was', 'up', '0', '58', 'per', 'cent']\n"
     ]
    }
   ],
   "source": [
    "print(processed_texts_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 1617, 4257, 5391, 999, 10, 296, 583, 18, 889, 112, 21, 11, 105, 18, 76, 907, 5, 1898, 842, 4402, 1424, 9, 6276, 6277, 228, 8838, 10, 4739, 434, 889, 958, 13, 12367, 199, 127, 5923, 12368, 2703, 127, 4257, 41, 5652, 4, 1424, 9, 9705, 8838, 186, 958, 4257, 5392, 14424, 5391, 8, 4, 5159, 14, 418, 6, 455, 9, 4, 68, 873, 7, 5393, 2591, 28, 14, 9, 471, 16, 4257, 1755, 6, 6635, 49, 4739, 12369, 6636, 9, 5394, 1618, 4, 2346, 889, 112, 21, 9, 5, 182, 6276, 6277, 41, 734, 6, 760, 4, 2471, 570, 842, 4402, 1424, 9, 9705, 60, 4740, 9, 4, 7086, 4103, 8, 1756, 4257, 51, 3326, 113, 334, 842, 4402, 26, 394, 3327, 561, 108, 4, 5653, 2756, 312, 15, 54, 1458, 3327, 842, 4402]\n"
     ]
    }
   ],
   "source": [
    "print(converted_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'delhi', 'infosys', 'ltd', 'india', 's', 'second', 'biggest', 'it', 'services', 'company', 'said', 'on', 'thursday', 'it', 'would', 'buy', 'a', '75', 'per', 'cent', 'stake', 'in', 'abn', 'amro', 'group', 'nv', 's', 'mortgage', 'administration', 'services', 'unit', 'for', '127', '5', 'million', 'euros', '143', '53', 'million', 'infosys', 'will', 'acquire', 'the', 'stake', 'in', 'stater', 'nv', 'through', 'unit', 'infosys', 'consulting', 'pvt', 'ltd', 'and', 'the', 'transaction', 'is', 'expected', 'to', 'close', 'in', 'the', 'first', 'quarter', 'of', 'fiscal', '2020', 'this', 'is', 'in', 'line', 'with', 'infosys', 'strategy', 'to', 'strengthen', 'its', 'mortgage', 'servicing', 'capabilities', 'in', 'continental', 'europe', 'the', 'software', 'services', 'company', 'said', 'in', 'a', 'statement', 'abn', 'amro', 'will', 'continue', 'to', 'hold', 'the', 'remaining', '25', 'per', 'cent', 'stake', 'in', 'stater', 'which', 'operates', 'in', 'the', 'netherlands', 'belgium', 'and', 'germany', 'infosys', 'had', 'gained', '1', '6', 'per', 'cent', 'by', '12', '58', 'pm', 'while', 'the', 'broader', 'mumbai', 'market', 'was', 'up', '0', '58', 'per', 'cent']\n",
      "['<SOS>', 'abn', 'amro', 'infosys', 'to', 'buy', '75', 'stake', 'in', 'abn', 'amro', 's', 'mortgage', 'services', 'arm', 'times', 'of', 'india', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# seems to have worked well. \n",
    "print(summarizer_data_utils.convert_inds_to_text(converted_texts[0], ind2word))\n",
    "print(summarizer_data_utils.convert_inds_to_text(converted_summaries[0], ind2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "Now we can build and train our model. First we define the hyperparameters we want to use. Then we create our Summarizer and call the function .build_graph(), which as the name suggests, builds the computation graph. Then we can train the model using .train()\n",
    "\n",
    "After training we can try our model using .infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters\n",
    "num_layers_encoder = 4\n",
    "num_layers_decoder = 4\n",
    "rnn_size_encoder = 300\n",
    "rnn_size_decoder = 300\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "clip = 5\n",
    "keep_probability = 0.8\n",
    "learning_rate = 0.0005\n",
    "max_lr=0.005\n",
    "learning_rate_decay_steps = 100\n",
    "learning_rate_decay = 0.90\n",
    "\n",
    "\n",
    "pretrained_embeddings_path = './tf_hub_embedding_headlinesA.npy'\n",
    "summary_dir = os.path.join('./tensorboard/modelA')\n",
    "\n",
    "use_cyclic_lr = True\n",
    "inference_targets=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained embeddings.\n",
      "WARNING:tensorflow:From /home/mitali/Summarizer.py:162: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:29.742573 140079934850880 deprecation.py:323] From /home/mitali/Summarizer.py:162: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mitali/Summarizer.py:259: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:29.754791 140079934850880 deprecation.py:323] From /home/mitali/Summarizer.py:259: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:29.756946 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:29.804081 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:29.908169 140079934850880 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mitali/Summarizer.py:388: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:31.095655 140079934850880 deprecation.py:323] From /home/mitali/Summarizer.py:388: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:311: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:37.555489 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:311: Bernoulli.__init__ (from tensorflow.python.ops.distributions.bernoulli) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:37.564748 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:97: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:314: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:37.598766 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/helper.py:314: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0402 15:19:37.610558 140079934850880 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built.\n",
      "-------------------- Epoch 0 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 10.0505\n",
      "Iteration: 2 of 241\ttrain_loss: 10.0464\n",
      "Iteration: 4 of 241\ttrain_loss: 10.0355\n",
      "Iteration: 6 of 241\ttrain_loss: 9.9627\n",
      "Iteration: 8 of 241\ttrain_loss: 9.6413\n",
      "Iteration: 10 of 241\ttrain_loss: 9.1372\n",
      "Iteration: 12 of 241\ttrain_loss: 8.5367\n",
      "Iteration: 14 of 241\ttrain_loss: 8.0125\n",
      "Iteration: 16 of 241\ttrain_loss: 7.6931\n",
      "Iteration: 18 of 241\ttrain_loss: 7.7183\n",
      "Iteration: 20 of 241\ttrain_loss: 7.7234\n",
      "Iteration: 22 of 241\ttrain_loss: 7.5492\n",
      "Iteration: 24 of 241\ttrain_loss: 7.4301\n",
      "Iteration: 26 of 241\ttrain_loss: 7.3753\n",
      "Iteration: 28 of 241\ttrain_loss: 7.4287\n",
      "Iteration: 30 of 241\ttrain_loss: 7.4462\n",
      "Iteration: 32 of 241\ttrain_loss: 7.5158\n",
      "Iteration: 34 of 241\ttrain_loss: 7.3164\n",
      "Iteration: 36 of 241\ttrain_loss: 7.5301\n",
      "Iteration: 38 of 241\ttrain_loss: 7.5154\n",
      "Iteration: 40 of 241\ttrain_loss: 7.3114\n",
      "Iteration: 42 of 241\ttrain_loss: 7.6156\n",
      "Iteration: 44 of 241\ttrain_loss: 7.4453\n",
      "Iteration: 46 of 241\ttrain_loss: 7.6381\n",
      "Iteration: 48 of 241\ttrain_loss: 7.3437\n",
      "Iteration: 50 of 241\ttrain_loss: 7.6632\n",
      "Iteration: 52 of 241\ttrain_loss: 7.6437\n",
      "Iteration: 54 of 241\ttrain_loss: 7.5975\n",
      "Iteration: 56 of 241\ttrain_loss: 7.7239\n",
      "Iteration: 58 of 241\ttrain_loss: 7.7047\n",
      "Iteration: 60 of 241\ttrain_loss: 7.6272\n",
      "Iteration: 62 of 241\ttrain_loss: 7.2725\n",
      "Iteration: 64 of 241\ttrain_loss: 7.6949\n",
      "Iteration: 66 of 241\ttrain_loss: 7.3188\n",
      "Iteration: 68 of 241\ttrain_loss: 7.4943\n",
      "Iteration: 70 of 241\ttrain_loss: 7.5989\n",
      "Iteration: 72 of 241\ttrain_loss: 7.3243\n",
      "Iteration: 74 of 241\ttrain_loss: 7.2008\n",
      "Iteration: 76 of 241\ttrain_loss: 7.7993\n",
      "Iteration: 78 of 241\ttrain_loss: 7.4487\n",
      "Iteration: 80 of 241\ttrain_loss: 7.7151\n",
      "Iteration: 82 of 241\ttrain_loss: 7.7665\n",
      "Iteration: 84 of 241\ttrain_loss: 7.5781\n",
      "Iteration: 86 of 241\ttrain_loss: 7.5567\n",
      "Iteration: 88 of 241\ttrain_loss: 7.7207\n",
      "Iteration: 90 of 241\ttrain_loss: 7.8238\n",
      "Iteration: 92 of 241\ttrain_loss: 7.6212\n",
      "Iteration: 94 of 241\ttrain_loss: 7.6683\n",
      "Iteration: 96 of 241\ttrain_loss: 7.8131\n",
      "Iteration: 98 of 241\ttrain_loss: 7.7982\n",
      "Iteration: 100 of 241\ttrain_loss: 7.7041\n",
      "Iteration: 102 of 241\ttrain_loss: 7.6226\n",
      "Iteration: 104 of 241\ttrain_loss: 7.3489\n",
      "Iteration: 106 of 241\ttrain_loss: 7.6845\n",
      "Iteration: 108 of 241\ttrain_loss: 7.7611\n",
      "Iteration: 110 of 241\ttrain_loss: 7.7653\n",
      "Iteration: 112 of 241\ttrain_loss: 7.7246\n",
      "Iteration: 114 of 241\ttrain_loss: 7.7169\n",
      "Iteration: 116 of 241\ttrain_loss: 7.7171\n",
      "Iteration: 118 of 241\ttrain_loss: 7.5100\n",
      "Iteration: 120 of 241\ttrain_loss: 7.7489\n",
      "Iteration: 122 of 241\ttrain_loss: 7.5615\n",
      "Iteration: 124 of 241\ttrain_loss: 7.6193\n",
      "Iteration: 126 of 241\ttrain_loss: 7.6268\n",
      "Iteration: 128 of 241\ttrain_loss: 7.6374\n",
      "Iteration: 130 of 241\ttrain_loss: 7.7665\n",
      "Iteration: 132 of 241\ttrain_loss: 7.5837\n",
      "Iteration: 134 of 241\ttrain_loss: 7.7001\n",
      "Iteration: 136 of 241\ttrain_loss: 7.2518\n",
      "Iteration: 138 of 241\ttrain_loss: 7.5252\n",
      "Iteration: 140 of 241\ttrain_loss: 7.5250\n",
      "Iteration: 142 of 241\ttrain_loss: 7.2207\n",
      "Iteration: 144 of 241\ttrain_loss: 7.6127\n",
      "Iteration: 146 of 241\ttrain_loss: 7.5121\n",
      "Iteration: 148 of 241\ttrain_loss: 7.1448\n",
      "Iteration: 150 of 241\ttrain_loss: 7.4231\n",
      "Iteration: 152 of 241\ttrain_loss: 6.9421\n",
      "Iteration: 154 of 241\ttrain_loss: 7.1049\n",
      "Iteration: 156 of 241\ttrain_loss: 7.3425\n",
      "Iteration: 158 of 241\ttrain_loss: 7.4741\n",
      "Iteration: 160 of 241\ttrain_loss: 7.2755\n",
      "Iteration: 162 of 241\ttrain_loss: 7.2091\n",
      "Iteration: 164 of 241\ttrain_loss: 7.3816\n",
      "Iteration: 166 of 241\ttrain_loss: 7.1252\n",
      "Iteration: 168 of 241\ttrain_loss: 7.3249\n",
      "Iteration: 170 of 241\ttrain_loss: 7.2098\n",
      "Iteration: 172 of 241\ttrain_loss: 7.2601\n",
      "Iteration: 174 of 241\ttrain_loss: 7.1233\n",
      "Iteration: 176 of 241\ttrain_loss: 6.9959\n",
      "Iteration: 178 of 241\ttrain_loss: 7.2656\n",
      "Iteration: 180 of 241\ttrain_loss: 7.2487\n",
      "Iteration: 182 of 241\ttrain_loss: 6.8889\n",
      "Iteration: 184 of 241\ttrain_loss: 6.8899\n",
      "Iteration: 186 of 241\ttrain_loss: 7.2247\n",
      "Iteration: 188 of 241\ttrain_loss: 7.2366\n",
      "Iteration: 190 of 241\ttrain_loss: 6.7903\n",
      "Iteration: 192 of 241\ttrain_loss: 7.1210\n",
      "Iteration: 194 of 241\ttrain_loss: 7.1434\n",
      "Iteration: 196 of 241\ttrain_loss: 7.1220\n",
      "Iteration: 198 of 241\ttrain_loss: 6.7462\n",
      "Iteration: 200 of 241\ttrain_loss: 7.1213\n",
      "Iteration: 202 of 241\ttrain_loss: 6.8444\n",
      "Iteration: 204 of 241\ttrain_loss: 6.9461\n",
      "Iteration: 206 of 241\ttrain_loss: 6.8780\n",
      "Iteration: 208 of 241\ttrain_loss: 6.9825\n",
      "Iteration: 210 of 241\ttrain_loss: 6.6372\n",
      "Iteration: 212 of 241\ttrain_loss: 7.0097\n",
      "Iteration: 214 of 241\ttrain_loss: 6.9262\n",
      "Iteration: 216 of 241\ttrain_loss: 6.7455\n",
      "Iteration: 218 of 241\ttrain_loss: 6.8325\n",
      "Iteration: 220 of 241\ttrain_loss: 7.1708\n",
      "Iteration: 222 of 241\ttrain_loss: 6.9301\n",
      "Iteration: 224 of 241\ttrain_loss: 7.1647\n",
      "Iteration: 226 of 241\ttrain_loss: 6.8475\n",
      "Iteration: 228 of 241\ttrain_loss: 6.8925\n",
      "Iteration: 230 of 241\ttrain_loss: 7.1117\n",
      "Iteration: 232 of 241\ttrain_loss: 7.1022\n",
      "Iteration: 234 of 241\ttrain_loss: 6.7909\n",
      "Iteration: 236 of 241\ttrain_loss: 7.2137\n",
      "Iteration: 238 of 241\ttrain_loss: 6.7230\n",
      "Iteration: 240 of 241\ttrain_loss: 6.8624\n",
      "Iteration: 241 of 241\ttrain_loss: 7.1477\n",
      "Average Score for this Epoch: 7.493853569030762\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 1 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.7478\n",
      "Iteration: 2 of 241\ttrain_loss: 6.6797\n",
      "Iteration: 4 of 241\ttrain_loss: 6.8811\n",
      "Iteration: 6 of 241\ttrain_loss: 6.8059\n",
      "Iteration: 8 of 241\ttrain_loss: 6.8987\n",
      "Iteration: 10 of 241\ttrain_loss: 7.1238\n",
      "Iteration: 12 of 241\ttrain_loss: 6.8763\n",
      "Iteration: 14 of 241\ttrain_loss: 7.0918\n",
      "Iteration: 16 of 241\ttrain_loss: 6.9673\n",
      "Iteration: 18 of 241\ttrain_loss: 7.0309\n",
      "Iteration: 20 of 241\ttrain_loss: 6.8586\n",
      "Iteration: 22 of 241\ttrain_loss: 6.9813\n",
      "Iteration: 24 of 241\ttrain_loss: 6.8983\n",
      "Iteration: 26 of 241\ttrain_loss: 7.0573\n",
      "Iteration: 28 of 241\ttrain_loss: 6.9659\n",
      "Iteration: 30 of 241\ttrain_loss: 7.1327\n",
      "Iteration: 32 of 241\ttrain_loss: 7.0272\n",
      "Iteration: 34 of 241\ttrain_loss: 7.0776\n",
      "Iteration: 36 of 241\ttrain_loss: 7.2447\n",
      "Iteration: 38 of 241\ttrain_loss: 7.0227\n",
      "Iteration: 40 of 241\ttrain_loss: 7.2453\n",
      "Iteration: 42 of 241\ttrain_loss: 7.3502\n",
      "Iteration: 44 of 241\ttrain_loss: 7.1542\n",
      "Iteration: 46 of 241\ttrain_loss: 6.7041\n",
      "Iteration: 48 of 241\ttrain_loss: 6.9617\n",
      "Iteration: 50 of 241\ttrain_loss: 6.9727\n",
      "Iteration: 52 of 241\ttrain_loss: 7.1672\n",
      "Iteration: 54 of 241\ttrain_loss: 6.9827\n",
      "Iteration: 56 of 241\ttrain_loss: 7.1707\n",
      "Iteration: 58 of 241\ttrain_loss: 7.4493\n",
      "Iteration: 60 of 241\ttrain_loss: 7.2255\n",
      "Iteration: 62 of 241\ttrain_loss: 7.2064\n",
      "Iteration: 64 of 241\ttrain_loss: 7.2596\n",
      "Iteration: 66 of 241\ttrain_loss: 7.3164\n",
      "Iteration: 68 of 241\ttrain_loss: 7.1945\n",
      "Iteration: 70 of 241\ttrain_loss: 7.2672\n",
      "Iteration: 72 of 241\ttrain_loss: 7.0579\n",
      "Iteration: 74 of 241\ttrain_loss: 6.9809\n",
      "Iteration: 76 of 241\ttrain_loss: 7.2597\n",
      "Iteration: 78 of 241\ttrain_loss: 7.0114\n",
      "Iteration: 80 of 241\ttrain_loss: 6.8806\n",
      "Iteration: 82 of 241\ttrain_loss: 7.1245\n",
      "Iteration: 84 of 241\ttrain_loss: 7.0266\n",
      "Iteration: 86 of 241\ttrain_loss: 6.9880\n",
      "Iteration: 88 of 241\ttrain_loss: 6.9790\n",
      "Iteration: 90 of 241\ttrain_loss: 6.8055\n",
      "Iteration: 92 of 241\ttrain_loss: 6.8788\n",
      "Iteration: 94 of 241\ttrain_loss: 6.7514\n",
      "Iteration: 96 of 241\ttrain_loss: 6.7688\n",
      "Iteration: 98 of 241\ttrain_loss: 6.7672\n",
      "Iteration: 100 of 241\ttrain_loss: 6.6992\n",
      "Iteration: 102 of 241\ttrain_loss: 6.6980\n",
      "Iteration: 104 of 241\ttrain_loss: 6.8799\n",
      "Iteration: 106 of 241\ttrain_loss: 6.7089\n",
      "Iteration: 108 of 241\ttrain_loss: 6.8593\n",
      "Iteration: 110 of 241\ttrain_loss: 6.6253\n",
      "Iteration: 112 of 241\ttrain_loss: 6.7195\n",
      "Iteration: 114 of 241\ttrain_loss: 6.7687\n",
      "Iteration: 116 of 241\ttrain_loss: 6.6345\n",
      "Iteration: 118 of 241\ttrain_loss: 6.8618\n",
      "Iteration: 120 of 241\ttrain_loss: 6.5658\n",
      "Iteration: 122 of 241\ttrain_loss: 6.8419\n",
      "Iteration: 124 of 241\ttrain_loss: 6.8515\n",
      "Iteration: 126 of 241\ttrain_loss: 6.7080\n",
      "Iteration: 128 of 241\ttrain_loss: 6.5266\n",
      "Iteration: 130 of 241\ttrain_loss: 6.6083\n",
      "Iteration: 132 of 241\ttrain_loss: 6.8090\n",
      "Iteration: 134 of 241\ttrain_loss: 6.5067\n",
      "Iteration: 136 of 241\ttrain_loss: 6.5794\n",
      "Iteration: 138 of 241\ttrain_loss: 6.6596\n",
      "Iteration: 140 of 241\ttrain_loss: 6.4445\n",
      "Iteration: 142 of 241\ttrain_loss: 6.6740\n",
      "Iteration: 144 of 241\ttrain_loss: 6.6341\n",
      "Iteration: 146 of 241\ttrain_loss: 6.5029\n",
      "Iteration: 148 of 241\ttrain_loss: 6.4191\n",
      "Iteration: 150 of 241\ttrain_loss: 6.7277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 152 of 241\ttrain_loss: 6.5395\n",
      "Iteration: 154 of 241\ttrain_loss: 6.6364\n",
      "Iteration: 156 of 241\ttrain_loss: 6.5365\n",
      "Iteration: 158 of 241\ttrain_loss: 6.7362\n",
      "Iteration: 160 of 241\ttrain_loss: 6.5483\n",
      "Iteration: 162 of 241\ttrain_loss: 6.5526\n",
      "Iteration: 164 of 241\ttrain_loss: 6.5998\n",
      "Iteration: 166 of 241\ttrain_loss: 6.5316\n",
      "Iteration: 168 of 241\ttrain_loss: 6.5836\n",
      "Iteration: 170 of 241\ttrain_loss: 6.6221\n",
      "Iteration: 172 of 241\ttrain_loss: 6.4601\n",
      "Iteration: 174 of 241\ttrain_loss: 6.4244\n",
      "Iteration: 176 of 241\ttrain_loss: 6.4997\n",
      "Iteration: 178 of 241\ttrain_loss: 6.5492\n",
      "Iteration: 180 of 241\ttrain_loss: 6.6839\n",
      "Iteration: 182 of 241\ttrain_loss: 6.5417\n",
      "Iteration: 184 of 241\ttrain_loss: 6.5399\n",
      "Iteration: 186 of 241\ttrain_loss: 6.4940\n",
      "Iteration: 188 of 241\ttrain_loss: 6.6722\n",
      "Iteration: 190 of 241\ttrain_loss: 6.5372\n",
      "Iteration: 192 of 241\ttrain_loss: 6.4331\n",
      "Iteration: 194 of 241\ttrain_loss: 6.3772\n",
      "Iteration: 196 of 241\ttrain_loss: 6.5643\n",
      "Iteration: 198 of 241\ttrain_loss: 6.6355\n",
      "Iteration: 200 of 241\ttrain_loss: 6.5634\n",
      "Iteration: 202 of 241\ttrain_loss: 6.6669\n",
      "Iteration: 204 of 241\ttrain_loss: 6.6038\n",
      "Iteration: 206 of 241\ttrain_loss: 6.5215\n",
      "Iteration: 208 of 241\ttrain_loss: 6.6906\n",
      "Iteration: 210 of 241\ttrain_loss: 6.6052\n",
      "Iteration: 212 of 241\ttrain_loss: 6.7638\n",
      "Iteration: 214 of 241\ttrain_loss: 6.6334\n",
      "Iteration: 216 of 241\ttrain_loss: 6.6367\n",
      "Iteration: 218 of 241\ttrain_loss: 6.5037\n",
      "Iteration: 220 of 241\ttrain_loss: 6.7425\n",
      "Iteration: 222 of 241\ttrain_loss: 6.4118\n",
      "Iteration: 224 of 241\ttrain_loss: 6.6386\n",
      "Iteration: 226 of 241\ttrain_loss: 6.6367\n",
      "Iteration: 228 of 241\ttrain_loss: 6.6087\n",
      "Iteration: 230 of 241\ttrain_loss: 6.7740\n",
      "Iteration: 232 of 241\ttrain_loss: 6.6922\n",
      "Iteration: 234 of 241\ttrain_loss: 6.8156\n",
      "Iteration: 236 of 241\ttrain_loss: 6.7660\n",
      "Iteration: 238 of 241\ttrain_loss: 6.6325\n",
      "Iteration: 240 of 241\ttrain_loss: 6.7161\n",
      "Iteration: 241 of 241\ttrain_loss: 6.3389\n",
      "Average Score for this Epoch: 6.780327320098877\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 2 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.4362\n",
      "Iteration: 2 of 241\ttrain_loss: 6.5533\n",
      "Iteration: 4 of 241\ttrain_loss: 6.4929\n",
      "Iteration: 6 of 241\ttrain_loss: 6.5810\n",
      "Iteration: 8 of 241\ttrain_loss: 6.5800\n",
      "Iteration: 10 of 241\ttrain_loss: 6.4869\n",
      "Iteration: 12 of 241\ttrain_loss: 6.6266\n",
      "Iteration: 14 of 241\ttrain_loss: 6.4088\n",
      "Iteration: 16 of 241\ttrain_loss: 6.5220\n",
      "Iteration: 18 of 241\ttrain_loss: 6.5821\n",
      "Iteration: 20 of 241\ttrain_loss: 6.6516\n",
      "Iteration: 22 of 241\ttrain_loss: 6.6623\n",
      "Iteration: 24 of 241\ttrain_loss: 6.5163\n",
      "Iteration: 26 of 241\ttrain_loss: 6.7276\n",
      "Iteration: 28 of 241\ttrain_loss: 6.6910\n",
      "Iteration: 30 of 241\ttrain_loss: 6.6498\n",
      "Iteration: 32 of 241\ttrain_loss: 6.6641\n",
      "Iteration: 34 of 241\ttrain_loss: 6.4938\n",
      "Iteration: 36 of 241\ttrain_loss: 6.7933\n",
      "Iteration: 38 of 241\ttrain_loss: 6.4188\n",
      "Iteration: 40 of 241\ttrain_loss: 6.5424\n",
      "Iteration: 42 of 241\ttrain_loss: 6.5064\n",
      "Iteration: 44 of 241\ttrain_loss: 6.7865\n",
      "Iteration: 46 of 241\ttrain_loss: 6.7923\n",
      "Iteration: 48 of 241\ttrain_loss: 6.5943\n",
      "Iteration: 50 of 241\ttrain_loss: 6.5515\n",
      "Iteration: 52 of 241\ttrain_loss: 6.6154\n",
      "Iteration: 54 of 241\ttrain_loss: 6.6369\n",
      "Iteration: 56 of 241\ttrain_loss: 6.5811\n",
      "Iteration: 58 of 241\ttrain_loss: 6.6224\n",
      "Iteration: 60 of 241\ttrain_loss: 6.6638\n",
      "Iteration: 62 of 241\ttrain_loss: 6.5931\n",
      "Iteration: 64 of 241\ttrain_loss: 6.5754\n",
      "Iteration: 66 of 241\ttrain_loss: 6.5634\n",
      "Iteration: 68 of 241\ttrain_loss: 6.4813\n",
      "Iteration: 70 of 241\ttrain_loss: 6.6179\n",
      "Iteration: 72 of 241\ttrain_loss: 6.5889\n",
      "Iteration: 74 of 241\ttrain_loss: 6.3789\n",
      "Iteration: 76 of 241\ttrain_loss: 6.5784\n",
      "Iteration: 78 of 241\ttrain_loss: 6.6586\n",
      "Iteration: 80 of 241\ttrain_loss: 6.6811\n",
      "Iteration: 82 of 241\ttrain_loss: 6.5841\n",
      "Iteration: 84 of 241\ttrain_loss: 6.5318\n",
      "Iteration: 86 of 241\ttrain_loss: 6.5517\n",
      "Iteration: 88 of 241\ttrain_loss: 6.3337\n",
      "Iteration: 90 of 241\ttrain_loss: 6.7629\n",
      "Iteration: 92 of 241\ttrain_loss: 6.3937\n",
      "Iteration: 94 of 241\ttrain_loss: 6.4684\n",
      "Iteration: 96 of 241\ttrain_loss: 6.3689\n",
      "Iteration: 98 of 241\ttrain_loss: 6.6235\n",
      "Iteration: 100 of 241\ttrain_loss: 6.5965\n",
      "Iteration: 102 of 241\ttrain_loss: 6.5031\n",
      "Iteration: 104 of 241\ttrain_loss: 6.6261\n",
      "Iteration: 106 of 241\ttrain_loss: 6.4793\n",
      "Iteration: 108 of 241\ttrain_loss: 6.3981\n",
      "Iteration: 110 of 241\ttrain_loss: 6.5348\n",
      "Iteration: 112 of 241\ttrain_loss: 6.5578\n",
      "Iteration: 114 of 241\ttrain_loss: 6.4717\n",
      "Iteration: 116 of 241\ttrain_loss: 6.4894\n",
      "Iteration: 118 of 241\ttrain_loss: 6.5729\n",
      "Iteration: 120 of 241\ttrain_loss: 6.5415\n",
      "Iteration: 122 of 241\ttrain_loss: 6.7158\n",
      "Iteration: 124 of 241\ttrain_loss: 6.6457\n",
      "Iteration: 126 of 241\ttrain_loss: 6.4590\n",
      "Iteration: 128 of 241\ttrain_loss: 6.6023\n",
      "Iteration: 130 of 241\ttrain_loss: 6.4767\n",
      "Iteration: 132 of 241\ttrain_loss: 6.5740\n",
      "Iteration: 134 of 241\ttrain_loss: 6.6001\n",
      "Iteration: 136 of 241\ttrain_loss: 6.4100\n",
      "Iteration: 138 of 241\ttrain_loss: 6.5846\n",
      "Iteration: 140 of 241\ttrain_loss: 6.4686\n",
      "Iteration: 142 of 241\ttrain_loss: 6.4117\n",
      "Iteration: 144 of 241\ttrain_loss: 6.6027\n",
      "Iteration: 146 of 241\ttrain_loss: 6.3220\n",
      "Iteration: 148 of 241\ttrain_loss: 6.5774\n",
      "Iteration: 150 of 241\ttrain_loss: 6.6523\n",
      "Iteration: 152 of 241\ttrain_loss: 6.6359\n",
      "Iteration: 154 of 241\ttrain_loss: 6.4794\n",
      "Iteration: 156 of 241\ttrain_loss: 6.5048\n",
      "Iteration: 158 of 241\ttrain_loss: 6.6086\n",
      "Iteration: 160 of 241\ttrain_loss: 6.5621\n",
      "Iteration: 162 of 241\ttrain_loss: 6.4994\n",
      "Iteration: 164 of 241\ttrain_loss: 6.6119\n",
      "Iteration: 166 of 241\ttrain_loss: 6.7045\n",
      "Iteration: 168 of 241\ttrain_loss: 6.4245\n",
      "Iteration: 170 of 241\ttrain_loss: 6.3218\n",
      "Iteration: 172 of 241\ttrain_loss: 6.6238\n",
      "Iteration: 174 of 241\ttrain_loss: 6.6067\n",
      "Iteration: 176 of 241\ttrain_loss: 6.5659\n",
      "Iteration: 178 of 241\ttrain_loss: 6.5386\n",
      "Iteration: 180 of 241\ttrain_loss: 6.6024\n",
      "Iteration: 182 of 241\ttrain_loss: 6.7538\n",
      "Iteration: 184 of 241\ttrain_loss: 6.4785\n",
      "Iteration: 186 of 241\ttrain_loss: 6.4201\n",
      "Iteration: 188 of 241\ttrain_loss: 6.3681\n",
      "Iteration: 190 of 241\ttrain_loss: 6.4582\n",
      "Iteration: 192 of 241\ttrain_loss: 6.5546\n",
      "Iteration: 194 of 241\ttrain_loss: 6.6286\n",
      "Iteration: 196 of 241\ttrain_loss: 6.6197\n",
      "Iteration: 198 of 241\ttrain_loss: 6.5457\n",
      "Iteration: 200 of 241\ttrain_loss: 6.6172\n",
      "Iteration: 202 of 241\ttrain_loss: 6.7097\n",
      "Iteration: 204 of 241\ttrain_loss: 6.6146\n",
      "Iteration: 206 of 241\ttrain_loss: 6.8225\n",
      "Iteration: 208 of 241\ttrain_loss: 6.5800\n",
      "Iteration: 210 of 241\ttrain_loss: 6.6766\n",
      "Iteration: 212 of 241\ttrain_loss: 6.5170\n",
      "Iteration: 214 of 241\ttrain_loss: 6.5672\n",
      "Iteration: 216 of 241\ttrain_loss: 6.6339\n",
      "Iteration: 218 of 241\ttrain_loss: 6.4469\n",
      "Iteration: 220 of 241\ttrain_loss: 6.6892\n",
      "Iteration: 222 of 241\ttrain_loss: 6.6003\n",
      "Iteration: 224 of 241\ttrain_loss: 6.6890\n",
      "Iteration: 226 of 241\ttrain_loss: 6.7189\n",
      "Iteration: 228 of 241\ttrain_loss: 6.4470\n",
      "Iteration: 230 of 241\ttrain_loss: 6.5792\n",
      "Iteration: 232 of 241\ttrain_loss: 6.7458\n",
      "Iteration: 234 of 241\ttrain_loss: 6.4547\n",
      "Iteration: 236 of 241\ttrain_loss: 6.5432\n",
      "Iteration: 238 of 241\ttrain_loss: 6.6686\n",
      "Iteration: 240 of 241\ttrain_loss: 6.5671\n",
      "Iteration: 241 of 241\ttrain_loss: 6.5036\n",
      "Average Score for this Epoch: 6.556488990783691\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 3 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.5074\n",
      "Iteration: 2 of 241\ttrain_loss: 6.5682\n",
      "Iteration: 4 of 241\ttrain_loss: 6.5372\n",
      "Iteration: 6 of 241\ttrain_loss: 6.4852\n",
      "Iteration: 8 of 241\ttrain_loss: 6.5386\n",
      "Iteration: 10 of 241\ttrain_loss: 6.6153\n",
      "Iteration: 12 of 241\ttrain_loss: 6.4688\n",
      "Iteration: 14 of 241\ttrain_loss: 6.5227\n",
      "Iteration: 16 of 241\ttrain_loss: 6.3709\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3603\n",
      "Iteration: 20 of 241\ttrain_loss: 6.4278\n",
      "Iteration: 22 of 241\ttrain_loss: 6.4477\n",
      "Iteration: 24 of 241\ttrain_loss: 6.4732\n",
      "Iteration: 26 of 241\ttrain_loss: 6.4302\n",
      "Iteration: 28 of 241\ttrain_loss: 6.3410\n",
      "Iteration: 30 of 241\ttrain_loss: 6.4089\n",
      "Iteration: 32 of 241\ttrain_loss: 6.4293\n",
      "Iteration: 34 of 241\ttrain_loss: 6.5173\n",
      "Iteration: 36 of 241\ttrain_loss: 6.3878\n",
      "Iteration: 38 of 241\ttrain_loss: 6.2957\n",
      "Iteration: 40 of 241\ttrain_loss: 6.5262\n",
      "Iteration: 42 of 241\ttrain_loss: 6.6322\n",
      "Iteration: 44 of 241\ttrain_loss: 6.4699\n",
      "Iteration: 46 of 241\ttrain_loss: 6.5856\n",
      "Iteration: 48 of 241\ttrain_loss: 6.4913\n",
      "Iteration: 50 of 241\ttrain_loss: 6.6751\n",
      "Iteration: 52 of 241\ttrain_loss: 6.4817\n",
      "Iteration: 54 of 241\ttrain_loss: 6.3864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 56 of 241\ttrain_loss: 6.3232\n",
      "Iteration: 58 of 241\ttrain_loss: 6.6135\n",
      "Iteration: 60 of 241\ttrain_loss: 6.5061\n",
      "Iteration: 62 of 241\ttrain_loss: 6.4357\n",
      "Iteration: 64 of 241\ttrain_loss: 6.6277\n",
      "Iteration: 66 of 241\ttrain_loss: 6.3125\n",
      "Iteration: 68 of 241\ttrain_loss: 6.4450\n",
      "Iteration: 70 of 241\ttrain_loss: 6.5981\n",
      "Iteration: 72 of 241\ttrain_loss: 6.5300\n",
      "Iteration: 74 of 241\ttrain_loss: 6.5954\n",
      "Iteration: 76 of 241\ttrain_loss: 6.4327\n",
      "Iteration: 78 of 241\ttrain_loss: 6.3872\n",
      "Iteration: 80 of 241\ttrain_loss: 6.4305\n",
      "Iteration: 82 of 241\ttrain_loss: 6.5806\n",
      "Iteration: 84 of 241\ttrain_loss: 6.4518\n",
      "Iteration: 86 of 241\ttrain_loss: 6.4303\n",
      "Iteration: 88 of 241\ttrain_loss: 6.3996\n",
      "Iteration: 90 of 241\ttrain_loss: 6.7113\n",
      "Iteration: 92 of 241\ttrain_loss: 6.5118\n",
      "Iteration: 94 of 241\ttrain_loss: 6.5471\n",
      "Iteration: 96 of 241\ttrain_loss: 6.4989\n",
      "Iteration: 98 of 241\ttrain_loss: 6.5261\n",
      "Iteration: 100 of 241\ttrain_loss: 6.3357\n",
      "Iteration: 102 of 241\ttrain_loss: 6.5976\n",
      "Iteration: 104 of 241\ttrain_loss: 6.4025\n",
      "Iteration: 106 of 241\ttrain_loss: 6.1763\n",
      "Iteration: 108 of 241\ttrain_loss: 6.4771\n",
      "Iteration: 110 of 241\ttrain_loss: 6.4462\n",
      "Iteration: 112 of 241\ttrain_loss: 6.4297\n",
      "Iteration: 114 of 241\ttrain_loss: 6.3632\n",
      "Iteration: 116 of 241\ttrain_loss: 6.3898\n",
      "Iteration: 118 of 241\ttrain_loss: 6.3867\n",
      "Iteration: 120 of 241\ttrain_loss: 6.5075\n",
      "Iteration: 122 of 241\ttrain_loss: 6.3498\n",
      "Iteration: 124 of 241\ttrain_loss: 6.6423\n",
      "Iteration: 126 of 241\ttrain_loss: 6.3962\n",
      "Iteration: 128 of 241\ttrain_loss: 6.6103\n",
      "Iteration: 130 of 241\ttrain_loss: 6.4461\n",
      "Iteration: 132 of 241\ttrain_loss: 6.3635\n",
      "Iteration: 134 of 241\ttrain_loss: 6.4442\n",
      "Iteration: 136 of 241\ttrain_loss: 6.3367\n",
      "Iteration: 138 of 241\ttrain_loss: 6.5810\n",
      "Iteration: 140 of 241\ttrain_loss: 6.4736\n",
      "Iteration: 142 of 241\ttrain_loss: 6.4434\n",
      "Iteration: 144 of 241\ttrain_loss: 6.6104\n",
      "Iteration: 146 of 241\ttrain_loss: 6.5663\n",
      "Iteration: 148 of 241\ttrain_loss: 6.4915\n",
      "Iteration: 150 of 241\ttrain_loss: 6.6044\n",
      "Iteration: 152 of 241\ttrain_loss: 6.4609\n",
      "Iteration: 154 of 241\ttrain_loss: 6.6585\n",
      "Iteration: 156 of 241\ttrain_loss: 6.5400\n",
      "Iteration: 158 of 241\ttrain_loss: 6.4610\n",
      "Iteration: 160 of 241\ttrain_loss: 6.5383\n",
      "Iteration: 162 of 241\ttrain_loss: 6.6221\n",
      "Iteration: 164 of 241\ttrain_loss: 6.6579\n",
      "Iteration: 166 of 241\ttrain_loss: 6.5939\n",
      "Iteration: 168 of 241\ttrain_loss: 6.6408\n",
      "Iteration: 170 of 241\ttrain_loss: 6.4817\n",
      "Iteration: 172 of 241\ttrain_loss: 6.5944\n",
      "Iteration: 174 of 241\ttrain_loss: 6.6277\n",
      "Iteration: 176 of 241\ttrain_loss: 6.4979\n",
      "Iteration: 178 of 241\ttrain_loss: 6.6147\n",
      "Iteration: 180 of 241\ttrain_loss: 6.5820\n",
      "Iteration: 182 of 241\ttrain_loss: 6.4797\n",
      "Iteration: 184 of 241\ttrain_loss: 6.5873\n",
      "Iteration: 186 of 241\ttrain_loss: 6.6495\n",
      "Iteration: 188 of 241\ttrain_loss: 6.6269\n",
      "Iteration: 190 of 241\ttrain_loss: 6.5661\n",
      "Iteration: 192 of 241\ttrain_loss: 6.5293\n",
      "Iteration: 194 of 241\ttrain_loss: 6.5010\n",
      "Iteration: 196 of 241\ttrain_loss: 6.5866\n",
      "Iteration: 198 of 241\ttrain_loss: 6.5897\n",
      "Iteration: 200 of 241\ttrain_loss: 6.5811\n",
      "Iteration: 202 of 241\ttrain_loss: 6.7936\n",
      "Iteration: 204 of 241\ttrain_loss: 6.4934\n",
      "Iteration: 206 of 241\ttrain_loss: 6.3023\n",
      "Iteration: 208 of 241\ttrain_loss: 6.7122\n",
      "Iteration: 210 of 241\ttrain_loss: 6.6030\n",
      "Iteration: 212 of 241\ttrain_loss: 6.5744\n",
      "Iteration: 214 of 241\ttrain_loss: 6.5434\n",
      "Iteration: 216 of 241\ttrain_loss: 6.6418\n",
      "Iteration: 218 of 241\ttrain_loss: 6.5134\n",
      "Iteration: 220 of 241\ttrain_loss: 6.7240\n",
      "Iteration: 222 of 241\ttrain_loss: 6.6914\n",
      "Iteration: 224 of 241\ttrain_loss: 6.6785\n",
      "Iteration: 226 of 241\ttrain_loss: 6.6433\n",
      "Iteration: 228 of 241\ttrain_loss: 6.6795\n",
      "Iteration: 230 of 241\ttrain_loss: 6.5868\n",
      "Iteration: 232 of 241\ttrain_loss: 6.5730\n",
      "Iteration: 234 of 241\ttrain_loss: 6.5091\n",
      "Iteration: 236 of 241\ttrain_loss: 6.6470\n",
      "Iteration: 238 of 241\ttrain_loss: 6.4434\n",
      "Iteration: 240 of 241\ttrain_loss: 6.5250\n",
      "Iteration: 241 of 241\ttrain_loss: 6.8503\n",
      "Average Score for this Epoch: 6.511036396026611\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 4 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.4589\n",
      "Iteration: 2 of 241\ttrain_loss: 6.2523\n",
      "Iteration: 4 of 241\ttrain_loss: 6.4361\n",
      "Iteration: 6 of 241\ttrain_loss: 6.5127\n",
      "Iteration: 8 of 241\ttrain_loss: 6.6911\n",
      "Iteration: 10 of 241\ttrain_loss: 6.6150\n",
      "Iteration: 12 of 241\ttrain_loss: 6.3709\n",
      "Iteration: 14 of 241\ttrain_loss: 6.4126\n",
      "Iteration: 16 of 241\ttrain_loss: 6.2690\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3424\n",
      "Iteration: 20 of 241\ttrain_loss: 6.6008\n",
      "Iteration: 22 of 241\ttrain_loss: 6.5000\n",
      "Iteration: 24 of 241\ttrain_loss: 6.2253\n",
      "Iteration: 26 of 241\ttrain_loss: 6.4650\n",
      "Iteration: 28 of 241\ttrain_loss: 6.5439\n",
      "Iteration: 30 of 241\ttrain_loss: 6.3456\n",
      "Iteration: 32 of 241\ttrain_loss: 6.2823\n",
      "Iteration: 34 of 241\ttrain_loss: 6.5869\n",
      "Iteration: 36 of 241\ttrain_loss: 6.3462\n",
      "Iteration: 38 of 241\ttrain_loss: 6.5271\n",
      "Iteration: 40 of 241\ttrain_loss: 6.3259\n",
      "Iteration: 42 of 241\ttrain_loss: 6.3763\n",
      "Iteration: 44 of 241\ttrain_loss: 6.3269\n",
      "Iteration: 46 of 241\ttrain_loss: 6.5403\n",
      "Iteration: 48 of 241\ttrain_loss: 6.3037\n",
      "Iteration: 50 of 241\ttrain_loss: 6.4136\n",
      "Iteration: 52 of 241\ttrain_loss: 6.5708\n",
      "Iteration: 54 of 241\ttrain_loss: 6.4616\n",
      "Iteration: 56 of 241\ttrain_loss: 6.4444\n",
      "Iteration: 58 of 241\ttrain_loss: 6.2303\n",
      "Iteration: 60 of 241\ttrain_loss: 6.3081\n",
      "Iteration: 62 of 241\ttrain_loss: 6.3419\n",
      "Iteration: 64 of 241\ttrain_loss: 6.4570\n",
      "Iteration: 66 of 241\ttrain_loss: 6.5774\n",
      "Iteration: 68 of 241\ttrain_loss: 6.3906\n",
      "Iteration: 70 of 241\ttrain_loss: 6.4307\n",
      "Iteration: 72 of 241\ttrain_loss: 6.4980\n",
      "Iteration: 74 of 241\ttrain_loss: 6.3561\n",
      "Iteration: 76 of 241\ttrain_loss: 6.3349\n",
      "Iteration: 78 of 241\ttrain_loss: 6.3930\n",
      "Iteration: 80 of 241\ttrain_loss: 6.2252\n",
      "Iteration: 82 of 241\ttrain_loss: 6.4415\n",
      "Iteration: 84 of 241\ttrain_loss: 6.2855\n",
      "Iteration: 86 of 241\ttrain_loss: 6.4035\n",
      "Iteration: 88 of 241\ttrain_loss: 6.2974\n",
      "Iteration: 90 of 241\ttrain_loss: 6.3529\n",
      "Iteration: 92 of 241\ttrain_loss: 6.4516\n",
      "Iteration: 94 of 241\ttrain_loss: 6.5883\n",
      "Iteration: 96 of 241\ttrain_loss: 6.3249\n",
      "Iteration: 98 of 241\ttrain_loss: 6.6546\n",
      "Iteration: 100 of 241\ttrain_loss: 6.4420\n",
      "Iteration: 102 of 241\ttrain_loss: 6.4056\n",
      "Iteration: 104 of 241\ttrain_loss: 6.1588\n",
      "Iteration: 106 of 241\ttrain_loss: 6.6298\n",
      "Iteration: 108 of 241\ttrain_loss: 6.4836\n",
      "Iteration: 110 of 241\ttrain_loss: 6.2695\n",
      "Iteration: 112 of 241\ttrain_loss: 6.4542\n",
      "Iteration: 114 of 241\ttrain_loss: 6.6177\n",
      "Iteration: 116 of 241\ttrain_loss: 6.6354\n",
      "Iteration: 118 of 241\ttrain_loss: 6.3996\n",
      "Iteration: 120 of 241\ttrain_loss: 6.7370\n",
      "Iteration: 122 of 241\ttrain_loss: 6.6153\n",
      "Iteration: 124 of 241\ttrain_loss: 6.5556\n",
      "Iteration: 126 of 241\ttrain_loss: 6.5497\n",
      "Iteration: 128 of 241\ttrain_loss: 6.6171\n",
      "Iteration: 130 of 241\ttrain_loss: 6.5836\n",
      "Iteration: 132 of 241\ttrain_loss: 6.6280\n",
      "Iteration: 134 of 241\ttrain_loss: 6.3866\n",
      "Iteration: 136 of 241\ttrain_loss: 6.4771\n",
      "Iteration: 138 of 241\ttrain_loss: 6.4706\n",
      "Iteration: 140 of 241\ttrain_loss: 6.6061\n",
      "Iteration: 142 of 241\ttrain_loss: 6.3833\n",
      "Iteration: 144 of 241\ttrain_loss: 6.6863\n",
      "Iteration: 146 of 241\ttrain_loss: 6.2806\n",
      "Iteration: 148 of 241\ttrain_loss: 6.7319\n",
      "Iteration: 150 of 241\ttrain_loss: 6.4915\n",
      "Iteration: 152 of 241\ttrain_loss: 6.7573\n",
      "Iteration: 154 of 241\ttrain_loss: 6.6485\n",
      "Iteration: 156 of 241\ttrain_loss: 6.6773\n",
      "Iteration: 158 of 241\ttrain_loss: 6.4556\n",
      "Iteration: 160 of 241\ttrain_loss: 6.4945\n",
      "Iteration: 162 of 241\ttrain_loss: 6.5784\n",
      "Iteration: 164 of 241\ttrain_loss: 6.4066\n",
      "Iteration: 166 of 241\ttrain_loss: 6.6175\n",
      "Iteration: 168 of 241\ttrain_loss: 6.4975\n",
      "Iteration: 170 of 241\ttrain_loss: 6.5174\n",
      "Iteration: 172 of 241\ttrain_loss: 6.4209\n",
      "Iteration: 174 of 241\ttrain_loss: 6.5609\n",
      "Iteration: 176 of 241\ttrain_loss: 6.5939\n",
      "Iteration: 178 of 241\ttrain_loss: 6.3863\n",
      "Iteration: 180 of 241\ttrain_loss: 6.5020\n",
      "Iteration: 182 of 241\ttrain_loss: 6.6731\n",
      "Iteration: 184 of 241\ttrain_loss: 6.4783\n",
      "Iteration: 186 of 241\ttrain_loss: 6.5755\n",
      "Iteration: 188 of 241\ttrain_loss: 6.7695\n",
      "Iteration: 190 of 241\ttrain_loss: 6.5910\n",
      "Iteration: 192 of 241\ttrain_loss: 6.4523\n",
      "Iteration: 194 of 241\ttrain_loss: 6.4683\n",
      "Iteration: 196 of 241\ttrain_loss: 6.4165\n",
      "Iteration: 198 of 241\ttrain_loss: 6.5297\n",
      "Iteration: 200 of 241\ttrain_loss: 6.5485\n",
      "Iteration: 202 of 241\ttrain_loss: 6.7072\n",
      "Iteration: 204 of 241\ttrain_loss: 6.7054\n",
      "Iteration: 206 of 241\ttrain_loss: 6.4566\n",
      "Iteration: 208 of 241\ttrain_loss: 6.5347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 210 of 241\ttrain_loss: 6.6704\n",
      "Iteration: 212 of 241\ttrain_loss: 6.6194\n",
      "Iteration: 214 of 241\ttrain_loss: 6.7039\n",
      "Iteration: 216 of 241\ttrain_loss: 6.4060\n",
      "Iteration: 218 of 241\ttrain_loss: 6.4602\n",
      "Iteration: 220 of 241\ttrain_loss: 6.6239\n",
      "Iteration: 222 of 241\ttrain_loss: 6.5800\n",
      "Iteration: 224 of 241\ttrain_loss: 6.5063\n",
      "Iteration: 226 of 241\ttrain_loss: 6.5239\n",
      "Iteration: 228 of 241\ttrain_loss: 6.4464\n",
      "Iteration: 230 of 241\ttrain_loss: 6.5552\n",
      "Iteration: 232 of 241\ttrain_loss: 6.4461\n",
      "Iteration: 234 of 241\ttrain_loss: 6.6975\n",
      "Iteration: 236 of 241\ttrain_loss: 6.4639\n",
      "Iteration: 238 of 241\ttrain_loss: 6.4504\n",
      "Iteration: 240 of 241\ttrain_loss: 6.5601\n",
      "Iteration: 241 of 241\ttrain_loss: 6.4484\n",
      "Average Score for this Epoch: 6.470277786254883\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 5 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.4617\n",
      "Iteration: 2 of 241\ttrain_loss: 6.3778\n",
      "Iteration: 4 of 241\ttrain_loss: 6.4901\n",
      "Iteration: 6 of 241\ttrain_loss: 6.4145\n",
      "Iteration: 8 of 241\ttrain_loss: 6.2227\n",
      "Iteration: 10 of 241\ttrain_loss: 6.1340\n",
      "Iteration: 12 of 241\ttrain_loss: 6.3831\n",
      "Iteration: 14 of 241\ttrain_loss: 6.3517\n",
      "Iteration: 16 of 241\ttrain_loss: 6.3176\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3196\n",
      "Iteration: 20 of 241\ttrain_loss: 6.4779\n",
      "Iteration: 22 of 241\ttrain_loss: 6.4628\n",
      "Iteration: 24 of 241\ttrain_loss: 6.3006\n",
      "Iteration: 26 of 241\ttrain_loss: 6.2995\n",
      "Iteration: 28 of 241\ttrain_loss: 6.3241\n",
      "Iteration: 30 of 241\ttrain_loss: 6.3508\n",
      "Iteration: 32 of 241\ttrain_loss: 6.3952\n",
      "Iteration: 34 of 241\ttrain_loss: 6.2421\n",
      "Iteration: 36 of 241\ttrain_loss: 6.4533\n",
      "Iteration: 38 of 241\ttrain_loss: 6.3971\n",
      "Iteration: 40 of 241\ttrain_loss: 6.6310\n",
      "Iteration: 42 of 241\ttrain_loss: 6.3267\n",
      "Iteration: 44 of 241\ttrain_loss: 6.4795\n",
      "Iteration: 46 of 241\ttrain_loss: 6.0883\n",
      "Iteration: 48 of 241\ttrain_loss: 6.4126\n",
      "Iteration: 50 of 241\ttrain_loss: 6.3526\n",
      "Iteration: 52 of 241\ttrain_loss: 6.6066\n",
      "Iteration: 54 of 241\ttrain_loss: 6.4374\n",
      "Iteration: 56 of 241\ttrain_loss: 6.4882\n",
      "Iteration: 58 of 241\ttrain_loss: 6.5011\n",
      "Iteration: 60 of 241\ttrain_loss: 6.2493\n",
      "Iteration: 62 of 241\ttrain_loss: 6.4455\n",
      "Iteration: 64 of 241\ttrain_loss: 6.3622\n",
      "Iteration: 66 of 241\ttrain_loss: 6.4403\n",
      "Iteration: 68 of 241\ttrain_loss: 6.5125\n",
      "Iteration: 70 of 241\ttrain_loss: 6.3723\n",
      "Iteration: 72 of 241\ttrain_loss: 6.5037\n",
      "Iteration: 74 of 241\ttrain_loss: 6.2946\n",
      "Iteration: 76 of 241\ttrain_loss: 6.6099\n",
      "Iteration: 78 of 241\ttrain_loss: 6.7852\n",
      "Iteration: 80 of 241\ttrain_loss: 6.4540\n",
      "Iteration: 82 of 241\ttrain_loss: 6.6922\n",
      "Iteration: 84 of 241\ttrain_loss: 6.4377\n",
      "Iteration: 86 of 241\ttrain_loss: 6.4568\n",
      "Iteration: 88 of 241\ttrain_loss: 6.6233\n",
      "Iteration: 90 of 241\ttrain_loss: 6.4996\n",
      "Iteration: 92 of 241\ttrain_loss: 6.6865\n",
      "Iteration: 94 of 241\ttrain_loss: 6.4809\n",
      "Iteration: 96 of 241\ttrain_loss: 6.6092\n",
      "Iteration: 98 of 241\ttrain_loss: 6.3809\n",
      "Iteration: 100 of 241\ttrain_loss: 6.5031\n",
      "Iteration: 102 of 241\ttrain_loss: 6.6763\n",
      "Iteration: 104 of 241\ttrain_loss: 6.4602\n",
      "Iteration: 106 of 241\ttrain_loss: 6.6714\n",
      "Iteration: 108 of 241\ttrain_loss: 6.5700\n",
      "Iteration: 110 of 241\ttrain_loss: 6.5547\n",
      "Iteration: 112 of 241\ttrain_loss: 6.5235\n",
      "Iteration: 114 of 241\ttrain_loss: 6.3894\n",
      "Iteration: 116 of 241\ttrain_loss: 6.4630\n",
      "Iteration: 118 of 241\ttrain_loss: 6.6329\n",
      "Iteration: 120 of 241\ttrain_loss: 6.4358\n",
      "Iteration: 122 of 241\ttrain_loss: 6.5258\n",
      "Iteration: 124 of 241\ttrain_loss: 6.5659\n",
      "Iteration: 126 of 241\ttrain_loss: 6.4821\n",
      "Iteration: 128 of 241\ttrain_loss: 6.5469\n",
      "Iteration: 130 of 241\ttrain_loss: 6.4885\n",
      "Iteration: 132 of 241\ttrain_loss: 6.4686\n",
      "Iteration: 134 of 241\ttrain_loss: 6.4929\n",
      "Iteration: 136 of 241\ttrain_loss: 6.4815\n",
      "Iteration: 138 of 241\ttrain_loss: 6.4631\n",
      "Iteration: 140 of 241\ttrain_loss: 6.5913\n",
      "Iteration: 142 of 241\ttrain_loss: 6.5597\n",
      "Iteration: 144 of 241\ttrain_loss: 6.4994\n",
      "Iteration: 146 of 241\ttrain_loss: 6.7553\n",
      "Iteration: 148 of 241\ttrain_loss: 6.4841\n",
      "Iteration: 150 of 241\ttrain_loss: 6.5789\n",
      "Iteration: 152 of 241\ttrain_loss: 6.6448\n",
      "Iteration: 154 of 241\ttrain_loss: 6.5189\n",
      "Iteration: 156 of 241\ttrain_loss: 6.3688\n",
      "Iteration: 158 of 241\ttrain_loss: 6.5850\n",
      "Iteration: 160 of 241\ttrain_loss: 6.6418\n",
      "Iteration: 162 of 241\ttrain_loss: 6.5567\n",
      "Iteration: 164 of 241\ttrain_loss: 6.3339\n",
      "Iteration: 166 of 241\ttrain_loss: 6.4785\n",
      "Iteration: 168 of 241\ttrain_loss: 6.7458\n",
      "Iteration: 170 of 241\ttrain_loss: 6.3478\n",
      "Iteration: 172 of 241\ttrain_loss: 6.5240\n",
      "Iteration: 174 of 241\ttrain_loss: 6.4467\n",
      "Iteration: 176 of 241\ttrain_loss: 6.5404\n",
      "Iteration: 178 of 241\ttrain_loss: 6.5699\n",
      "Iteration: 180 of 241\ttrain_loss: 6.3558\n",
      "Iteration: 182 of 241\ttrain_loss: 6.4493\n",
      "Iteration: 184 of 241\ttrain_loss: 6.3940\n",
      "Iteration: 186 of 241\ttrain_loss: 6.5231\n",
      "Iteration: 188 of 241\ttrain_loss: 6.3193\n",
      "Iteration: 190 of 241\ttrain_loss: 6.2775\n",
      "Iteration: 192 of 241\ttrain_loss: 6.2243\n",
      "Iteration: 194 of 241\ttrain_loss: 6.4760\n",
      "Iteration: 196 of 241\ttrain_loss: 6.3963\n",
      "Iteration: 198 of 241\ttrain_loss: 6.4229\n",
      "Iteration: 200 of 241\ttrain_loss: 6.3434\n",
      "Iteration: 202 of 241\ttrain_loss: 6.5039\n",
      "Iteration: 204 of 241\ttrain_loss: 6.4771\n",
      "Iteration: 206 of 241\ttrain_loss: 6.3971\n",
      "Iteration: 208 of 241\ttrain_loss: 6.4593\n",
      "Iteration: 210 of 241\ttrain_loss: 6.4161\n",
      "Iteration: 212 of 241\ttrain_loss: 6.3159\n",
      "Iteration: 214 of 241\ttrain_loss: 6.4167\n",
      "Iteration: 216 of 241\ttrain_loss: 6.4190\n",
      "Iteration: 218 of 241\ttrain_loss: 6.3671\n",
      "Iteration: 220 of 241\ttrain_loss: 6.2931\n",
      "Iteration: 222 of 241\ttrain_loss: 6.5298\n",
      "Iteration: 224 of 241\ttrain_loss: 6.8159\n",
      "Iteration: 226 of 241\ttrain_loss: 6.4366\n",
      "Iteration: 228 of 241\ttrain_loss: 6.5518\n",
      "Iteration: 230 of 241\ttrain_loss: 6.3829\n",
      "Iteration: 232 of 241\ttrain_loss: 6.3544\n",
      "Iteration: 234 of 241\ttrain_loss: 6.5829\n",
      "Iteration: 236 of 241\ttrain_loss: 6.5275\n",
      "Iteration: 238 of 241\ttrain_loss: 6.3941\n",
      "Iteration: 240 of 241\ttrain_loss: 6.2834\n",
      "Iteration: 241 of 241\ttrain_loss: 6.4778\n",
      "Average Score for this Epoch: 6.462885856628418\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 6 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.2143\n",
      "Iteration: 2 of 241\ttrain_loss: 6.1846\n",
      "Iteration: 4 of 241\ttrain_loss: 6.3055\n",
      "Iteration: 6 of 241\ttrain_loss: 6.5991\n",
      "Iteration: 8 of 241\ttrain_loss: 6.1473\n",
      "Iteration: 10 of 241\ttrain_loss: 6.2448\n",
      "Iteration: 12 of 241\ttrain_loss: 6.4106\n",
      "Iteration: 14 of 241\ttrain_loss: 6.2750\n",
      "Iteration: 16 of 241\ttrain_loss: 6.4173\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3691\n",
      "Iteration: 20 of 241\ttrain_loss: 6.3708\n",
      "Iteration: 22 of 241\ttrain_loss: 6.4990\n",
      "Iteration: 24 of 241\ttrain_loss: 6.5787\n",
      "Iteration: 26 of 241\ttrain_loss: 6.2726\n",
      "Iteration: 28 of 241\ttrain_loss: 6.5317\n",
      "Iteration: 30 of 241\ttrain_loss: 6.4633\n",
      "Iteration: 32 of 241\ttrain_loss: 6.3420\n",
      "Iteration: 34 of 241\ttrain_loss: 6.4311\n",
      "Iteration: 36 of 241\ttrain_loss: 6.4829\n",
      "Iteration: 38 of 241\ttrain_loss: 6.5436\n",
      "Iteration: 40 of 241\ttrain_loss: 6.2849\n",
      "Iteration: 42 of 241\ttrain_loss: 6.3634\n",
      "Iteration: 44 of 241\ttrain_loss: 6.3798\n",
      "Iteration: 46 of 241\ttrain_loss: 6.4771\n",
      "Iteration: 48 of 241\ttrain_loss: 6.4786\n",
      "Iteration: 50 of 241\ttrain_loss: 6.4101\n",
      "Iteration: 52 of 241\ttrain_loss: 6.4010\n",
      "Iteration: 54 of 241\ttrain_loss: 6.3205\n",
      "Iteration: 56 of 241\ttrain_loss: 6.6166\n",
      "Iteration: 58 of 241\ttrain_loss: 6.5280\n",
      "Iteration: 60 of 241\ttrain_loss: 6.5999\n",
      "Iteration: 62 of 241\ttrain_loss: 6.4203\n",
      "Iteration: 64 of 241\ttrain_loss: 6.5326\n",
      "Iteration: 66 of 241\ttrain_loss: 6.6435\n",
      "Iteration: 68 of 241\ttrain_loss: 6.3518\n",
      "Iteration: 70 of 241\ttrain_loss: 6.3498\n",
      "Iteration: 72 of 241\ttrain_loss: 6.4436\n",
      "Iteration: 74 of 241\ttrain_loss: 6.4472\n",
      "Iteration: 76 of 241\ttrain_loss: 6.6164\n",
      "Iteration: 78 of 241\ttrain_loss: 6.4691\n",
      "Iteration: 80 of 241\ttrain_loss: 6.5018\n",
      "Iteration: 82 of 241\ttrain_loss: 6.4775\n",
      "Iteration: 84 of 241\ttrain_loss: 6.5171\n",
      "Iteration: 86 of 241\ttrain_loss: 6.3179\n",
      "Iteration: 88 of 241\ttrain_loss: 6.5772\n",
      "Iteration: 90 of 241\ttrain_loss: 6.3847\n",
      "Iteration: 92 of 241\ttrain_loss: 6.6431\n",
      "Iteration: 94 of 241\ttrain_loss: 6.5219\n",
      "Iteration: 96 of 241\ttrain_loss: 6.5259\n",
      "Iteration: 98 of 241\ttrain_loss: 6.2855\n",
      "Iteration: 100 of 241\ttrain_loss: 6.5572\n",
      "Iteration: 102 of 241\ttrain_loss: 6.4730\n",
      "Iteration: 104 of 241\ttrain_loss: 6.3021\n",
      "Iteration: 106 of 241\ttrain_loss: 6.4749\n",
      "Iteration: 108 of 241\ttrain_loss: 6.6093\n",
      "Iteration: 110 of 241\ttrain_loss: 6.6686\n",
      "Iteration: 112 of 241\ttrain_loss: 6.3712\n",
      "Iteration: 114 of 241\ttrain_loss: 6.3525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 116 of 241\ttrain_loss: 6.6013\n",
      "Iteration: 118 of 241\ttrain_loss: 6.4201\n",
      "Iteration: 120 of 241\ttrain_loss: 6.5372\n",
      "Iteration: 122 of 241\ttrain_loss: 6.4126\n",
      "Iteration: 124 of 241\ttrain_loss: 6.4983\n",
      "Iteration: 126 of 241\ttrain_loss: 6.4841\n",
      "Iteration: 128 of 241\ttrain_loss: 6.4577\n",
      "Iteration: 130 of 241\ttrain_loss: 6.2193\n",
      "Iteration: 132 of 241\ttrain_loss: 6.3941\n",
      "Iteration: 134 of 241\ttrain_loss: 6.1471\n",
      "Iteration: 136 of 241\ttrain_loss: 6.4562\n",
      "Iteration: 138 of 241\ttrain_loss: 6.5977\n",
      "Iteration: 140 of 241\ttrain_loss: 6.4343\n",
      "Iteration: 142 of 241\ttrain_loss: 6.5186\n",
      "Iteration: 144 of 241\ttrain_loss: 6.4882\n",
      "Iteration: 146 of 241\ttrain_loss: 6.2909\n",
      "Iteration: 148 of 241\ttrain_loss: 6.5125\n",
      "Iteration: 150 of 241\ttrain_loss: 6.5118\n",
      "Iteration: 152 of 241\ttrain_loss: 6.4241\n",
      "Iteration: 154 of 241\ttrain_loss: 6.5312\n",
      "Iteration: 156 of 241\ttrain_loss: 6.3542\n",
      "Iteration: 158 of 241\ttrain_loss: 6.5052\n",
      "Iteration: 160 of 241\ttrain_loss: 6.6790\n",
      "Iteration: 162 of 241\ttrain_loss: 6.3155\n",
      "Iteration: 164 of 241\ttrain_loss: 6.4917\n",
      "Iteration: 166 of 241\ttrain_loss: 6.3358\n",
      "Iteration: 168 of 241\ttrain_loss: 6.6939\n",
      "Iteration: 170 of 241\ttrain_loss: 6.3140\n",
      "Iteration: 172 of 241\ttrain_loss: 6.3694\n",
      "Iteration: 174 of 241\ttrain_loss: 6.4200\n",
      "Iteration: 176 of 241\ttrain_loss: 6.6017\n",
      "Iteration: 178 of 241\ttrain_loss: 6.4962\n",
      "Iteration: 180 of 241\ttrain_loss: 6.5116\n",
      "Iteration: 182 of 241\ttrain_loss: 6.3496\n",
      "Iteration: 184 of 241\ttrain_loss: 6.3629\n",
      "Iteration: 186 of 241\ttrain_loss: 6.4086\n",
      "Iteration: 188 of 241\ttrain_loss: 6.5348\n",
      "Iteration: 190 of 241\ttrain_loss: 6.2605\n",
      "Iteration: 192 of 241\ttrain_loss: 6.5234\n",
      "Iteration: 194 of 241\ttrain_loss: 6.1873\n",
      "Iteration: 196 of 241\ttrain_loss: 6.3522\n",
      "Iteration: 198 of 241\ttrain_loss: 6.4834\n",
      "Iteration: 200 of 241\ttrain_loss: 6.4242\n",
      "Iteration: 202 of 241\ttrain_loss: 6.4012\n",
      "Iteration: 204 of 241\ttrain_loss: 6.5757\n",
      "Iteration: 206 of 241\ttrain_loss: 6.3853\n",
      "Iteration: 208 of 241\ttrain_loss: 6.5123\n",
      "Iteration: 210 of 241\ttrain_loss: 6.3082\n",
      "Iteration: 212 of 241\ttrain_loss: 6.5270\n",
      "Iteration: 214 of 241\ttrain_loss: 6.2972\n",
      "Iteration: 216 of 241\ttrain_loss: 6.4721\n",
      "Iteration: 218 of 241\ttrain_loss: 6.4372\n",
      "Iteration: 220 of 241\ttrain_loss: 6.4496\n",
      "Iteration: 222 of 241\ttrain_loss: 6.3981\n",
      "Iteration: 224 of 241\ttrain_loss: 6.5584\n",
      "Iteration: 226 of 241\ttrain_loss: 6.3378\n",
      "Iteration: 228 of 241\ttrain_loss: 6.4205\n",
      "Iteration: 230 of 241\ttrain_loss: 6.4709\n",
      "Iteration: 232 of 241\ttrain_loss: 6.3930\n",
      "Iteration: 234 of 241\ttrain_loss: 6.5192\n",
      "Iteration: 236 of 241\ttrain_loss: 6.4572\n",
      "Iteration: 238 of 241\ttrain_loss: 6.6741\n",
      "Iteration: 240 of 241\ttrain_loss: 6.4771\n",
      "Iteration: 241 of 241\ttrain_loss: 6.3895\n",
      "Average Score for this Epoch: 6.43770170211792\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 7 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.3698\n",
      "Iteration: 2 of 241\ttrain_loss: 6.3009\n",
      "Iteration: 4 of 241\ttrain_loss: 6.3258\n",
      "Iteration: 6 of 241\ttrain_loss: 6.0224\n",
      "Iteration: 8 of 241\ttrain_loss: 6.3627\n",
      "Iteration: 10 of 241\ttrain_loss: 6.4760\n",
      "Iteration: 12 of 241\ttrain_loss: 6.2753\n",
      "Iteration: 14 of 241\ttrain_loss: 6.4883\n",
      "Iteration: 16 of 241\ttrain_loss: 6.4681\n",
      "Iteration: 18 of 241\ttrain_loss: 6.4313\n",
      "Iteration: 20 of 241\ttrain_loss: 6.3126\n",
      "Iteration: 22 of 241\ttrain_loss: 6.3710\n",
      "Iteration: 24 of 241\ttrain_loss: 6.3517\n",
      "Iteration: 26 of 241\ttrain_loss: 6.3045\n",
      "Iteration: 28 of 241\ttrain_loss: 6.4363\n",
      "Iteration: 30 of 241\ttrain_loss: 6.5756\n",
      "Iteration: 32 of 241\ttrain_loss: 6.2884\n",
      "Iteration: 34 of 241\ttrain_loss: 6.3625\n",
      "Iteration: 36 of 241\ttrain_loss: 6.1517\n",
      "Iteration: 38 of 241\ttrain_loss: 6.4773\n",
      "Iteration: 40 of 241\ttrain_loss: 6.2129\n",
      "Iteration: 42 of 241\ttrain_loss: 6.5144\n",
      "Iteration: 44 of 241\ttrain_loss: 6.4500\n",
      "Iteration: 46 of 241\ttrain_loss: 6.4513\n",
      "Iteration: 48 of 241\ttrain_loss: 6.3154\n",
      "Iteration: 50 of 241\ttrain_loss: 6.2988\n",
      "Iteration: 52 of 241\ttrain_loss: 6.3338\n",
      "Iteration: 54 of 241\ttrain_loss: 6.3803\n",
      "Iteration: 56 of 241\ttrain_loss: 6.2589\n",
      "Iteration: 58 of 241\ttrain_loss: 6.3325\n",
      "Iteration: 60 of 241\ttrain_loss: 6.3763\n",
      "Iteration: 62 of 241\ttrain_loss: 6.4174\n",
      "Iteration: 64 of 241\ttrain_loss: 6.4653\n",
      "Iteration: 66 of 241\ttrain_loss: 6.3557\n",
      "Iteration: 68 of 241\ttrain_loss: 6.5695\n",
      "Iteration: 70 of 241\ttrain_loss: 6.2812\n",
      "Iteration: 72 of 241\ttrain_loss: 6.2913\n",
      "Iteration: 74 of 241\ttrain_loss: 6.4169\n",
      "Iteration: 76 of 241\ttrain_loss: 6.3210\n",
      "Iteration: 78 of 241\ttrain_loss: 6.3110\n",
      "Iteration: 80 of 241\ttrain_loss: 6.3527\n",
      "Iteration: 82 of 241\ttrain_loss: 6.3551\n",
      "Iteration: 84 of 241\ttrain_loss: 6.5623\n",
      "Iteration: 86 of 241\ttrain_loss: 6.5022\n",
      "Iteration: 88 of 241\ttrain_loss: 6.4610\n",
      "Iteration: 90 of 241\ttrain_loss: 6.5201\n",
      "Iteration: 92 of 241\ttrain_loss: 6.4658\n",
      "Iteration: 94 of 241\ttrain_loss: 6.3395\n",
      "Iteration: 96 of 241\ttrain_loss: 6.4169\n",
      "Iteration: 98 of 241\ttrain_loss: 6.3045\n",
      "Iteration: 100 of 241\ttrain_loss: 6.4323\n",
      "Iteration: 102 of 241\ttrain_loss: 6.3362\n",
      "Iteration: 104 of 241\ttrain_loss: 6.2573\n",
      "Iteration: 106 of 241\ttrain_loss: 6.3495\n",
      "Iteration: 108 of 241\ttrain_loss: 6.3046\n",
      "Iteration: 110 of 241\ttrain_loss: 6.4973\n",
      "Iteration: 112 of 241\ttrain_loss: 6.4807\n",
      "Iteration: 114 of 241\ttrain_loss: 6.3869\n",
      "Iteration: 116 of 241\ttrain_loss: 6.3632\n",
      "Iteration: 118 of 241\ttrain_loss: 6.3293\n",
      "Iteration: 120 of 241\ttrain_loss: 6.4811\n",
      "Iteration: 122 of 241\ttrain_loss: 6.3650\n",
      "Iteration: 124 of 241\ttrain_loss: 6.3988\n",
      "Iteration: 126 of 241\ttrain_loss: 6.3837\n",
      "Iteration: 128 of 241\ttrain_loss: 6.3467\n",
      "Iteration: 130 of 241\ttrain_loss: 6.3964\n",
      "Iteration: 132 of 241\ttrain_loss: 6.3924\n",
      "Iteration: 134 of 241\ttrain_loss: 6.3722\n",
      "Iteration: 136 of 241\ttrain_loss: 6.2346\n",
      "Iteration: 138 of 241\ttrain_loss: 6.3693\n",
      "Iteration: 140 of 241\ttrain_loss: 6.5250\n",
      "Iteration: 142 of 241\ttrain_loss: 6.4747\n",
      "Iteration: 144 of 241\ttrain_loss: 6.4645\n",
      "Iteration: 146 of 241\ttrain_loss: 6.4823\n",
      "Iteration: 148 of 241\ttrain_loss: 6.2705\n",
      "Iteration: 150 of 241\ttrain_loss: 6.2890\n",
      "Iteration: 152 of 241\ttrain_loss: 6.1754\n",
      "Iteration: 154 of 241\ttrain_loss: 6.3999\n",
      "Iteration: 156 of 241\ttrain_loss: 6.4266\n",
      "Iteration: 158 of 241\ttrain_loss: 6.3405\n",
      "Iteration: 160 of 241\ttrain_loss: 6.7124\n",
      "Iteration: 162 of 241\ttrain_loss: 6.4783\n",
      "Iteration: 164 of 241\ttrain_loss: 6.4874\n",
      "Iteration: 166 of 241\ttrain_loss: 6.3770\n",
      "Iteration: 168 of 241\ttrain_loss: 6.3360\n",
      "Iteration: 170 of 241\ttrain_loss: 6.4457\n",
      "Iteration: 172 of 241\ttrain_loss: 6.3931\n",
      "Iteration: 174 of 241\ttrain_loss: 6.4602\n",
      "Iteration: 176 of 241\ttrain_loss: 6.6014\n",
      "Iteration: 178 of 241\ttrain_loss: 6.3470\n",
      "Iteration: 180 of 241\ttrain_loss: 6.2669\n",
      "Iteration: 182 of 241\ttrain_loss: 6.3098\n",
      "Iteration: 184 of 241\ttrain_loss: 6.4855\n",
      "Iteration: 186 of 241\ttrain_loss: 6.5421\n",
      "Iteration: 188 of 241\ttrain_loss: 6.3618\n",
      "Iteration: 190 of 241\ttrain_loss: 6.3749\n",
      "Iteration: 192 of 241\ttrain_loss: 6.5591\n",
      "Iteration: 194 of 241\ttrain_loss: 6.5628\n",
      "Iteration: 196 of 241\ttrain_loss: 6.2704\n",
      "Iteration: 198 of 241\ttrain_loss: 6.5750\n",
      "Iteration: 200 of 241\ttrain_loss: 6.4250\n",
      "Iteration: 202 of 241\ttrain_loss: 6.6461\n",
      "Iteration: 204 of 241\ttrain_loss: 6.4823\n",
      "Iteration: 206 of 241\ttrain_loss: 6.4979\n",
      "Iteration: 208 of 241\ttrain_loss: 6.2982\n",
      "Iteration: 210 of 241\ttrain_loss: 6.4145\n",
      "Iteration: 212 of 241\ttrain_loss: 6.3743\n",
      "Iteration: 214 of 241\ttrain_loss: 6.5967\n",
      "Iteration: 216 of 241\ttrain_loss: 6.3897\n",
      "Iteration: 218 of 241\ttrain_loss: 6.5415\n",
      "Iteration: 220 of 241\ttrain_loss: 6.4289\n",
      "Iteration: 222 of 241\ttrain_loss: 6.3641\n",
      "Iteration: 224 of 241\ttrain_loss: 6.5521\n",
      "Iteration: 226 of 241\ttrain_loss: 6.4104\n",
      "Iteration: 228 of 241\ttrain_loss: 6.3784\n",
      "Iteration: 230 of 241\ttrain_loss: 6.5299\n",
      "Iteration: 232 of 241\ttrain_loss: 6.6912\n",
      "Iteration: 234 of 241\ttrain_loss: 6.6375\n",
      "Iteration: 236 of 241\ttrain_loss: 6.2216\n",
      "Iteration: 238 of 241\ttrain_loss: 6.5859\n",
      "Iteration: 240 of 241\ttrain_loss: 6.6558\n",
      "Iteration: 241 of 241\ttrain_loss: 6.4233\n",
      "Average Score for this Epoch: 6.4083452224731445\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 8 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.2619\n",
      "Iteration: 2 of 241\ttrain_loss: 6.4498\n",
      "Iteration: 4 of 241\ttrain_loss: 6.4808\n",
      "Iteration: 6 of 241\ttrain_loss: 6.2710\n",
      "Iteration: 8 of 241\ttrain_loss: 6.4216\n",
      "Iteration: 10 of 241\ttrain_loss: 6.2629\n",
      "Iteration: 12 of 241\ttrain_loss: 6.1446\n",
      "Iteration: 14 of 241\ttrain_loss: 6.5616\n",
      "Iteration: 16 of 241\ttrain_loss: 6.4278\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20 of 241\ttrain_loss: 6.3804\n",
      "Iteration: 22 of 241\ttrain_loss: 6.2379\n",
      "Iteration: 24 of 241\ttrain_loss: 6.4331\n",
      "Iteration: 26 of 241\ttrain_loss: 6.2848\n",
      "Iteration: 28 of 241\ttrain_loss: 6.3857\n",
      "Iteration: 30 of 241\ttrain_loss: 6.2935\n",
      "Iteration: 32 of 241\ttrain_loss: 6.3626\n",
      "Iteration: 34 of 241\ttrain_loss: 6.5334\n",
      "Iteration: 36 of 241\ttrain_loss: 6.4038\n",
      "Iteration: 38 of 241\ttrain_loss: 6.2873\n",
      "Iteration: 40 of 241\ttrain_loss: 6.3916\n",
      "Iteration: 42 of 241\ttrain_loss: 6.3992\n",
      "Iteration: 44 of 241\ttrain_loss: 6.3719\n",
      "Iteration: 46 of 241\ttrain_loss: 6.3720\n",
      "Iteration: 48 of 241\ttrain_loss: 6.2453\n",
      "Iteration: 50 of 241\ttrain_loss: 6.3529\n",
      "Iteration: 52 of 241\ttrain_loss: 6.3040\n",
      "Iteration: 54 of 241\ttrain_loss: 6.3593\n",
      "Iteration: 56 of 241\ttrain_loss: 6.2718\n",
      "Iteration: 58 of 241\ttrain_loss: 6.5053\n",
      "Iteration: 60 of 241\ttrain_loss: 6.5742\n",
      "Iteration: 62 of 241\ttrain_loss: 6.4977\n",
      "Iteration: 64 of 241\ttrain_loss: 6.3280\n",
      "Iteration: 66 of 241\ttrain_loss: 6.2242\n",
      "Iteration: 68 of 241\ttrain_loss: 6.1995\n",
      "Iteration: 70 of 241\ttrain_loss: 6.2777\n",
      "Iteration: 72 of 241\ttrain_loss: 6.1151\n",
      "Iteration: 74 of 241\ttrain_loss: 6.3647\n",
      "Iteration: 76 of 241\ttrain_loss: 6.3197\n",
      "Iteration: 78 of 241\ttrain_loss: 6.1608\n",
      "Iteration: 80 of 241\ttrain_loss: 6.3620\n",
      "Iteration: 82 of 241\ttrain_loss: 6.1399\n",
      "Iteration: 84 of 241\ttrain_loss: 6.2651\n",
      "Iteration: 86 of 241\ttrain_loss: 6.3738\n",
      "Iteration: 88 of 241\ttrain_loss: 6.3088\n",
      "Iteration: 90 of 241\ttrain_loss: 6.2314\n",
      "Iteration: 92 of 241\ttrain_loss: 6.2311\n",
      "Iteration: 94 of 241\ttrain_loss: 6.3254\n",
      "Iteration: 96 of 241\ttrain_loss: 6.4249\n",
      "Iteration: 98 of 241\ttrain_loss: 6.2651\n",
      "Iteration: 100 of 241\ttrain_loss: 6.4694\n",
      "Iteration: 102 of 241\ttrain_loss: 6.4339\n",
      "Iteration: 104 of 241\ttrain_loss: 6.2202\n",
      "Iteration: 106 of 241\ttrain_loss: 6.2907\n",
      "Iteration: 108 of 241\ttrain_loss: 6.3577\n",
      "Iteration: 110 of 241\ttrain_loss: 6.5011\n",
      "Iteration: 112 of 241\ttrain_loss: 6.4235\n",
      "Iteration: 114 of 241\ttrain_loss: 6.5434\n",
      "Iteration: 116 of 241\ttrain_loss: 6.5980\n",
      "Iteration: 118 of 241\ttrain_loss: 6.3015\n",
      "Iteration: 120 of 241\ttrain_loss: 6.3079\n",
      "Iteration: 122 of 241\ttrain_loss: 6.4998\n",
      "Iteration: 124 of 241\ttrain_loss: 6.5386\n",
      "Iteration: 126 of 241\ttrain_loss: 6.3502\n",
      "Iteration: 128 of 241\ttrain_loss: 6.2668\n",
      "Iteration: 130 of 241\ttrain_loss: 6.5541\n",
      "Iteration: 132 of 241\ttrain_loss: 6.4639\n",
      "Iteration: 134 of 241\ttrain_loss: 6.2766\n",
      "Iteration: 136 of 241\ttrain_loss: 6.4428\n",
      "Iteration: 138 of 241\ttrain_loss: 6.2977\n",
      "Iteration: 140 of 241\ttrain_loss: 6.3646\n",
      "Iteration: 142 of 241\ttrain_loss: 6.5173\n",
      "Iteration: 144 of 241\ttrain_loss: 6.4707\n",
      "Iteration: 146 of 241\ttrain_loss: 6.2968\n",
      "Iteration: 148 of 241\ttrain_loss: 6.2597\n",
      "Iteration: 150 of 241\ttrain_loss: 6.3888\n",
      "Iteration: 152 of 241\ttrain_loss: 6.3792\n",
      "Iteration: 154 of 241\ttrain_loss: 6.3711\n",
      "Iteration: 156 of 241\ttrain_loss: 6.4674\n",
      "Iteration: 158 of 241\ttrain_loss: 6.3123\n",
      "Iteration: 160 of 241\ttrain_loss: 6.2914\n",
      "Iteration: 162 of 241\ttrain_loss: 6.3662\n",
      "Iteration: 164 of 241\ttrain_loss: 6.3491\n",
      "Iteration: 166 of 241\ttrain_loss: 6.5015\n",
      "Iteration: 168 of 241\ttrain_loss: 6.3874\n",
      "Iteration: 170 of 241\ttrain_loss: 6.4860\n",
      "Iteration: 172 of 241\ttrain_loss: 6.3588\n",
      "Iteration: 174 of 241\ttrain_loss: 6.4554\n",
      "Iteration: 176 of 241\ttrain_loss: 6.3804\n",
      "Iteration: 178 of 241\ttrain_loss: 6.3227\n",
      "Iteration: 180 of 241\ttrain_loss: 6.4746\n",
      "Iteration: 182 of 241\ttrain_loss: 6.4254\n",
      "Iteration: 184 of 241\ttrain_loss: 6.2923\n",
      "Iteration: 186 of 241\ttrain_loss: 6.0524\n",
      "Iteration: 188 of 241\ttrain_loss: 6.3374\n",
      "Iteration: 190 of 241\ttrain_loss: 6.3911\n",
      "Iteration: 192 of 241\ttrain_loss: 6.4087\n",
      "Iteration: 194 of 241\ttrain_loss: 6.3480\n",
      "Iteration: 196 of 241\ttrain_loss: 6.2627\n",
      "Iteration: 198 of 241\ttrain_loss: 6.5018\n",
      "Iteration: 200 of 241\ttrain_loss: 6.3608\n",
      "Iteration: 202 of 241\ttrain_loss: 6.3920\n",
      "Iteration: 204 of 241\ttrain_loss: 6.5555\n",
      "Iteration: 206 of 241\ttrain_loss: 6.3155\n",
      "Iteration: 208 of 241\ttrain_loss: 6.3358\n",
      "Iteration: 210 of 241\ttrain_loss: 6.4921\n",
      "Iteration: 212 of 241\ttrain_loss: 6.5897\n",
      "Iteration: 214 of 241\ttrain_loss: 6.3717\n",
      "Iteration: 216 of 241\ttrain_loss: 6.2839\n",
      "Iteration: 218 of 241\ttrain_loss: 6.4330\n",
      "Iteration: 220 of 241\ttrain_loss: 6.4576\n",
      "Iteration: 222 of 241\ttrain_loss: 6.1773\n",
      "Iteration: 224 of 241\ttrain_loss: 6.2996\n",
      "Iteration: 226 of 241\ttrain_loss: 6.4617\n",
      "Iteration: 228 of 241\ttrain_loss: 6.2547\n",
      "Iteration: 230 of 241\ttrain_loss: 6.5161\n",
      "Iteration: 232 of 241\ttrain_loss: 6.6556\n",
      "Iteration: 234 of 241\ttrain_loss: 6.4052\n",
      "Iteration: 236 of 241\ttrain_loss: 6.4183\n",
      "Iteration: 238 of 241\ttrain_loss: 6.4289\n",
      "Iteration: 240 of 241\ttrain_loss: 6.4528\n",
      "Iteration: 241 of 241\ttrain_loss: 6.2261\n",
      "Average Score for this Epoch: 6.358157634735107\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 9 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.2202\n",
      "Iteration: 2 of 241\ttrain_loss: 6.4043\n",
      "Iteration: 4 of 241\ttrain_loss: 6.3670\n",
      "Iteration: 6 of 241\ttrain_loss: 6.3057\n",
      "Iteration: 8 of 241\ttrain_loss: 6.3317\n",
      "Iteration: 10 of 241\ttrain_loss: 6.2932\n",
      "Iteration: 12 of 241\ttrain_loss: 6.2979\n",
      "Iteration: 14 of 241\ttrain_loss: 6.2099\n",
      "Iteration: 16 of 241\ttrain_loss: 6.3783\n",
      "Iteration: 18 of 241\ttrain_loss: 6.2839\n",
      "Iteration: 20 of 241\ttrain_loss: 6.1285\n",
      "Iteration: 22 of 241\ttrain_loss: 6.2994\n",
      "Iteration: 24 of 241\ttrain_loss: 6.0307\n",
      "Iteration: 26 of 241\ttrain_loss: 6.3428\n",
      "Iteration: 28 of 241\ttrain_loss: 6.3674\n",
      "Iteration: 30 of 241\ttrain_loss: 6.2038\n",
      "Iteration: 32 of 241\ttrain_loss: 6.2329\n",
      "Iteration: 34 of 241\ttrain_loss: 6.0395\n",
      "Iteration: 36 of 241\ttrain_loss: 6.3260\n",
      "Iteration: 38 of 241\ttrain_loss: 6.4682\n",
      "Iteration: 40 of 241\ttrain_loss: 6.1119\n",
      "Iteration: 42 of 241\ttrain_loss: 6.0850\n",
      "Iteration: 44 of 241\ttrain_loss: 6.3447\n",
      "Iteration: 46 of 241\ttrain_loss: 6.2393\n",
      "Iteration: 48 of 241\ttrain_loss: 6.1975\n",
      "Iteration: 50 of 241\ttrain_loss: 6.0679\n",
      "Iteration: 52 of 241\ttrain_loss: 6.2151\n",
      "Iteration: 54 of 241\ttrain_loss: 6.2663\n",
      "Iteration: 56 of 241\ttrain_loss: 6.2798\n",
      "Iteration: 58 of 241\ttrain_loss: 6.0277\n",
      "Iteration: 60 of 241\ttrain_loss: 6.4233\n",
      "Iteration: 62 of 241\ttrain_loss: 6.0763\n",
      "Iteration: 64 of 241\ttrain_loss: 5.9950\n",
      "Iteration: 66 of 241\ttrain_loss: 6.2857\n",
      "Iteration: 68 of 241\ttrain_loss: 6.1597\n",
      "Iteration: 70 of 241\ttrain_loss: 6.2456\n",
      "Iteration: 72 of 241\ttrain_loss: 6.2200\n",
      "Iteration: 74 of 241\ttrain_loss: 6.3235\n",
      "Iteration: 76 of 241\ttrain_loss: 6.0113\n",
      "Iteration: 78 of 241\ttrain_loss: 6.4022\n",
      "Iteration: 80 of 241\ttrain_loss: 6.3257\n",
      "Iteration: 82 of 241\ttrain_loss: 6.1972\n",
      "Iteration: 84 of 241\ttrain_loss: 6.1090\n",
      "Iteration: 86 of 241\ttrain_loss: 6.1379\n",
      "Iteration: 88 of 241\ttrain_loss: 6.2534\n",
      "Iteration: 90 of 241\ttrain_loss: 6.2535\n",
      "Iteration: 92 of 241\ttrain_loss: 6.1674\n",
      "Iteration: 94 of 241\ttrain_loss: 6.1098\n",
      "Iteration: 96 of 241\ttrain_loss: 6.3007\n",
      "Iteration: 98 of 241\ttrain_loss: 6.1954\n",
      "Iteration: 100 of 241\ttrain_loss: 6.2204\n",
      "Iteration: 102 of 241\ttrain_loss: 6.1391\n",
      "Iteration: 104 of 241\ttrain_loss: 6.3556\n",
      "Iteration: 106 of 241\ttrain_loss: 5.8994\n",
      "Iteration: 108 of 241\ttrain_loss: 6.3243\n",
      "Iteration: 110 of 241\ttrain_loss: 6.2269\n",
      "Iteration: 112 of 241\ttrain_loss: 6.3253\n",
      "Iteration: 114 of 241\ttrain_loss: 6.1003\n",
      "Iteration: 116 of 241\ttrain_loss: 6.1987\n",
      "Iteration: 118 of 241\ttrain_loss: 6.3842\n",
      "Iteration: 120 of 241\ttrain_loss: 6.0125\n",
      "Iteration: 122 of 241\ttrain_loss: 6.2147\n",
      "Iteration: 124 of 241\ttrain_loss: 6.3682\n",
      "Iteration: 126 of 241\ttrain_loss: 6.0236\n",
      "Iteration: 128 of 241\ttrain_loss: 6.2401\n",
      "Iteration: 130 of 241\ttrain_loss: 6.2605\n",
      "Iteration: 132 of 241\ttrain_loss: 6.1183\n",
      "Iteration: 134 of 241\ttrain_loss: 6.1935\n",
      "Iteration: 136 of 241\ttrain_loss: 6.1791\n",
      "Iteration: 138 of 241\ttrain_loss: 6.3197\n",
      "Iteration: 140 of 241\ttrain_loss: 6.2601\n",
      "Iteration: 142 of 241\ttrain_loss: 5.8847\n",
      "Iteration: 144 of 241\ttrain_loss: 6.0678\n",
      "Iteration: 146 of 241\ttrain_loss: 6.3035\n",
      "Iteration: 148 of 241\ttrain_loss: 6.2341\n",
      "Iteration: 150 of 241\ttrain_loss: 6.1020\n",
      "Iteration: 152 of 241\ttrain_loss: 6.1137\n",
      "Iteration: 154 of 241\ttrain_loss: 6.4345\n",
      "Iteration: 156 of 241\ttrain_loss: 6.1482\n",
      "Iteration: 158 of 241\ttrain_loss: 6.1265\n",
      "Iteration: 160 of 241\ttrain_loss: 6.2847\n",
      "Iteration: 162 of 241\ttrain_loss: 6.1881\n",
      "Iteration: 164 of 241\ttrain_loss: 6.4337\n",
      "Iteration: 166 of 241\ttrain_loss: 6.2483\n",
      "Iteration: 168 of 241\ttrain_loss: 6.3440\n",
      "Iteration: 170 of 241\ttrain_loss: 6.0355\n",
      "Iteration: 172 of 241\ttrain_loss: 6.3008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 174 of 241\ttrain_loss: 6.2379\n",
      "Iteration: 176 of 241\ttrain_loss: 6.1927\n",
      "Iteration: 178 of 241\ttrain_loss: 6.2037\n",
      "Iteration: 180 of 241\ttrain_loss: 6.0135\n",
      "Iteration: 182 of 241\ttrain_loss: 6.3671\n",
      "Iteration: 184 of 241\ttrain_loss: 6.0968\n",
      "Iteration: 186 of 241\ttrain_loss: 6.2738\n",
      "Iteration: 188 of 241\ttrain_loss: 6.3332\n",
      "Iteration: 190 of 241\ttrain_loss: 6.2560\n",
      "Iteration: 192 of 241\ttrain_loss: 6.0771\n",
      "Iteration: 194 of 241\ttrain_loss: 6.2351\n",
      "Iteration: 196 of 241\ttrain_loss: 6.1751\n",
      "Iteration: 198 of 241\ttrain_loss: 6.2406\n",
      "Iteration: 200 of 241\ttrain_loss: 6.2859\n",
      "Iteration: 202 of 241\ttrain_loss: 6.3430\n",
      "Iteration: 204 of 241\ttrain_loss: 6.1767\n",
      "Iteration: 206 of 241\ttrain_loss: 6.1426\n",
      "Iteration: 208 of 241\ttrain_loss: 6.1210\n",
      "Iteration: 210 of 241\ttrain_loss: 5.9972\n",
      "Iteration: 212 of 241\ttrain_loss: 6.2716\n",
      "Iteration: 214 of 241\ttrain_loss: 6.3352\n",
      "Iteration: 216 of 241\ttrain_loss: 6.1369\n",
      "Iteration: 218 of 241\ttrain_loss: 6.2288\n",
      "Iteration: 220 of 241\ttrain_loss: 6.2047\n",
      "Iteration: 222 of 241\ttrain_loss: 6.3898\n",
      "Iteration: 224 of 241\ttrain_loss: 6.1792\n",
      "Iteration: 226 of 241\ttrain_loss: 6.2916\n",
      "Iteration: 228 of 241\ttrain_loss: 6.2338\n",
      "Iteration: 230 of 241\ttrain_loss: 6.2541\n",
      "Iteration: 232 of 241\ttrain_loss: 6.2498\n",
      "Iteration: 234 of 241\ttrain_loss: 6.4005\n",
      "Iteration: 236 of 241\ttrain_loss: 6.1074\n",
      "Iteration: 238 of 241\ttrain_loss: 6.3259\n",
      "Iteration: 240 of 241\ttrain_loss: 6.2570\n",
      "Iteration: 241 of 241\ttrain_loss: 6.1357\n",
      "Average Score for this Epoch: 6.228817939758301\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 10 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.1931\n",
      "Iteration: 2 of 241\ttrain_loss: 6.0350\n",
      "Iteration: 4 of 241\ttrain_loss: 6.1420\n",
      "Iteration: 6 of 241\ttrain_loss: 6.1052\n",
      "Iteration: 8 of 241\ttrain_loss: 5.9384\n",
      "Iteration: 10 of 241\ttrain_loss: 6.1268\n",
      "Iteration: 12 of 241\ttrain_loss: 6.4393\n",
      "Iteration: 14 of 241\ttrain_loss: 6.0459\n",
      "Iteration: 16 of 241\ttrain_loss: 6.1050\n",
      "Iteration: 18 of 241\ttrain_loss: 6.1747\n",
      "Iteration: 20 of 241\ttrain_loss: 6.0374\n",
      "Iteration: 22 of 241\ttrain_loss: 6.1466\n",
      "Iteration: 24 of 241\ttrain_loss: 6.0640\n",
      "Iteration: 26 of 241\ttrain_loss: 6.1055\n",
      "Iteration: 28 of 241\ttrain_loss: 6.0774\n",
      "Iteration: 30 of 241\ttrain_loss: 6.0639\n",
      "Iteration: 32 of 241\ttrain_loss: 6.2216\n",
      "Iteration: 34 of 241\ttrain_loss: 6.1246\n",
      "Iteration: 36 of 241\ttrain_loss: 6.2501\n",
      "Iteration: 38 of 241\ttrain_loss: 6.1529\n",
      "Iteration: 40 of 241\ttrain_loss: 6.1996\n",
      "Iteration: 42 of 241\ttrain_loss: 6.2333\n",
      "Iteration: 44 of 241\ttrain_loss: 6.0299\n",
      "Iteration: 46 of 241\ttrain_loss: 6.2181\n",
      "Iteration: 48 of 241\ttrain_loss: 5.9966\n",
      "Iteration: 50 of 241\ttrain_loss: 6.1109\n",
      "Iteration: 52 of 241\ttrain_loss: 6.3931\n",
      "Iteration: 54 of 241\ttrain_loss: 6.2747\n",
      "Iteration: 56 of 241\ttrain_loss: 6.2703\n",
      "Iteration: 58 of 241\ttrain_loss: 5.9102\n",
      "Iteration: 60 of 241\ttrain_loss: 6.0596\n",
      "Iteration: 62 of 241\ttrain_loss: 6.2782\n",
      "Iteration: 64 of 241\ttrain_loss: 6.2272\n",
      "Iteration: 66 of 241\ttrain_loss: 6.1811\n",
      "Iteration: 68 of 241\ttrain_loss: 6.0507\n",
      "Iteration: 70 of 241\ttrain_loss: 6.1644\n",
      "Iteration: 72 of 241\ttrain_loss: 6.1952\n",
      "Iteration: 74 of 241\ttrain_loss: 6.1872\n",
      "Iteration: 76 of 241\ttrain_loss: 6.1433\n",
      "Iteration: 78 of 241\ttrain_loss: 6.2977\n",
      "Iteration: 80 of 241\ttrain_loss: 6.0206\n",
      "Iteration: 82 of 241\ttrain_loss: 6.0826\n",
      "Iteration: 84 of 241\ttrain_loss: 6.2346\n",
      "Iteration: 86 of 241\ttrain_loss: 6.3977\n",
      "Iteration: 88 of 241\ttrain_loss: 6.1921\n",
      "Iteration: 90 of 241\ttrain_loss: 6.2483\n",
      "Iteration: 92 of 241\ttrain_loss: 6.2161\n",
      "Iteration: 94 of 241\ttrain_loss: 6.3145\n",
      "Iteration: 96 of 241\ttrain_loss: 6.2307\n",
      "Iteration: 98 of 241\ttrain_loss: 5.9667\n",
      "Iteration: 100 of 241\ttrain_loss: 5.9859\n",
      "Iteration: 102 of 241\ttrain_loss: 6.2742\n",
      "Iteration: 104 of 241\ttrain_loss: 6.1909\n",
      "Iteration: 106 of 241\ttrain_loss: 6.1833\n",
      "Iteration: 108 of 241\ttrain_loss: 6.2845\n",
      "Iteration: 110 of 241\ttrain_loss: 6.3635\n",
      "Iteration: 112 of 241\ttrain_loss: 6.3102\n",
      "Iteration: 114 of 241\ttrain_loss: 6.1885\n",
      "Iteration: 116 of 241\ttrain_loss: 6.3977\n",
      "Iteration: 118 of 241\ttrain_loss: 6.0723\n",
      "Iteration: 120 of 241\ttrain_loss: 6.1782\n",
      "Iteration: 122 of 241\ttrain_loss: 6.3445\n",
      "Iteration: 124 of 241\ttrain_loss: 6.1671\n",
      "Iteration: 126 of 241\ttrain_loss: 6.0927\n",
      "Iteration: 128 of 241\ttrain_loss: 6.2656\n",
      "Iteration: 130 of 241\ttrain_loss: 6.2689\n",
      "Iteration: 132 of 241\ttrain_loss: 6.4140\n",
      "Iteration: 134 of 241\ttrain_loss: 6.2104\n",
      "Iteration: 136 of 241\ttrain_loss: 6.2842\n",
      "Iteration: 138 of 241\ttrain_loss: 6.2193\n",
      "Iteration: 140 of 241\ttrain_loss: 6.2606\n",
      "Iteration: 142 of 241\ttrain_loss: 6.1095\n",
      "Iteration: 144 of 241\ttrain_loss: 6.1411\n",
      "Iteration: 146 of 241\ttrain_loss: 6.3239\n",
      "Iteration: 148 of 241\ttrain_loss: 6.3846\n",
      "Iteration: 150 of 241\ttrain_loss: 6.2703\n",
      "Iteration: 152 of 241\ttrain_loss: 6.0077\n",
      "Iteration: 154 of 241\ttrain_loss: 6.2224\n",
      "Iteration: 156 of 241\ttrain_loss: 6.3418\n",
      "Iteration: 158 of 241\ttrain_loss: 6.2485\n",
      "Iteration: 160 of 241\ttrain_loss: 6.2285\n",
      "Iteration: 162 of 241\ttrain_loss: 6.2703\n",
      "Iteration: 164 of 241\ttrain_loss: 6.0365\n",
      "Iteration: 166 of 241\ttrain_loss: 6.0952\n",
      "Iteration: 168 of 241\ttrain_loss: 6.2222\n",
      "Iteration: 170 of 241\ttrain_loss: 6.1361\n",
      "Iteration: 172 of 241\ttrain_loss: 6.2730\n",
      "Iteration: 174 of 241\ttrain_loss: 6.2447\n",
      "Iteration: 176 of 241\ttrain_loss: 6.2641\n",
      "Iteration: 178 of 241\ttrain_loss: 6.1466\n",
      "Iteration: 180 of 241\ttrain_loss: 6.2683\n",
      "Iteration: 182 of 241\ttrain_loss: 6.3540\n",
      "Iteration: 184 of 241\ttrain_loss: 6.0927\n",
      "Iteration: 186 of 241\ttrain_loss: 6.1611\n",
      "Iteration: 188 of 241\ttrain_loss: 6.0702\n",
      "Iteration: 190 of 241\ttrain_loss: 6.2035\n",
      "Iteration: 192 of 241\ttrain_loss: 6.2889\n",
      "Iteration: 194 of 241\ttrain_loss: 6.3094\n",
      "Iteration: 196 of 241\ttrain_loss: 6.0067\n",
      "Iteration: 198 of 241\ttrain_loss: 6.0606\n",
      "Iteration: 200 of 241\ttrain_loss: 6.2961\n",
      "Iteration: 202 of 241\ttrain_loss: 6.3654\n",
      "Iteration: 204 of 241\ttrain_loss: 6.2034\n",
      "Iteration: 206 of 241\ttrain_loss: 6.2645\n",
      "Iteration: 208 of 241\ttrain_loss: 6.2568\n",
      "Iteration: 210 of 241\ttrain_loss: 6.2729\n",
      "Iteration: 212 of 241\ttrain_loss: 6.3737\n",
      "Iteration: 214 of 241\ttrain_loss: 6.3044\n",
      "Iteration: 216 of 241\ttrain_loss: 6.2936\n",
      "Iteration: 218 of 241\ttrain_loss: 6.0914\n",
      "Iteration: 220 of 241\ttrain_loss: 6.1063\n",
      "Iteration: 222 of 241\ttrain_loss: 6.2031\n",
      "Iteration: 224 of 241\ttrain_loss: 6.0998\n",
      "Iteration: 226 of 241\ttrain_loss: 5.9788\n",
      "Iteration: 228 of 241\ttrain_loss: 6.1976\n",
      "Iteration: 230 of 241\ttrain_loss: 6.1279\n",
      "Iteration: 232 of 241\ttrain_loss: 6.0570\n",
      "Iteration: 234 of 241\ttrain_loss: 6.1752\n",
      "Iteration: 236 of 241\ttrain_loss: 6.3079\n",
      "Iteration: 238 of 241\ttrain_loss: 6.2207\n",
      "Iteration: 240 of 241\ttrain_loss: 6.4016\n",
      "Iteration: 241 of 241\ttrain_loss: 6.0483\n",
      "Average Score for this Epoch: 6.190483093261719\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 11 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.1395\n",
      "Iteration: 2 of 241\ttrain_loss: 5.9899\n",
      "Iteration: 4 of 241\ttrain_loss: 6.1506\n",
      "Iteration: 6 of 241\ttrain_loss: 6.3054\n",
      "Iteration: 8 of 241\ttrain_loss: 5.9991\n",
      "Iteration: 10 of 241\ttrain_loss: 6.1452\n",
      "Iteration: 12 of 241\ttrain_loss: 6.2269\n",
      "Iteration: 14 of 241\ttrain_loss: 6.1435\n",
      "Iteration: 16 of 241\ttrain_loss: 6.1847\n",
      "Iteration: 18 of 241\ttrain_loss: 6.3172\n",
      "Iteration: 20 of 241\ttrain_loss: 6.1681\n",
      "Iteration: 22 of 241\ttrain_loss: 6.0641\n",
      "Iteration: 24 of 241\ttrain_loss: 6.1683\n",
      "Iteration: 26 of 241\ttrain_loss: 6.1408\n",
      "Iteration: 28 of 241\ttrain_loss: 6.0696\n",
      "Iteration: 30 of 241\ttrain_loss: 6.1076\n",
      "Iteration: 32 of 241\ttrain_loss: 6.3633\n",
      "Iteration: 34 of 241\ttrain_loss: 6.0820\n",
      "Iteration: 36 of 241\ttrain_loss: 6.1127\n",
      "Iteration: 38 of 241\ttrain_loss: 6.1181\n",
      "Iteration: 40 of 241\ttrain_loss: 6.0878\n",
      "Iteration: 42 of 241\ttrain_loss: 6.1356\n",
      "Iteration: 44 of 241\ttrain_loss: 6.1067\n",
      "Iteration: 46 of 241\ttrain_loss: 6.1704\n",
      "Iteration: 48 of 241\ttrain_loss: 6.0448\n",
      "Iteration: 50 of 241\ttrain_loss: 6.1095\n",
      "Iteration: 52 of 241\ttrain_loss: 6.1262\n",
      "Iteration: 54 of 241\ttrain_loss: 6.1799\n",
      "Iteration: 56 of 241\ttrain_loss: 6.1150\n",
      "Iteration: 58 of 241\ttrain_loss: 6.0223\n",
      "Iteration: 60 of 241\ttrain_loss: 6.1151\n",
      "Iteration: 62 of 241\ttrain_loss: 6.3339\n",
      "Iteration: 64 of 241\ttrain_loss: 6.0178\n",
      "Iteration: 66 of 241\ttrain_loss: 6.0927\n",
      "Iteration: 68 of 241\ttrain_loss: 6.4097\n",
      "Iteration: 70 of 241\ttrain_loss: 6.3001\n",
      "Iteration: 72 of 241\ttrain_loss: 6.1875\n",
      "Iteration: 74 of 241\ttrain_loss: 6.1901\n",
      "Iteration: 76 of 241\ttrain_loss: 6.2126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 78 of 241\ttrain_loss: 6.1466\n",
      "Iteration: 80 of 241\ttrain_loss: 6.0925\n",
      "Iteration: 82 of 241\ttrain_loss: 6.1901\n",
      "Iteration: 84 of 241\ttrain_loss: 6.1870\n",
      "Iteration: 86 of 241\ttrain_loss: 6.2840\n",
      "Iteration: 88 of 241\ttrain_loss: 6.2289\n",
      "Iteration: 90 of 241\ttrain_loss: 6.1051\n",
      "Iteration: 92 of 241\ttrain_loss: 5.9748\n",
      "Iteration: 94 of 241\ttrain_loss: 6.2517\n",
      "Iteration: 96 of 241\ttrain_loss: 6.0406\n",
      "Iteration: 98 of 241\ttrain_loss: 6.2163\n",
      "Iteration: 100 of 241\ttrain_loss: 5.8961\n",
      "Iteration: 102 of 241\ttrain_loss: 6.2681\n",
      "Iteration: 104 of 241\ttrain_loss: 6.1502\n",
      "Iteration: 106 of 241\ttrain_loss: 6.1136\n",
      "Iteration: 108 of 241\ttrain_loss: 6.0285\n",
      "Iteration: 110 of 241\ttrain_loss: 6.2806\n",
      "Iteration: 112 of 241\ttrain_loss: 6.0754\n",
      "Iteration: 114 of 241\ttrain_loss: 6.0982\n",
      "Iteration: 116 of 241\ttrain_loss: 6.0744\n",
      "Iteration: 118 of 241\ttrain_loss: 6.4337\n",
      "Iteration: 120 of 241\ttrain_loss: 6.1967\n",
      "Iteration: 122 of 241\ttrain_loss: 6.0609\n",
      "Iteration: 124 of 241\ttrain_loss: 6.1873\n",
      "Iteration: 126 of 241\ttrain_loss: 6.0364\n",
      "Iteration: 128 of 241\ttrain_loss: 6.2035\n",
      "Iteration: 130 of 241\ttrain_loss: 6.1402\n",
      "Iteration: 132 of 241\ttrain_loss: 6.0839\n",
      "Iteration: 134 of 241\ttrain_loss: 6.2289\n",
      "Iteration: 136 of 241\ttrain_loss: 6.1243\n",
      "Iteration: 138 of 241\ttrain_loss: 6.1213\n",
      "Iteration: 140 of 241\ttrain_loss: 6.1216\n",
      "Iteration: 142 of 241\ttrain_loss: 6.1519\n",
      "Iteration: 144 of 241\ttrain_loss: 6.1879\n",
      "Iteration: 146 of 241\ttrain_loss: 5.9514\n",
      "Iteration: 148 of 241\ttrain_loss: 6.1750\n",
      "Iteration: 150 of 241\ttrain_loss: 6.0591\n",
      "Iteration: 152 of 241\ttrain_loss: 6.0978\n",
      "Iteration: 154 of 241\ttrain_loss: 6.0709\n",
      "Iteration: 156 of 241\ttrain_loss: 6.2237\n",
      "Iteration: 158 of 241\ttrain_loss: 6.1733\n",
      "Iteration: 160 of 241\ttrain_loss: 6.2322\n",
      "Iteration: 162 of 241\ttrain_loss: 6.2521\n",
      "Iteration: 164 of 241\ttrain_loss: 6.3470\n",
      "Iteration: 166 of 241\ttrain_loss: 6.1917\n",
      "Iteration: 168 of 241\ttrain_loss: 6.2621\n",
      "Iteration: 170 of 241\ttrain_loss: 6.1456\n",
      "Iteration: 172 of 241\ttrain_loss: 6.1862\n",
      "Iteration: 174 of 241\ttrain_loss: 6.2243\n",
      "Iteration: 176 of 241\ttrain_loss: 5.9621\n",
      "Iteration: 178 of 241\ttrain_loss: 6.3183\n",
      "Iteration: 180 of 241\ttrain_loss: 6.1363\n",
      "Iteration: 182 of 241\ttrain_loss: 6.0533\n",
      "Iteration: 184 of 241\ttrain_loss: 6.1781\n",
      "Iteration: 186 of 241\ttrain_loss: 6.1899\n",
      "Iteration: 188 of 241\ttrain_loss: 6.0418\n",
      "Iteration: 190 of 241\ttrain_loss: 6.1879\n",
      "Iteration: 192 of 241\ttrain_loss: 6.1453\n",
      "Iteration: 194 of 241\ttrain_loss: 6.0626\n",
      "Iteration: 196 of 241\ttrain_loss: 6.1353\n",
      "Iteration: 198 of 241\ttrain_loss: 6.2629\n",
      "Iteration: 200 of 241\ttrain_loss: 6.1992\n",
      "Iteration: 202 of 241\ttrain_loss: 6.0809\n",
      "Iteration: 204 of 241\ttrain_loss: 6.1255\n",
      "Iteration: 206 of 241\ttrain_loss: 6.2558\n",
      "Iteration: 208 of 241\ttrain_loss: 6.1470\n",
      "Iteration: 210 of 241\ttrain_loss: 6.1983\n",
      "Iteration: 212 of 241\ttrain_loss: 6.2161\n",
      "Iteration: 214 of 241\ttrain_loss: 6.2444\n",
      "Iteration: 216 of 241\ttrain_loss: 5.9783\n",
      "Iteration: 218 of 241\ttrain_loss: 6.0869\n",
      "Iteration: 220 of 241\ttrain_loss: 6.1178\n",
      "Iteration: 222 of 241\ttrain_loss: 6.2613\n",
      "Iteration: 224 of 241\ttrain_loss: 6.1061\n",
      "Iteration: 226 of 241\ttrain_loss: 6.2001\n",
      "Iteration: 228 of 241\ttrain_loss: 6.1582\n",
      "Iteration: 230 of 241\ttrain_loss: 6.2053\n",
      "Iteration: 232 of 241\ttrain_loss: 6.0705\n",
      "Iteration: 234 of 241\ttrain_loss: 6.2228\n",
      "Iteration: 236 of 241\ttrain_loss: 6.1470\n",
      "Iteration: 238 of 241\ttrain_loss: 6.1704\n",
      "Iteration: 240 of 241\ttrain_loss: 6.0927\n",
      "Iteration: 241 of 241\ttrain_loss: 6.1447\n",
      "Average Score for this Epoch: 6.152309894561768\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 12 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.9093\n",
      "Iteration: 2 of 241\ttrain_loss: 5.8705\n",
      "Iteration: 4 of 241\ttrain_loss: 5.8120\n",
      "Iteration: 6 of 241\ttrain_loss: 6.0603\n",
      "Iteration: 8 of 241\ttrain_loss: 6.1270\n",
      "Iteration: 10 of 241\ttrain_loss: 6.1518\n",
      "Iteration: 12 of 241\ttrain_loss: 6.1770\n",
      "Iteration: 14 of 241\ttrain_loss: 6.1371\n",
      "Iteration: 16 of 241\ttrain_loss: 6.0305\n",
      "Iteration: 18 of 241\ttrain_loss: 6.2253\n",
      "Iteration: 20 of 241\ttrain_loss: 5.9662\n",
      "Iteration: 22 of 241\ttrain_loss: 6.1099\n",
      "Iteration: 24 of 241\ttrain_loss: 6.1034\n",
      "Iteration: 26 of 241\ttrain_loss: 5.9786\n",
      "Iteration: 28 of 241\ttrain_loss: 6.1596\n",
      "Iteration: 30 of 241\ttrain_loss: 6.1067\n",
      "Iteration: 32 of 241\ttrain_loss: 6.0919\n",
      "Iteration: 34 of 241\ttrain_loss: 6.2757\n",
      "Iteration: 36 of 241\ttrain_loss: 6.2721\n",
      "Iteration: 38 of 241\ttrain_loss: 5.9970\n",
      "Iteration: 40 of 241\ttrain_loss: 6.0570\n",
      "Iteration: 42 of 241\ttrain_loss: 5.9060\n",
      "Iteration: 44 of 241\ttrain_loss: 6.0443\n",
      "Iteration: 46 of 241\ttrain_loss: 6.1378\n",
      "Iteration: 48 of 241\ttrain_loss: 6.0114\n",
      "Iteration: 50 of 241\ttrain_loss: 6.0167\n",
      "Iteration: 52 of 241\ttrain_loss: 6.1172\n",
      "Iteration: 54 of 241\ttrain_loss: 6.0793\n",
      "Iteration: 56 of 241\ttrain_loss: 6.2206\n",
      "Iteration: 58 of 241\ttrain_loss: 5.9657\n",
      "Iteration: 60 of 241\ttrain_loss: 5.9716\n",
      "Iteration: 62 of 241\ttrain_loss: 6.0623\n",
      "Iteration: 64 of 241\ttrain_loss: 6.0333\n",
      "Iteration: 66 of 241\ttrain_loss: 6.0698\n",
      "Iteration: 68 of 241\ttrain_loss: 6.1674\n",
      "Iteration: 70 of 241\ttrain_loss: 6.1153\n",
      "Iteration: 72 of 241\ttrain_loss: 6.1213\n",
      "Iteration: 74 of 241\ttrain_loss: 6.0808\n",
      "Iteration: 76 of 241\ttrain_loss: 5.9524\n",
      "Iteration: 78 of 241\ttrain_loss: 6.1413\n",
      "Iteration: 80 of 241\ttrain_loss: 6.2973\n",
      "Iteration: 82 of 241\ttrain_loss: 5.9401\n",
      "Iteration: 84 of 241\ttrain_loss: 6.0472\n",
      "Iteration: 86 of 241\ttrain_loss: 6.2300\n",
      "Iteration: 88 of 241\ttrain_loss: 6.2032\n",
      "Iteration: 90 of 241\ttrain_loss: 6.2790\n",
      "Iteration: 92 of 241\ttrain_loss: 6.0402\n",
      "Iteration: 94 of 241\ttrain_loss: 6.0597\n",
      "Iteration: 96 of 241\ttrain_loss: 6.0510\n",
      "Iteration: 98 of 241\ttrain_loss: 6.0172\n",
      "Iteration: 100 of 241\ttrain_loss: 6.0868\n",
      "Iteration: 102 of 241\ttrain_loss: 6.0865\n",
      "Iteration: 104 of 241\ttrain_loss: 6.0157\n",
      "Iteration: 106 of 241\ttrain_loss: 6.0393\n",
      "Iteration: 108 of 241\ttrain_loss: 6.2131\n",
      "Iteration: 110 of 241\ttrain_loss: 5.9980\n",
      "Iteration: 112 of 241\ttrain_loss: 6.0142\n",
      "Iteration: 114 of 241\ttrain_loss: 6.0237\n",
      "Iteration: 116 of 241\ttrain_loss: 6.1438\n",
      "Iteration: 118 of 241\ttrain_loss: 6.2466\n",
      "Iteration: 120 of 241\ttrain_loss: 6.0079\n",
      "Iteration: 122 of 241\ttrain_loss: 6.0335\n",
      "Iteration: 124 of 241\ttrain_loss: 6.1223\n",
      "Iteration: 126 of 241\ttrain_loss: 6.1296\n",
      "Iteration: 128 of 241\ttrain_loss: 6.0534\n",
      "Iteration: 130 of 241\ttrain_loss: 6.3593\n",
      "Iteration: 132 of 241\ttrain_loss: 6.0533\n",
      "Iteration: 134 of 241\ttrain_loss: 6.0923\n",
      "Iteration: 136 of 241\ttrain_loss: 6.2627\n",
      "Iteration: 138 of 241\ttrain_loss: 5.8705\n",
      "Iteration: 140 of 241\ttrain_loss: 5.9016\n",
      "Iteration: 142 of 241\ttrain_loss: 6.1986\n",
      "Iteration: 144 of 241\ttrain_loss: 6.1428\n",
      "Iteration: 146 of 241\ttrain_loss: 5.9040\n",
      "Iteration: 148 of 241\ttrain_loss: 5.9604\n",
      "Iteration: 150 of 241\ttrain_loss: 6.0799\n",
      "Iteration: 152 of 241\ttrain_loss: 6.1658\n",
      "Iteration: 154 of 241\ttrain_loss: 6.2234\n",
      "Iteration: 156 of 241\ttrain_loss: 6.0778\n",
      "Iteration: 158 of 241\ttrain_loss: 6.2131\n",
      "Iteration: 160 of 241\ttrain_loss: 6.2360\n",
      "Iteration: 162 of 241\ttrain_loss: 6.0550\n",
      "Iteration: 164 of 241\ttrain_loss: 6.2706\n",
      "Iteration: 166 of 241\ttrain_loss: 6.0936\n",
      "Iteration: 168 of 241\ttrain_loss: 5.9792\n",
      "Iteration: 170 of 241\ttrain_loss: 6.0193\n",
      "Iteration: 172 of 241\ttrain_loss: 6.0298\n",
      "Iteration: 174 of 241\ttrain_loss: 6.0375\n",
      "Iteration: 176 of 241\ttrain_loss: 6.0832\n",
      "Iteration: 178 of 241\ttrain_loss: 6.2218\n",
      "Iteration: 180 of 241\ttrain_loss: 6.1191\n",
      "Iteration: 182 of 241\ttrain_loss: 6.0505\n",
      "Iteration: 184 of 241\ttrain_loss: 6.0972\n",
      "Iteration: 186 of 241\ttrain_loss: 6.0880\n",
      "Iteration: 188 of 241\ttrain_loss: 6.1663\n",
      "Iteration: 190 of 241\ttrain_loss: 6.3280\n",
      "Iteration: 192 of 241\ttrain_loss: 6.1620\n",
      "Iteration: 194 of 241\ttrain_loss: 6.2328\n",
      "Iteration: 196 of 241\ttrain_loss: 6.0973\n",
      "Iteration: 198 of 241\ttrain_loss: 6.0814\n",
      "Iteration: 200 of 241\ttrain_loss: 6.1328\n",
      "Iteration: 202 of 241\ttrain_loss: 6.1918\n",
      "Iteration: 204 of 241\ttrain_loss: 6.2369\n",
      "Iteration: 206 of 241\ttrain_loss: 6.1413\n",
      "Iteration: 208 of 241\ttrain_loss: 6.2872\n",
      "Iteration: 210 of 241\ttrain_loss: 6.1994\n",
      "Iteration: 212 of 241\ttrain_loss: 6.1692\n",
      "Iteration: 214 of 241\ttrain_loss: 6.1026\n",
      "Iteration: 216 of 241\ttrain_loss: 6.2225\n",
      "Iteration: 218 of 241\ttrain_loss: 6.1153\n",
      "Iteration: 220 of 241\ttrain_loss: 6.2030\n",
      "Iteration: 222 of 241\ttrain_loss: 6.3393\n",
      "Iteration: 224 of 241\ttrain_loss: 6.2432\n",
      "Iteration: 226 of 241\ttrain_loss: 6.1976\n",
      "Iteration: 228 of 241\ttrain_loss: 6.1798\n",
      "Iteration: 230 of 241\ttrain_loss: 6.0605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 232 of 241\ttrain_loss: 5.9830\n",
      "Iteration: 234 of 241\ttrain_loss: 6.1291\n",
      "Iteration: 236 of 241\ttrain_loss: 5.9949\n",
      "Iteration: 238 of 241\ttrain_loss: 6.1983\n",
      "Iteration: 240 of 241\ttrain_loss: 6.2048\n",
      "Iteration: 241 of 241\ttrain_loss: 5.9709\n",
      "Average Score for this Epoch: 6.111618995666504\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 13 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.8352\n",
      "Iteration: 2 of 241\ttrain_loss: 6.1023\n",
      "Iteration: 4 of 241\ttrain_loss: 5.9668\n",
      "Iteration: 6 of 241\ttrain_loss: 6.1132\n",
      "Iteration: 8 of 241\ttrain_loss: 6.1951\n",
      "Iteration: 10 of 241\ttrain_loss: 5.9359\n",
      "Iteration: 12 of 241\ttrain_loss: 5.9358\n",
      "Iteration: 14 of 241\ttrain_loss: 5.8975\n",
      "Iteration: 16 of 241\ttrain_loss: 6.0981\n",
      "Iteration: 18 of 241\ttrain_loss: 6.0721\n",
      "Iteration: 20 of 241\ttrain_loss: 6.1114\n",
      "Iteration: 22 of 241\ttrain_loss: 6.0381\n",
      "Iteration: 24 of 241\ttrain_loss: 6.0269\n",
      "Iteration: 26 of 241\ttrain_loss: 5.9885\n",
      "Iteration: 28 of 241\ttrain_loss: 5.8161\n",
      "Iteration: 30 of 241\ttrain_loss: 6.2416\n",
      "Iteration: 32 of 241\ttrain_loss: 6.1525\n",
      "Iteration: 34 of 241\ttrain_loss: 6.0015\n",
      "Iteration: 36 of 241\ttrain_loss: 6.0426\n",
      "Iteration: 38 of 241\ttrain_loss: 5.8584\n",
      "Iteration: 40 of 241\ttrain_loss: 6.1234\n",
      "Iteration: 42 of 241\ttrain_loss: 6.0909\n",
      "Iteration: 44 of 241\ttrain_loss: 6.1171\n",
      "Iteration: 46 of 241\ttrain_loss: 6.0302\n",
      "Iteration: 48 of 241\ttrain_loss: 6.0286\n",
      "Iteration: 50 of 241\ttrain_loss: 5.9603\n",
      "Iteration: 52 of 241\ttrain_loss: 5.9837\n",
      "Iteration: 54 of 241\ttrain_loss: 6.0649\n",
      "Iteration: 56 of 241\ttrain_loss: 5.8777\n",
      "Iteration: 58 of 241\ttrain_loss: 6.0326\n",
      "Iteration: 60 of 241\ttrain_loss: 5.9990\n",
      "Iteration: 62 of 241\ttrain_loss: 5.7594\n",
      "Iteration: 64 of 241\ttrain_loss: 5.9971\n",
      "Iteration: 66 of 241\ttrain_loss: 6.0079\n",
      "Iteration: 68 of 241\ttrain_loss: 5.9653\n",
      "Iteration: 70 of 241\ttrain_loss: 5.9260\n",
      "Iteration: 72 of 241\ttrain_loss: 5.8946\n",
      "Iteration: 74 of 241\ttrain_loss: 5.9485\n",
      "Iteration: 76 of 241\ttrain_loss: 5.9537\n",
      "Iteration: 78 of 241\ttrain_loss: 6.0193\n",
      "Iteration: 80 of 241\ttrain_loss: 6.1434\n",
      "Iteration: 82 of 241\ttrain_loss: 6.0323\n",
      "Iteration: 84 of 241\ttrain_loss: 6.1564\n",
      "Iteration: 86 of 241\ttrain_loss: 6.0551\n",
      "Iteration: 88 of 241\ttrain_loss: 6.0612\n",
      "Iteration: 90 of 241\ttrain_loss: 6.0504\n",
      "Iteration: 92 of 241\ttrain_loss: 6.0546\n",
      "Iteration: 94 of 241\ttrain_loss: 5.8675\n",
      "Iteration: 96 of 241\ttrain_loss: 5.9797\n",
      "Iteration: 98 of 241\ttrain_loss: 5.9490\n",
      "Iteration: 100 of 241\ttrain_loss: 6.0221\n",
      "Iteration: 102 of 241\ttrain_loss: 6.1791\n",
      "Iteration: 104 of 241\ttrain_loss: 5.9265\n",
      "Iteration: 106 of 241\ttrain_loss: 5.8439\n",
      "Iteration: 108 of 241\ttrain_loss: 6.1529\n",
      "Iteration: 110 of 241\ttrain_loss: 5.9814\n",
      "Iteration: 112 of 241\ttrain_loss: 6.0056\n",
      "Iteration: 114 of 241\ttrain_loss: 5.9832\n",
      "Iteration: 116 of 241\ttrain_loss: 6.1382\n",
      "Iteration: 118 of 241\ttrain_loss: 6.0720\n",
      "Iteration: 120 of 241\ttrain_loss: 6.0227\n",
      "Iteration: 122 of 241\ttrain_loss: 6.0437\n",
      "Iteration: 124 of 241\ttrain_loss: 5.9504\n",
      "Iteration: 126 of 241\ttrain_loss: 6.0659\n",
      "Iteration: 128 of 241\ttrain_loss: 5.9724\n",
      "Iteration: 130 of 241\ttrain_loss: 6.0173\n",
      "Iteration: 132 of 241\ttrain_loss: 6.1885\n",
      "Iteration: 134 of 241\ttrain_loss: 6.0950\n",
      "Iteration: 136 of 241\ttrain_loss: 6.2580\n",
      "Iteration: 138 of 241\ttrain_loss: 6.0102\n",
      "Iteration: 140 of 241\ttrain_loss: 5.9916\n",
      "Iteration: 142 of 241\ttrain_loss: 5.9664\n",
      "Iteration: 144 of 241\ttrain_loss: 6.0899\n",
      "Iteration: 146 of 241\ttrain_loss: 6.1952\n",
      "Iteration: 148 of 241\ttrain_loss: 5.9419\n",
      "Iteration: 150 of 241\ttrain_loss: 6.3222\n",
      "Iteration: 152 of 241\ttrain_loss: 6.0568\n",
      "Iteration: 154 of 241\ttrain_loss: 6.2396\n",
      "Iteration: 156 of 241\ttrain_loss: 6.0826\n",
      "Iteration: 158 of 241\ttrain_loss: 6.1433\n",
      "Iteration: 160 of 241\ttrain_loss: 6.0929\n",
      "Iteration: 162 of 241\ttrain_loss: 6.0656\n",
      "Iteration: 164 of 241\ttrain_loss: 6.0548\n",
      "Iteration: 166 of 241\ttrain_loss: 6.1169\n",
      "Iteration: 168 of 241\ttrain_loss: 5.9536\n",
      "Iteration: 170 of 241\ttrain_loss: 6.0663\n",
      "Iteration: 172 of 241\ttrain_loss: 6.1327\n",
      "Iteration: 174 of 241\ttrain_loss: 5.9279\n",
      "Iteration: 176 of 241\ttrain_loss: 6.1201\n",
      "Iteration: 178 of 241\ttrain_loss: 6.2491\n",
      "Iteration: 180 of 241\ttrain_loss: 6.2330\n",
      "Iteration: 182 of 241\ttrain_loss: 5.9920\n",
      "Iteration: 184 of 241\ttrain_loss: 6.0961\n",
      "Iteration: 186 of 241\ttrain_loss: 5.9302\n",
      "Iteration: 188 of 241\ttrain_loss: 6.1337\n",
      "Iteration: 190 of 241\ttrain_loss: 6.0357\n",
      "Iteration: 192 of 241\ttrain_loss: 6.0844\n",
      "Iteration: 194 of 241\ttrain_loss: 6.0228\n",
      "Iteration: 196 of 241\ttrain_loss: 5.9086\n",
      "Iteration: 198 of 241\ttrain_loss: 6.0349\n",
      "Iteration: 200 of 241\ttrain_loss: 5.9902\n",
      "Iteration: 202 of 241\ttrain_loss: 6.0969\n",
      "Iteration: 204 of 241\ttrain_loss: 6.0837\n",
      "Iteration: 206 of 241\ttrain_loss: 5.9494\n",
      "Iteration: 208 of 241\ttrain_loss: 5.9263\n",
      "Iteration: 210 of 241\ttrain_loss: 6.1527\n",
      "Iteration: 212 of 241\ttrain_loss: 5.8917\n",
      "Iteration: 214 of 241\ttrain_loss: 6.0059\n",
      "Iteration: 216 of 241\ttrain_loss: 6.1512\n",
      "Iteration: 218 of 241\ttrain_loss: 6.2133\n",
      "Iteration: 220 of 241\ttrain_loss: 5.9260\n",
      "Iteration: 222 of 241\ttrain_loss: 6.0693\n",
      "Iteration: 224 of 241\ttrain_loss: 6.1159\n",
      "Iteration: 226 of 241\ttrain_loss: 6.1154\n",
      "Iteration: 228 of 241\ttrain_loss: 6.0183\n",
      "Iteration: 230 of 241\ttrain_loss: 6.0849\n",
      "Iteration: 232 of 241\ttrain_loss: 5.9781\n",
      "Iteration: 234 of 241\ttrain_loss: 5.9863\n",
      "Iteration: 236 of 241\ttrain_loss: 6.0789\n",
      "Iteration: 238 of 241\ttrain_loss: 6.0607\n",
      "Iteration: 240 of 241\ttrain_loss: 6.1913\n",
      "Iteration: 241 of 241\ttrain_loss: 6.0810\n",
      "Average Score for this Epoch: 6.052431106567383\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 14 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 6.0968\n",
      "Iteration: 2 of 241\ttrain_loss: 5.8266\n",
      "Iteration: 4 of 241\ttrain_loss: 6.0683\n",
      "Iteration: 6 of 241\ttrain_loss: 5.9227\n",
      "Iteration: 8 of 241\ttrain_loss: 6.0205\n",
      "Iteration: 10 of 241\ttrain_loss: 5.8716\n",
      "Iteration: 12 of 241\ttrain_loss: 5.9733\n",
      "Iteration: 14 of 241\ttrain_loss: 5.7206\n",
      "Iteration: 16 of 241\ttrain_loss: 5.9833\n",
      "Iteration: 18 of 241\ttrain_loss: 6.0004\n",
      "Iteration: 20 of 241\ttrain_loss: 5.9601\n",
      "Iteration: 22 of 241\ttrain_loss: 5.9151\n",
      "Iteration: 24 of 241\ttrain_loss: 5.7986\n",
      "Iteration: 26 of 241\ttrain_loss: 5.9096\n",
      "Iteration: 28 of 241\ttrain_loss: 5.9586\n",
      "Iteration: 30 of 241\ttrain_loss: 5.9834\n",
      "Iteration: 32 of 241\ttrain_loss: 5.9586\n",
      "Iteration: 34 of 241\ttrain_loss: 5.8274\n",
      "Iteration: 36 of 241\ttrain_loss: 5.8759\n",
      "Iteration: 38 of 241\ttrain_loss: 5.8417\n",
      "Iteration: 40 of 241\ttrain_loss: 5.9599\n",
      "Iteration: 42 of 241\ttrain_loss: 6.0413\n",
      "Iteration: 44 of 241\ttrain_loss: 5.8496\n",
      "Iteration: 46 of 241\ttrain_loss: 6.1093\n",
      "Iteration: 48 of 241\ttrain_loss: 5.9934\n",
      "Iteration: 50 of 241\ttrain_loss: 6.0273\n",
      "Iteration: 52 of 241\ttrain_loss: 6.0771\n",
      "Iteration: 54 of 241\ttrain_loss: 5.9880\n",
      "Iteration: 56 of 241\ttrain_loss: 6.0718\n",
      "Iteration: 58 of 241\ttrain_loss: 5.9537\n",
      "Iteration: 60 of 241\ttrain_loss: 5.8656\n",
      "Iteration: 62 of 241\ttrain_loss: 6.0358\n",
      "Iteration: 64 of 241\ttrain_loss: 5.8365\n",
      "Iteration: 66 of 241\ttrain_loss: 5.9317\n",
      "Iteration: 68 of 241\ttrain_loss: 5.9705\n",
      "Iteration: 70 of 241\ttrain_loss: 5.8596\n",
      "Iteration: 72 of 241\ttrain_loss: 6.0356\n",
      "Iteration: 74 of 241\ttrain_loss: 5.8584\n",
      "Iteration: 76 of 241\ttrain_loss: 6.0749\n",
      "Iteration: 78 of 241\ttrain_loss: 6.1279\n",
      "Iteration: 80 of 241\ttrain_loss: 5.7055\n",
      "Iteration: 82 of 241\ttrain_loss: 6.1007\n",
      "Iteration: 84 of 241\ttrain_loss: 6.0845\n",
      "Iteration: 86 of 241\ttrain_loss: 5.7657\n",
      "Iteration: 88 of 241\ttrain_loss: 5.9621\n",
      "Iteration: 90 of 241\ttrain_loss: 5.9500\n",
      "Iteration: 92 of 241\ttrain_loss: 5.8896\n",
      "Iteration: 94 of 241\ttrain_loss: 5.8716\n",
      "Iteration: 96 of 241\ttrain_loss: 5.9455\n",
      "Iteration: 98 of 241\ttrain_loss: 6.1748\n",
      "Iteration: 100 of 241\ttrain_loss: 5.8789\n",
      "Iteration: 102 of 241\ttrain_loss: 6.0791\n",
      "Iteration: 104 of 241\ttrain_loss: 6.1303\n",
      "Iteration: 106 of 241\ttrain_loss: 5.9212\n",
      "Iteration: 108 of 241\ttrain_loss: 6.1073\n",
      "Iteration: 110 of 241\ttrain_loss: 6.0370\n",
      "Iteration: 112 of 241\ttrain_loss: 5.9776\n",
      "Iteration: 114 of 241\ttrain_loss: 5.9937\n",
      "Iteration: 116 of 241\ttrain_loss: 6.0197\n",
      "Iteration: 118 of 241\ttrain_loss: 6.1963\n",
      "Iteration: 120 of 241\ttrain_loss: 5.9894\n",
      "Iteration: 122 of 241\ttrain_loss: 5.9959\n",
      "Iteration: 124 of 241\ttrain_loss: 5.9314\n",
      "Iteration: 126 of 241\ttrain_loss: 6.0412\n",
      "Iteration: 128 of 241\ttrain_loss: 6.0744\n",
      "Iteration: 130 of 241\ttrain_loss: 6.0497\n",
      "Iteration: 132 of 241\ttrain_loss: 6.1150\n",
      "Iteration: 134 of 241\ttrain_loss: 6.0377\n",
      "Iteration: 136 of 241\ttrain_loss: 6.2357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 138 of 241\ttrain_loss: 6.0412\n",
      "Iteration: 140 of 241\ttrain_loss: 6.2387\n",
      "Iteration: 142 of 241\ttrain_loss: 5.9587\n",
      "Iteration: 144 of 241\ttrain_loss: 5.8550\n",
      "Iteration: 146 of 241\ttrain_loss: 6.0460\n",
      "Iteration: 148 of 241\ttrain_loss: 6.0248\n",
      "Iteration: 150 of 241\ttrain_loss: 6.0206\n",
      "Iteration: 152 of 241\ttrain_loss: 6.0860\n",
      "Iteration: 154 of 241\ttrain_loss: 5.9044\n",
      "Iteration: 156 of 241\ttrain_loss: 6.0589\n",
      "Iteration: 158 of 241\ttrain_loss: 5.8576\n",
      "Iteration: 160 of 241\ttrain_loss: 5.9648\n",
      "Iteration: 162 of 241\ttrain_loss: 6.0011\n",
      "Iteration: 164 of 241\ttrain_loss: 6.1442\n",
      "Iteration: 166 of 241\ttrain_loss: 6.2306\n",
      "Iteration: 168 of 241\ttrain_loss: 6.0039\n",
      "Iteration: 170 of 241\ttrain_loss: 6.1178\n",
      "Iteration: 172 of 241\ttrain_loss: 6.0017\n",
      "Iteration: 174 of 241\ttrain_loss: 6.1184\n",
      "Iteration: 176 of 241\ttrain_loss: 6.1203\n",
      "Iteration: 178 of 241\ttrain_loss: 5.9906\n",
      "Iteration: 180 of 241\ttrain_loss: 6.1343\n",
      "Iteration: 182 of 241\ttrain_loss: 6.0989\n",
      "Iteration: 184 of 241\ttrain_loss: 6.0227\n",
      "Iteration: 186 of 241\ttrain_loss: 6.1135\n",
      "Iteration: 188 of 241\ttrain_loss: 6.0235\n",
      "Iteration: 190 of 241\ttrain_loss: 6.1143\n",
      "Iteration: 192 of 241\ttrain_loss: 5.8555\n",
      "Iteration: 194 of 241\ttrain_loss: 6.2166\n",
      "Iteration: 196 of 241\ttrain_loss: 5.9121\n",
      "Iteration: 198 of 241\ttrain_loss: 6.0907\n",
      "Iteration: 200 of 241\ttrain_loss: 6.1416\n",
      "Iteration: 202 of 241\ttrain_loss: 5.9848\n",
      "Iteration: 204 of 241\ttrain_loss: 5.8667\n",
      "Iteration: 206 of 241\ttrain_loss: 6.1324\n",
      "Iteration: 208 of 241\ttrain_loss: 5.9919\n",
      "Iteration: 210 of 241\ttrain_loss: 5.9024\n",
      "Iteration: 212 of 241\ttrain_loss: 6.1023\n",
      "Iteration: 214 of 241\ttrain_loss: 6.0914\n",
      "Iteration: 216 of 241\ttrain_loss: 6.0201\n",
      "Iteration: 218 of 241\ttrain_loss: 6.0254\n",
      "Iteration: 220 of 241\ttrain_loss: 5.9697\n",
      "Iteration: 222 of 241\ttrain_loss: 6.1842\n",
      "Iteration: 224 of 241\ttrain_loss: 5.9165\n",
      "Iteration: 226 of 241\ttrain_loss: 5.9045\n",
      "Iteration: 228 of 241\ttrain_loss: 6.1039\n",
      "Iteration: 230 of 241\ttrain_loss: 5.9295\n",
      "Iteration: 232 of 241\ttrain_loss: 6.0006\n",
      "Iteration: 234 of 241\ttrain_loss: 6.1442\n",
      "Iteration: 236 of 241\ttrain_loss: 6.1562\n",
      "Iteration: 238 of 241\ttrain_loss: 5.9969\n",
      "Iteration: 240 of 241\ttrain_loss: 6.0034\n",
      "Iteration: 241 of 241\ttrain_loss: 6.1038\n",
      "Average Score for this Epoch: 6.013090133666992\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 15 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.9743\n",
      "Iteration: 2 of 241\ttrain_loss: 5.8171\n",
      "Iteration: 4 of 241\ttrain_loss: 6.0354\n",
      "Iteration: 6 of 241\ttrain_loss: 5.9326\n",
      "Iteration: 8 of 241\ttrain_loss: 6.0520\n",
      "Iteration: 10 of 241\ttrain_loss: 5.6561\n",
      "Iteration: 12 of 241\ttrain_loss: 5.9320\n",
      "Iteration: 14 of 241\ttrain_loss: 5.6912\n",
      "Iteration: 16 of 241\ttrain_loss: 6.0231\n",
      "Iteration: 18 of 241\ttrain_loss: 5.7944\n",
      "Iteration: 20 of 241\ttrain_loss: 5.8233\n",
      "Iteration: 22 of 241\ttrain_loss: 6.0091\n",
      "Iteration: 24 of 241\ttrain_loss: 5.9901\n",
      "Iteration: 26 of 241\ttrain_loss: 5.8704\n",
      "Iteration: 28 of 241\ttrain_loss: 5.9282\n",
      "Iteration: 30 of 241\ttrain_loss: 5.9361\n",
      "Iteration: 32 of 241\ttrain_loss: 5.9929\n",
      "Iteration: 34 of 241\ttrain_loss: 5.8167\n",
      "Iteration: 36 of 241\ttrain_loss: 6.1019\n",
      "Iteration: 38 of 241\ttrain_loss: 5.8356\n",
      "Iteration: 40 of 241\ttrain_loss: 6.0951\n",
      "Iteration: 42 of 241\ttrain_loss: 6.0215\n",
      "Iteration: 44 of 241\ttrain_loss: 5.8557\n",
      "Iteration: 46 of 241\ttrain_loss: 6.0755\n",
      "Iteration: 48 of 241\ttrain_loss: 5.8101\n",
      "Iteration: 50 of 241\ttrain_loss: 5.9792\n",
      "Iteration: 52 of 241\ttrain_loss: 5.8446\n",
      "Iteration: 54 of 241\ttrain_loss: 5.9235\n",
      "Iteration: 56 of 241\ttrain_loss: 5.9439\n",
      "Iteration: 58 of 241\ttrain_loss: 5.8369\n",
      "Iteration: 60 of 241\ttrain_loss: 6.0485\n",
      "Iteration: 62 of 241\ttrain_loss: 5.9281\n",
      "Iteration: 64 of 241\ttrain_loss: 6.1338\n",
      "Iteration: 66 of 241\ttrain_loss: 5.9742\n",
      "Iteration: 68 of 241\ttrain_loss: 5.9500\n",
      "Iteration: 70 of 241\ttrain_loss: 5.8667\n",
      "Iteration: 72 of 241\ttrain_loss: 5.9404\n",
      "Iteration: 74 of 241\ttrain_loss: 5.9789\n",
      "Iteration: 76 of 241\ttrain_loss: 5.9215\n",
      "Iteration: 78 of 241\ttrain_loss: 5.8734\n",
      "Iteration: 80 of 241\ttrain_loss: 5.8792\n",
      "Iteration: 82 of 241\ttrain_loss: 5.9922\n",
      "Iteration: 84 of 241\ttrain_loss: 6.0609\n",
      "Iteration: 86 of 241\ttrain_loss: 5.9614\n",
      "Iteration: 88 of 241\ttrain_loss: 5.8766\n",
      "Iteration: 90 of 241\ttrain_loss: 6.0718\n",
      "Iteration: 92 of 241\ttrain_loss: 6.1317\n",
      "Iteration: 94 of 241\ttrain_loss: 5.9751\n",
      "Iteration: 96 of 241\ttrain_loss: 6.0605\n",
      "Iteration: 98 of 241\ttrain_loss: 6.1130\n",
      "Iteration: 100 of 241\ttrain_loss: 6.1308\n",
      "Iteration: 102 of 241\ttrain_loss: 5.8957\n",
      "Iteration: 104 of 241\ttrain_loss: 6.1580\n",
      "Iteration: 106 of 241\ttrain_loss: 5.9981\n",
      "Iteration: 108 of 241\ttrain_loss: 6.2611\n",
      "Iteration: 110 of 241\ttrain_loss: 6.0337\n",
      "Iteration: 112 of 241\ttrain_loss: 5.9800\n",
      "Iteration: 114 of 241\ttrain_loss: 5.8897\n",
      "Iteration: 116 of 241\ttrain_loss: 6.0686\n",
      "Iteration: 118 of 241\ttrain_loss: 6.0081\n",
      "Iteration: 120 of 241\ttrain_loss: 5.8796\n",
      "Iteration: 122 of 241\ttrain_loss: 5.8862\n",
      "Iteration: 124 of 241\ttrain_loss: 6.0143\n",
      "Iteration: 126 of 241\ttrain_loss: 6.2752\n",
      "Iteration: 128 of 241\ttrain_loss: 5.9751\n",
      "Iteration: 130 of 241\ttrain_loss: 5.8788\n",
      "Iteration: 132 of 241\ttrain_loss: 5.9400\n",
      "Iteration: 134 of 241\ttrain_loss: 6.1027\n",
      "Iteration: 136 of 241\ttrain_loss: 6.0979\n",
      "Iteration: 138 of 241\ttrain_loss: 5.9230\n",
      "Iteration: 140 of 241\ttrain_loss: 5.9939\n",
      "Iteration: 142 of 241\ttrain_loss: 6.1051\n",
      "Iteration: 144 of 241\ttrain_loss: 6.1465\n",
      "Iteration: 146 of 241\ttrain_loss: 6.1363\n",
      "Iteration: 148 of 241\ttrain_loss: 5.8679\n",
      "Iteration: 150 of 241\ttrain_loss: 5.9229\n",
      "Iteration: 152 of 241\ttrain_loss: 5.9162\n",
      "Iteration: 154 of 241\ttrain_loss: 5.9158\n",
      "Iteration: 156 of 241\ttrain_loss: 6.0194\n",
      "Iteration: 158 of 241\ttrain_loss: 6.0677\n",
      "Iteration: 160 of 241\ttrain_loss: 5.9835\n",
      "Iteration: 162 of 241\ttrain_loss: 5.8729\n",
      "Iteration: 164 of 241\ttrain_loss: 5.8497\n",
      "Iteration: 166 of 241\ttrain_loss: 6.1429\n",
      "Iteration: 168 of 241\ttrain_loss: 6.1399\n",
      "Iteration: 170 of 241\ttrain_loss: 6.0474\n",
      "Iteration: 172 of 241\ttrain_loss: 5.8653\n",
      "Iteration: 174 of 241\ttrain_loss: 5.8606\n",
      "Iteration: 176 of 241\ttrain_loss: 6.0818\n",
      "Iteration: 178 of 241\ttrain_loss: 5.9069\n",
      "Iteration: 180 of 241\ttrain_loss: 6.1159\n",
      "Iteration: 182 of 241\ttrain_loss: 5.9604\n",
      "Iteration: 184 of 241\ttrain_loss: 5.8622\n",
      "Iteration: 186 of 241\ttrain_loss: 5.9721\n",
      "Iteration: 188 of 241\ttrain_loss: 5.9012\n",
      "Iteration: 190 of 241\ttrain_loss: 6.0897\n",
      "Iteration: 192 of 241\ttrain_loss: 5.9332\n",
      "Iteration: 194 of 241\ttrain_loss: 5.8931\n",
      "Iteration: 196 of 241\ttrain_loss: 5.9907\n",
      "Iteration: 198 of 241\ttrain_loss: 5.9562\n",
      "Iteration: 200 of 241\ttrain_loss: 6.1318\n",
      "Iteration: 202 of 241\ttrain_loss: 6.0272\n",
      "Iteration: 204 of 241\ttrain_loss: 5.9247\n",
      "Iteration: 206 of 241\ttrain_loss: 6.0167\n",
      "Iteration: 208 of 241\ttrain_loss: 6.0215\n",
      "Iteration: 210 of 241\ttrain_loss: 5.9109\n",
      "Iteration: 212 of 241\ttrain_loss: 6.1071\n",
      "Iteration: 214 of 241\ttrain_loss: 6.1468\n",
      "Iteration: 216 of 241\ttrain_loss: 5.9457\n",
      "Iteration: 218 of 241\ttrain_loss: 5.9222\n",
      "Iteration: 220 of 241\ttrain_loss: 5.9890\n",
      "Iteration: 222 of 241\ttrain_loss: 6.0123\n",
      "Iteration: 224 of 241\ttrain_loss: 6.2246\n",
      "Iteration: 226 of 241\ttrain_loss: 6.0905\n",
      "Iteration: 228 of 241\ttrain_loss: 5.9387\n",
      "Iteration: 230 of 241\ttrain_loss: 5.8681\n",
      "Iteration: 232 of 241\ttrain_loss: 6.2074\n",
      "Iteration: 234 of 241\ttrain_loss: 6.0049\n",
      "Iteration: 236 of 241\ttrain_loss: 5.9114\n",
      "Iteration: 238 of 241\ttrain_loss: 6.0506\n",
      "Iteration: 240 of 241\ttrain_loss: 6.0826\n",
      "Iteration: 241 of 241\ttrain_loss: 5.9362\n",
      "Average Score for this Epoch: 5.980996608734131\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 16 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.6876\n",
      "Iteration: 2 of 241\ttrain_loss: 5.7572\n",
      "Iteration: 4 of 241\ttrain_loss: 5.8983\n",
      "Iteration: 6 of 241\ttrain_loss: 5.8468\n",
      "Iteration: 8 of 241\ttrain_loss: 5.6964\n",
      "Iteration: 10 of 241\ttrain_loss: 5.8511\n",
      "Iteration: 12 of 241\ttrain_loss: 5.8681\n",
      "Iteration: 14 of 241\ttrain_loss: 6.0221\n",
      "Iteration: 16 of 241\ttrain_loss: 5.8724\n",
      "Iteration: 18 of 241\ttrain_loss: 5.8531\n",
      "Iteration: 20 of 241\ttrain_loss: 5.8902\n",
      "Iteration: 22 of 241\ttrain_loss: 5.7566\n",
      "Iteration: 24 of 241\ttrain_loss: 5.7097\n",
      "Iteration: 26 of 241\ttrain_loss: 5.9103\n",
      "Iteration: 28 of 241\ttrain_loss: 5.9037\n",
      "Iteration: 30 of 241\ttrain_loss: 5.8337\n",
      "Iteration: 32 of 241\ttrain_loss: 5.8992\n",
      "Iteration: 34 of 241\ttrain_loss: 6.0199\n",
      "Iteration: 36 of 241\ttrain_loss: 5.8774\n",
      "Iteration: 38 of 241\ttrain_loss: 5.7217\n",
      "Iteration: 40 of 241\ttrain_loss: 6.0256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 42 of 241\ttrain_loss: 5.7616\n",
      "Iteration: 44 of 241\ttrain_loss: 5.7856\n",
      "Iteration: 46 of 241\ttrain_loss: 5.7713\n",
      "Iteration: 48 of 241\ttrain_loss: 5.8272\n",
      "Iteration: 50 of 241\ttrain_loss: 5.9689\n",
      "Iteration: 52 of 241\ttrain_loss: 5.7522\n",
      "Iteration: 54 of 241\ttrain_loss: 5.6418\n",
      "Iteration: 56 of 241\ttrain_loss: 6.0094\n",
      "Iteration: 58 of 241\ttrain_loss: 5.9171\n",
      "Iteration: 60 of 241\ttrain_loss: 5.8986\n",
      "Iteration: 62 of 241\ttrain_loss: 5.8170\n",
      "Iteration: 64 of 241\ttrain_loss: 5.8811\n",
      "Iteration: 66 of 241\ttrain_loss: 5.9026\n",
      "Iteration: 68 of 241\ttrain_loss: 5.9120\n",
      "Iteration: 70 of 241\ttrain_loss: 5.9395\n",
      "Iteration: 72 of 241\ttrain_loss: 5.8357\n",
      "Iteration: 74 of 241\ttrain_loss: 6.0035\n",
      "Iteration: 76 of 241\ttrain_loss: 5.9841\n",
      "Iteration: 78 of 241\ttrain_loss: 5.9805\n",
      "Iteration: 80 of 241\ttrain_loss: 6.0030\n",
      "Iteration: 82 of 241\ttrain_loss: 5.7711\n",
      "Iteration: 84 of 241\ttrain_loss: 5.8304\n",
      "Iteration: 86 of 241\ttrain_loss: 5.7571\n",
      "Iteration: 88 of 241\ttrain_loss: 5.8434\n",
      "Iteration: 90 of 241\ttrain_loss: 6.1406\n",
      "Iteration: 92 of 241\ttrain_loss: 5.8730\n",
      "Iteration: 94 of 241\ttrain_loss: 5.8187\n",
      "Iteration: 96 of 241\ttrain_loss: 5.9794\n",
      "Iteration: 98 of 241\ttrain_loss: 6.0238\n",
      "Iteration: 100 of 241\ttrain_loss: 6.0312\n",
      "Iteration: 102 of 241\ttrain_loss: 5.8900\n",
      "Iteration: 104 of 241\ttrain_loss: 5.8826\n",
      "Iteration: 106 of 241\ttrain_loss: 5.9271\n",
      "Iteration: 108 of 241\ttrain_loss: 5.7699\n",
      "Iteration: 110 of 241\ttrain_loss: 5.9661\n",
      "Iteration: 112 of 241\ttrain_loss: 5.7859\n",
      "Iteration: 114 of 241\ttrain_loss: 5.9567\n",
      "Iteration: 116 of 241\ttrain_loss: 6.0225\n",
      "Iteration: 118 of 241\ttrain_loss: 6.0134\n",
      "Iteration: 120 of 241\ttrain_loss: 5.8085\n",
      "Iteration: 122 of 241\ttrain_loss: 6.1751\n",
      "Iteration: 124 of 241\ttrain_loss: 5.9489\n",
      "Iteration: 126 of 241\ttrain_loss: 5.9803\n",
      "Iteration: 128 of 241\ttrain_loss: 5.9448\n",
      "Iteration: 130 of 241\ttrain_loss: 5.9589\n",
      "Iteration: 132 of 241\ttrain_loss: 6.0506\n",
      "Iteration: 134 of 241\ttrain_loss: 5.9159\n",
      "Iteration: 136 of 241\ttrain_loss: 5.6682\n",
      "Iteration: 138 of 241\ttrain_loss: 5.8151\n",
      "Iteration: 140 of 241\ttrain_loss: 5.9284\n",
      "Iteration: 142 of 241\ttrain_loss: 5.9192\n",
      "Iteration: 144 of 241\ttrain_loss: 5.9299\n",
      "Iteration: 146 of 241\ttrain_loss: 6.0125\n",
      "Iteration: 148 of 241\ttrain_loss: 6.1327\n",
      "Iteration: 150 of 241\ttrain_loss: 6.1687\n",
      "Iteration: 152 of 241\ttrain_loss: 6.0584\n",
      "Iteration: 154 of 241\ttrain_loss: 5.9564\n",
      "Iteration: 156 of 241\ttrain_loss: 5.9394\n",
      "Iteration: 158 of 241\ttrain_loss: 5.9106\n",
      "Iteration: 160 of 241\ttrain_loss: 5.9684\n",
      "Iteration: 162 of 241\ttrain_loss: 5.8771\n",
      "Iteration: 164 of 241\ttrain_loss: 5.9305\n",
      "Iteration: 166 of 241\ttrain_loss: 6.1040\n",
      "Iteration: 168 of 241\ttrain_loss: 6.0480\n",
      "Iteration: 170 of 241\ttrain_loss: 5.9395\n",
      "Iteration: 172 of 241\ttrain_loss: 5.9574\n",
      "Iteration: 174 of 241\ttrain_loss: 6.0283\n",
      "Iteration: 176 of 241\ttrain_loss: 6.1288\n",
      "Iteration: 178 of 241\ttrain_loss: 5.8838\n",
      "Iteration: 180 of 241\ttrain_loss: 6.0803\n",
      "Iteration: 182 of 241\ttrain_loss: 5.9869\n",
      "Iteration: 184 of 241\ttrain_loss: 5.8743\n",
      "Iteration: 186 of 241\ttrain_loss: 5.9211\n",
      "Iteration: 188 of 241\ttrain_loss: 5.9708\n",
      "Iteration: 190 of 241\ttrain_loss: 5.8956\n",
      "Iteration: 192 of 241\ttrain_loss: 6.1566\n",
      "Iteration: 194 of 241\ttrain_loss: 5.8230\n",
      "Iteration: 196 of 241\ttrain_loss: 6.2267\n",
      "Iteration: 198 of 241\ttrain_loss: 6.0776\n",
      "Iteration: 200 of 241\ttrain_loss: 6.0450\n",
      "Iteration: 202 of 241\ttrain_loss: 5.8752\n",
      "Iteration: 204 of 241\ttrain_loss: 6.0272\n",
      "Iteration: 206 of 241\ttrain_loss: 5.9980\n",
      "Iteration: 208 of 241\ttrain_loss: 5.9949\n",
      "Iteration: 210 of 241\ttrain_loss: 6.0142\n",
      "Iteration: 212 of 241\ttrain_loss: 5.8431\n",
      "Iteration: 214 of 241\ttrain_loss: 6.0090\n",
      "Iteration: 216 of 241\ttrain_loss: 5.9577\n",
      "Iteration: 218 of 241\ttrain_loss: 6.0142\n",
      "Iteration: 220 of 241\ttrain_loss: 5.9845\n",
      "Iteration: 222 of 241\ttrain_loss: 5.8805\n",
      "Iteration: 224 of 241\ttrain_loss: 6.1044\n",
      "Iteration: 226 of 241\ttrain_loss: 5.9404\n",
      "Iteration: 228 of 241\ttrain_loss: 5.9974\n",
      "Iteration: 230 of 241\ttrain_loss: 6.1330\n",
      "Iteration: 232 of 241\ttrain_loss: 5.8732\n",
      "Iteration: 234 of 241\ttrain_loss: 6.0716\n",
      "Iteration: 236 of 241\ttrain_loss: 6.0669\n",
      "Iteration: 238 of 241\ttrain_loss: 6.0336\n",
      "Iteration: 240 of 241\ttrain_loss: 6.1416\n",
      "Iteration: 241 of 241\ttrain_loss: 5.8998\n",
      "Average Score for this Epoch: 5.931425094604492\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 17 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.6971\n",
      "Iteration: 2 of 241\ttrain_loss: 5.7757\n",
      "Iteration: 4 of 241\ttrain_loss: 5.7870\n",
      "Iteration: 6 of 241\ttrain_loss: 5.6745\n",
      "Iteration: 8 of 241\ttrain_loss: 5.8338\n",
      "Iteration: 10 of 241\ttrain_loss: 5.9320\n",
      "Iteration: 12 of 241\ttrain_loss: 5.8802\n",
      "Iteration: 14 of 241\ttrain_loss: 5.9430\n",
      "Iteration: 16 of 241\ttrain_loss: 5.7301\n",
      "Iteration: 18 of 241\ttrain_loss: 5.6723\n",
      "Iteration: 20 of 241\ttrain_loss: 5.7629\n",
      "Iteration: 22 of 241\ttrain_loss: 5.8341\n",
      "Iteration: 24 of 241\ttrain_loss: 5.8811\n",
      "Iteration: 26 of 241\ttrain_loss: 5.8598\n",
      "Iteration: 28 of 241\ttrain_loss: 5.8672\n",
      "Iteration: 30 of 241\ttrain_loss: 6.0284\n",
      "Iteration: 32 of 241\ttrain_loss: 5.8193\n",
      "Iteration: 34 of 241\ttrain_loss: 5.8261\n",
      "Iteration: 36 of 241\ttrain_loss: 5.8836\n",
      "Iteration: 38 of 241\ttrain_loss: 5.7870\n",
      "Iteration: 40 of 241\ttrain_loss: 5.9110\n",
      "Iteration: 42 of 241\ttrain_loss: 5.8089\n",
      "Iteration: 44 of 241\ttrain_loss: 5.7092\n",
      "Iteration: 46 of 241\ttrain_loss: 5.7511\n",
      "Iteration: 48 of 241\ttrain_loss: 5.7478\n",
      "Iteration: 50 of 241\ttrain_loss: 5.9251\n",
      "Iteration: 52 of 241\ttrain_loss: 5.7430\n",
      "Iteration: 54 of 241\ttrain_loss: 5.5875\n",
      "Iteration: 56 of 241\ttrain_loss: 5.7988\n",
      "Iteration: 58 of 241\ttrain_loss: 5.8348\n",
      "Iteration: 60 of 241\ttrain_loss: 5.7470\n",
      "Iteration: 62 of 241\ttrain_loss: 5.8892\n",
      "Iteration: 64 of 241\ttrain_loss: 5.9500\n",
      "Iteration: 66 of 241\ttrain_loss: 5.8123\n",
      "Iteration: 68 of 241\ttrain_loss: 5.8006\n",
      "Iteration: 70 of 241\ttrain_loss: 5.7227\n",
      "Iteration: 72 of 241\ttrain_loss: 5.7120\n",
      "Iteration: 74 of 241\ttrain_loss: 5.5093\n",
      "Iteration: 76 of 241\ttrain_loss: 5.7575\n",
      "Iteration: 78 of 241\ttrain_loss: 5.8899\n",
      "Iteration: 80 of 241\ttrain_loss: 5.7557\n",
      "Iteration: 82 of 241\ttrain_loss: 5.6184\n",
      "Iteration: 84 of 241\ttrain_loss: 5.5991\n",
      "Iteration: 86 of 241\ttrain_loss: 5.8747\n",
      "Iteration: 88 of 241\ttrain_loss: 5.8948\n",
      "Iteration: 90 of 241\ttrain_loss: 5.7313\n",
      "Iteration: 92 of 241\ttrain_loss: 5.9280\n",
      "Iteration: 94 of 241\ttrain_loss: 5.8888\n",
      "Iteration: 96 of 241\ttrain_loss: 5.7295\n",
      "Iteration: 98 of 241\ttrain_loss: 5.8109\n",
      "Iteration: 100 of 241\ttrain_loss: 5.8266\n",
      "Iteration: 102 of 241\ttrain_loss: 5.6651\n",
      "Iteration: 104 of 241\ttrain_loss: 5.6883\n",
      "Iteration: 106 of 241\ttrain_loss: 5.7046\n",
      "Iteration: 108 of 241\ttrain_loss: 5.7937\n",
      "Iteration: 110 of 241\ttrain_loss: 5.9515\n",
      "Iteration: 112 of 241\ttrain_loss: 5.6767\n",
      "Iteration: 114 of 241\ttrain_loss: 5.8202\n",
      "Iteration: 116 of 241\ttrain_loss: 5.7423\n",
      "Iteration: 118 of 241\ttrain_loss: 5.7495\n",
      "Iteration: 120 of 241\ttrain_loss: 5.6022\n",
      "Iteration: 122 of 241\ttrain_loss: 5.7076\n",
      "Iteration: 124 of 241\ttrain_loss: 5.7606\n",
      "Iteration: 126 of 241\ttrain_loss: 5.6859\n",
      "Iteration: 128 of 241\ttrain_loss: 5.5826\n",
      "Iteration: 130 of 241\ttrain_loss: 5.7614\n",
      "Iteration: 132 of 241\ttrain_loss: 5.9845\n",
      "Iteration: 134 of 241\ttrain_loss: 5.8078\n",
      "Iteration: 136 of 241\ttrain_loss: 5.8495\n",
      "Iteration: 138 of 241\ttrain_loss: 5.6518\n",
      "Iteration: 140 of 241\ttrain_loss: 5.7136\n",
      "Iteration: 142 of 241\ttrain_loss: 5.7673\n",
      "Iteration: 144 of 241\ttrain_loss: 5.8142\n",
      "Iteration: 146 of 241\ttrain_loss: 5.7331\n",
      "Iteration: 148 of 241\ttrain_loss: 5.8886\n",
      "Iteration: 150 of 241\ttrain_loss: 5.8782\n",
      "Iteration: 152 of 241\ttrain_loss: 5.7310\n",
      "Iteration: 154 of 241\ttrain_loss: 5.6443\n",
      "Iteration: 156 of 241\ttrain_loss: 5.8458\n",
      "Iteration: 158 of 241\ttrain_loss: 5.7206\n",
      "Iteration: 160 of 241\ttrain_loss: 5.7401\n",
      "Iteration: 162 of 241\ttrain_loss: 5.9128\n",
      "Iteration: 164 of 241\ttrain_loss: 5.5978\n",
      "Iteration: 166 of 241\ttrain_loss: 5.8432\n",
      "Iteration: 168 of 241\ttrain_loss: 5.7997\n",
      "Iteration: 170 of 241\ttrain_loss: 5.7639\n",
      "Iteration: 172 of 241\ttrain_loss: 5.8398\n",
      "Iteration: 174 of 241\ttrain_loss: 5.7837\n",
      "Iteration: 176 of 241\ttrain_loss: 5.7769\n",
      "Iteration: 178 of 241\ttrain_loss: 5.6557\n",
      "Iteration: 180 of 241\ttrain_loss: 5.8956\n",
      "Iteration: 182 of 241\ttrain_loss: 5.9814\n",
      "Iteration: 184 of 241\ttrain_loss: 5.8435\n",
      "Iteration: 186 of 241\ttrain_loss: 5.8366\n",
      "Iteration: 188 of 241\ttrain_loss: 5.8222\n",
      "Iteration: 190 of 241\ttrain_loss: 5.8229\n",
      "Iteration: 192 of 241\ttrain_loss: 5.8956\n",
      "Iteration: 194 of 241\ttrain_loss: 5.7185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 196 of 241\ttrain_loss: 5.8500\n",
      "Iteration: 198 of 241\ttrain_loss: 5.9203\n",
      "Iteration: 200 of 241\ttrain_loss: 5.8685\n",
      "Iteration: 202 of 241\ttrain_loss: 5.8519\n",
      "Iteration: 204 of 241\ttrain_loss: 6.0222\n",
      "Iteration: 206 of 241\ttrain_loss: 5.6779\n",
      "Iteration: 208 of 241\ttrain_loss: 5.6217\n",
      "Iteration: 210 of 241\ttrain_loss: 5.8005\n",
      "Iteration: 212 of 241\ttrain_loss: 5.7903\n",
      "Iteration: 214 of 241\ttrain_loss: 5.9597\n",
      "Iteration: 216 of 241\ttrain_loss: 5.7080\n",
      "Iteration: 218 of 241\ttrain_loss: 5.8755\n",
      "Iteration: 220 of 241\ttrain_loss: 5.8217\n",
      "Iteration: 222 of 241\ttrain_loss: 5.6870\n",
      "Iteration: 224 of 241\ttrain_loss: 6.0348\n",
      "Iteration: 226 of 241\ttrain_loss: 5.9359\n",
      "Iteration: 228 of 241\ttrain_loss: 5.8811\n",
      "Iteration: 230 of 241\ttrain_loss: 5.8926\n",
      "Iteration: 232 of 241\ttrain_loss: 5.9071\n",
      "Iteration: 234 of 241\ttrain_loss: 5.5764\n",
      "Iteration: 236 of 241\ttrain_loss: 5.8312\n",
      "Iteration: 238 of 241\ttrain_loss: 5.6687\n",
      "Iteration: 240 of 241\ttrain_loss: 5.8892\n",
      "Iteration: 241 of 241\ttrain_loss: 5.8139\n",
      "Average Score for this Epoch: 5.795536041259766\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 18 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.7789\n",
      "Iteration: 2 of 241\ttrain_loss: 5.6466\n",
      "Iteration: 4 of 241\ttrain_loss: 5.6688\n",
      "Iteration: 6 of 241\ttrain_loss: 5.3732\n",
      "Iteration: 8 of 241\ttrain_loss: 5.4069\n",
      "Iteration: 10 of 241\ttrain_loss: 5.5954\n",
      "Iteration: 12 of 241\ttrain_loss: 5.4561\n",
      "Iteration: 14 of 241\ttrain_loss: 5.6206\n",
      "Iteration: 16 of 241\ttrain_loss: 5.5433\n",
      "Iteration: 18 of 241\ttrain_loss: 5.5796\n",
      "Iteration: 20 of 241\ttrain_loss: 5.5402\n",
      "Iteration: 22 of 241\ttrain_loss: 5.7773\n",
      "Iteration: 24 of 241\ttrain_loss: 5.4779\n",
      "Iteration: 26 of 241\ttrain_loss: 5.5166\n",
      "Iteration: 28 of 241\ttrain_loss: 5.3729\n",
      "Iteration: 30 of 241\ttrain_loss: 5.4228\n",
      "Iteration: 32 of 241\ttrain_loss: 5.6465\n",
      "Iteration: 34 of 241\ttrain_loss: 5.5655\n",
      "Iteration: 36 of 241\ttrain_loss: 5.5382\n",
      "Iteration: 38 of 241\ttrain_loss: 5.5786\n",
      "Iteration: 40 of 241\ttrain_loss: 5.4800\n",
      "Iteration: 42 of 241\ttrain_loss: 5.7827\n",
      "Iteration: 44 of 241\ttrain_loss: 5.4902\n",
      "Iteration: 46 of 241\ttrain_loss: 5.7863\n",
      "Iteration: 48 of 241\ttrain_loss: 5.7052\n",
      "Iteration: 50 of 241\ttrain_loss: 5.7790\n",
      "Iteration: 52 of 241\ttrain_loss: 5.5091\n",
      "Iteration: 54 of 241\ttrain_loss: 5.5898\n",
      "Iteration: 56 of 241\ttrain_loss: 5.6189\n",
      "Iteration: 58 of 241\ttrain_loss: 5.7456\n",
      "Iteration: 60 of 241\ttrain_loss: 5.7751\n",
      "Iteration: 62 of 241\ttrain_loss: 5.6718\n",
      "Iteration: 64 of 241\ttrain_loss: 5.4881\n",
      "Iteration: 66 of 241\ttrain_loss: 5.4704\n",
      "Iteration: 68 of 241\ttrain_loss: 5.8041\n",
      "Iteration: 70 of 241\ttrain_loss: 5.7379\n",
      "Iteration: 72 of 241\ttrain_loss: 5.5839\n",
      "Iteration: 74 of 241\ttrain_loss: 5.6077\n",
      "Iteration: 76 of 241\ttrain_loss: 5.5918\n",
      "Iteration: 78 of 241\ttrain_loss: 5.6237\n",
      "Iteration: 80 of 241\ttrain_loss: 5.5881\n",
      "Iteration: 82 of 241\ttrain_loss: 5.6478\n",
      "Iteration: 84 of 241\ttrain_loss: 5.7323\n",
      "Iteration: 86 of 241\ttrain_loss: 5.7944\n",
      "Iteration: 88 of 241\ttrain_loss: 5.6431\n",
      "Iteration: 90 of 241\ttrain_loss: 5.4479\n",
      "Iteration: 92 of 241\ttrain_loss: 5.5853\n",
      "Iteration: 94 of 241\ttrain_loss: 5.5908\n",
      "Iteration: 96 of 241\ttrain_loss: 5.7916\n",
      "Iteration: 98 of 241\ttrain_loss: 5.8579\n",
      "Iteration: 100 of 241\ttrain_loss: 5.6403\n",
      "Iteration: 102 of 241\ttrain_loss: 5.8940\n",
      "Iteration: 104 of 241\ttrain_loss: 5.3301\n",
      "Iteration: 106 of 241\ttrain_loss: 5.7245\n",
      "Iteration: 108 of 241\ttrain_loss: 5.7005\n",
      "Iteration: 110 of 241\ttrain_loss: 5.8809\n",
      "Iteration: 112 of 241\ttrain_loss: 5.5801\n",
      "Iteration: 114 of 241\ttrain_loss: 5.8512\n",
      "Iteration: 116 of 241\ttrain_loss: 5.6378\n",
      "Iteration: 118 of 241\ttrain_loss: 5.3803\n",
      "Iteration: 120 of 241\ttrain_loss: 5.6035\n",
      "Iteration: 122 of 241\ttrain_loss: 5.9461\n",
      "Iteration: 124 of 241\ttrain_loss: 5.7624\n",
      "Iteration: 126 of 241\ttrain_loss: 5.6983\n",
      "Iteration: 128 of 241\ttrain_loss: 5.5016\n",
      "Iteration: 130 of 241\ttrain_loss: 5.5241\n",
      "Iteration: 132 of 241\ttrain_loss: 5.8976\n",
      "Iteration: 134 of 241\ttrain_loss: 5.5611\n",
      "Iteration: 136 of 241\ttrain_loss: 5.8763\n",
      "Iteration: 138 of 241\ttrain_loss: 5.6855\n",
      "Iteration: 140 of 241\ttrain_loss: 5.6026\n",
      "Iteration: 142 of 241\ttrain_loss: 5.6764\n",
      "Iteration: 144 of 241\ttrain_loss: 5.8548\n",
      "Iteration: 146 of 241\ttrain_loss: 5.6391\n",
      "Iteration: 148 of 241\ttrain_loss: 5.9036\n",
      "Iteration: 150 of 241\ttrain_loss: 5.7741\n",
      "Iteration: 152 of 241\ttrain_loss: 5.7616\n",
      "Iteration: 154 of 241\ttrain_loss: 5.7206\n",
      "Iteration: 156 of 241\ttrain_loss: 5.6579\n",
      "Iteration: 158 of 241\ttrain_loss: 5.6628\n",
      "Iteration: 160 of 241\ttrain_loss: 5.7104\n",
      "Iteration: 162 of 241\ttrain_loss: 5.7395\n",
      "Iteration: 164 of 241\ttrain_loss: 5.7736\n",
      "Iteration: 166 of 241\ttrain_loss: 5.8366\n",
      "Iteration: 168 of 241\ttrain_loss: 5.6409\n",
      "Iteration: 170 of 241\ttrain_loss: 5.7055\n",
      "Iteration: 172 of 241\ttrain_loss: 5.5370\n",
      "Iteration: 174 of 241\ttrain_loss: 5.5982\n",
      "Iteration: 176 of 241\ttrain_loss: 5.7619\n",
      "Iteration: 178 of 241\ttrain_loss: 5.7846\n",
      "Iteration: 180 of 241\ttrain_loss: 5.9021\n",
      "Iteration: 182 of 241\ttrain_loss: 5.7798\n",
      "Iteration: 184 of 241\ttrain_loss: 5.7735\n",
      "Iteration: 186 of 241\ttrain_loss: 5.8455\n",
      "Iteration: 188 of 241\ttrain_loss: 5.5548\n",
      "Iteration: 190 of 241\ttrain_loss: 5.7931\n",
      "Iteration: 192 of 241\ttrain_loss: 5.7727\n",
      "Iteration: 194 of 241\ttrain_loss: 5.7981\n",
      "Iteration: 196 of 241\ttrain_loss: 5.7784\n",
      "Iteration: 198 of 241\ttrain_loss: 5.8217\n",
      "Iteration: 200 of 241\ttrain_loss: 5.7479\n",
      "Iteration: 202 of 241\ttrain_loss: 5.7867\n",
      "Iteration: 204 of 241\ttrain_loss: 5.7341\n",
      "Iteration: 206 of 241\ttrain_loss: 5.8204\n",
      "Iteration: 208 of 241\ttrain_loss: 5.6930\n",
      "Iteration: 210 of 241\ttrain_loss: 5.6882\n",
      "Iteration: 212 of 241\ttrain_loss: 5.8008\n",
      "Iteration: 214 of 241\ttrain_loss: 5.7341\n",
      "Iteration: 216 of 241\ttrain_loss: 5.6803\n",
      "Iteration: 218 of 241\ttrain_loss: 5.7169\n",
      "Iteration: 220 of 241\ttrain_loss: 5.7281\n",
      "Iteration: 222 of 241\ttrain_loss: 5.7135\n",
      "Iteration: 224 of 241\ttrain_loss: 5.8303\n",
      "Iteration: 226 of 241\ttrain_loss: 5.6823\n",
      "Iteration: 228 of 241\ttrain_loss: 6.0237\n",
      "Iteration: 230 of 241\ttrain_loss: 5.8346\n",
      "Iteration: 232 of 241\ttrain_loss: 5.7672\n",
      "Iteration: 234 of 241\ttrain_loss: 5.7967\n",
      "Iteration: 236 of 241\ttrain_loss: 5.7872\n",
      "Iteration: 238 of 241\ttrain_loss: 5.8699\n",
      "Iteration: 240 of 241\ttrain_loss: 5.6798\n",
      "Iteration: 241 of 241\ttrain_loss: 5.5110\n",
      "Average Score for this Epoch: 5.679352283477783\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 19 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.5379\n",
      "Iteration: 2 of 241\ttrain_loss: 5.6005\n",
      "Iteration: 4 of 241\ttrain_loss: 5.3251\n",
      "Iteration: 6 of 241\ttrain_loss: 5.4620\n",
      "Iteration: 8 of 241\ttrain_loss: 5.5053\n",
      "Iteration: 10 of 241\ttrain_loss: 5.5547\n",
      "Iteration: 12 of 241\ttrain_loss: 5.5976\n",
      "Iteration: 14 of 241\ttrain_loss: 5.3962\n",
      "Iteration: 16 of 241\ttrain_loss: 5.4369\n",
      "Iteration: 18 of 241\ttrain_loss: 5.8404\n",
      "Iteration: 20 of 241\ttrain_loss: 5.5645\n",
      "Iteration: 22 of 241\ttrain_loss: 5.4090\n",
      "Iteration: 24 of 241\ttrain_loss: 5.6181\n",
      "Iteration: 26 of 241\ttrain_loss: 5.5158\n",
      "Iteration: 28 of 241\ttrain_loss: 5.5621\n",
      "Iteration: 30 of 241\ttrain_loss: 5.4050\n",
      "Iteration: 32 of 241\ttrain_loss: 5.3992\n",
      "Iteration: 34 of 241\ttrain_loss: 5.5862\n",
      "Iteration: 36 of 241\ttrain_loss: 5.5147\n",
      "Iteration: 38 of 241\ttrain_loss: 5.4950\n",
      "Iteration: 40 of 241\ttrain_loss: 5.5909\n",
      "Iteration: 42 of 241\ttrain_loss: 5.5475\n",
      "Iteration: 44 of 241\ttrain_loss: 5.6154\n",
      "Iteration: 46 of 241\ttrain_loss: 5.5441\n",
      "Iteration: 48 of 241\ttrain_loss: 5.5924\n",
      "Iteration: 50 of 241\ttrain_loss: 5.5742\n",
      "Iteration: 52 of 241\ttrain_loss: 5.5293\n",
      "Iteration: 54 of 241\ttrain_loss: 5.4477\n",
      "Iteration: 56 of 241\ttrain_loss: 5.6874\n",
      "Iteration: 58 of 241\ttrain_loss: 5.6352\n",
      "Iteration: 60 of 241\ttrain_loss: 5.5531\n",
      "Iteration: 62 of 241\ttrain_loss: 5.5974\n",
      "Iteration: 64 of 241\ttrain_loss: 5.7025\n",
      "Iteration: 66 of 241\ttrain_loss: 5.6267\n",
      "Iteration: 68 of 241\ttrain_loss: 5.5301\n",
      "Iteration: 70 of 241\ttrain_loss: 5.7009\n",
      "Iteration: 72 of 241\ttrain_loss: 5.5948\n",
      "Iteration: 74 of 241\ttrain_loss: 5.5629\n",
      "Iteration: 76 of 241\ttrain_loss: 5.6130\n",
      "Iteration: 78 of 241\ttrain_loss: 5.7491\n",
      "Iteration: 80 of 241\ttrain_loss: 5.4874\n",
      "Iteration: 82 of 241\ttrain_loss: 5.6733\n",
      "Iteration: 84 of 241\ttrain_loss: 5.3909\n",
      "Iteration: 86 of 241\ttrain_loss: 5.6783\n",
      "Iteration: 88 of 241\ttrain_loss: 5.5797\n",
      "Iteration: 90 of 241\ttrain_loss: 5.6465\n",
      "Iteration: 92 of 241\ttrain_loss: 5.7262\n",
      "Iteration: 94 of 241\ttrain_loss: 5.6301\n",
      "Iteration: 96 of 241\ttrain_loss: 5.6927\n",
      "Iteration: 98 of 241\ttrain_loss: 5.5652\n",
      "Iteration: 100 of 241\ttrain_loss: 5.7134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 102 of 241\ttrain_loss: 5.6901\n",
      "Iteration: 104 of 241\ttrain_loss: 5.5465\n",
      "Iteration: 106 of 241\ttrain_loss: 5.5538\n",
      "Iteration: 108 of 241\ttrain_loss: 5.5089\n",
      "Iteration: 110 of 241\ttrain_loss: 5.6106\n",
      "Iteration: 112 of 241\ttrain_loss: 5.5331\n",
      "Iteration: 114 of 241\ttrain_loss: 5.7319\n",
      "Iteration: 116 of 241\ttrain_loss: 5.5905\n",
      "Iteration: 118 of 241\ttrain_loss: 5.3817\n",
      "Iteration: 120 of 241\ttrain_loss: 5.6676\n",
      "Iteration: 122 of 241\ttrain_loss: 5.5364\n",
      "Iteration: 124 of 241\ttrain_loss: 5.6983\n",
      "Iteration: 126 of 241\ttrain_loss: 5.6186\n",
      "Iteration: 128 of 241\ttrain_loss: 5.7127\n",
      "Iteration: 130 of 241\ttrain_loss: 5.7774\n",
      "Iteration: 132 of 241\ttrain_loss: 5.7725\n",
      "Iteration: 134 of 241\ttrain_loss: 5.6916\n",
      "Iteration: 136 of 241\ttrain_loss: 5.5965\n",
      "Iteration: 138 of 241\ttrain_loss: 5.7216\n",
      "Iteration: 140 of 241\ttrain_loss: 5.5987\n",
      "Iteration: 142 of 241\ttrain_loss: 5.8383\n",
      "Iteration: 144 of 241\ttrain_loss: 5.3889\n",
      "Iteration: 146 of 241\ttrain_loss: 5.7149\n",
      "Iteration: 148 of 241\ttrain_loss: 5.6689\n",
      "Iteration: 150 of 241\ttrain_loss: 5.8047\n",
      "Iteration: 152 of 241\ttrain_loss: 5.5651\n",
      "Iteration: 154 of 241\ttrain_loss: 5.5852\n",
      "Iteration: 156 of 241\ttrain_loss: 5.4744\n",
      "Iteration: 158 of 241\ttrain_loss: 5.6903\n",
      "Iteration: 160 of 241\ttrain_loss: 5.7809\n",
      "Iteration: 162 of 241\ttrain_loss: 5.5038\n",
      "Iteration: 164 of 241\ttrain_loss: 5.6874\n",
      "Iteration: 166 of 241\ttrain_loss: 5.5589\n",
      "Iteration: 168 of 241\ttrain_loss: 5.4320\n",
      "Iteration: 170 of 241\ttrain_loss: 5.4270\n",
      "Iteration: 172 of 241\ttrain_loss: 5.5443\n",
      "Iteration: 174 of 241\ttrain_loss: 5.8834\n",
      "Iteration: 176 of 241\ttrain_loss: 5.8383\n",
      "Iteration: 178 of 241\ttrain_loss: 5.6905\n",
      "Iteration: 180 of 241\ttrain_loss: 5.7004\n",
      "Iteration: 182 of 241\ttrain_loss: 5.6054\n",
      "Iteration: 184 of 241\ttrain_loss: 5.6365\n",
      "Iteration: 186 of 241\ttrain_loss: 5.6243\n",
      "Iteration: 188 of 241\ttrain_loss: 5.8143\n",
      "Iteration: 190 of 241\ttrain_loss: 5.4578\n",
      "Iteration: 192 of 241\ttrain_loss: 5.5456\n",
      "Iteration: 194 of 241\ttrain_loss: 5.6169\n",
      "Iteration: 196 of 241\ttrain_loss: 5.6792\n",
      "Iteration: 198 of 241\ttrain_loss: 5.6067\n",
      "Iteration: 200 of 241\ttrain_loss: 5.6698\n",
      "Iteration: 202 of 241\ttrain_loss: 5.7090\n",
      "Iteration: 204 of 241\ttrain_loss: 5.4456\n",
      "Iteration: 206 of 241\ttrain_loss: 5.5144\n",
      "Iteration: 208 of 241\ttrain_loss: 5.4436\n",
      "Iteration: 210 of 241\ttrain_loss: 5.8383\n",
      "Iteration: 212 of 241\ttrain_loss: 5.4446\n",
      "Iteration: 214 of 241\ttrain_loss: 5.5001\n",
      "Iteration: 216 of 241\ttrain_loss: 5.7341\n",
      "Iteration: 218 of 241\ttrain_loss: 5.5517\n",
      "Iteration: 220 of 241\ttrain_loss: 5.6829\n",
      "Iteration: 222 of 241\ttrain_loss: 5.6367\n",
      "Iteration: 224 of 241\ttrain_loss: 5.6415\n",
      "Iteration: 226 of 241\ttrain_loss: 5.6389\n",
      "Iteration: 228 of 241\ttrain_loss: 5.6973\n",
      "Iteration: 230 of 241\ttrain_loss: 5.5195\n",
      "Iteration: 232 of 241\ttrain_loss: 6.0080\n",
      "Iteration: 234 of 241\ttrain_loss: 5.5076\n",
      "Iteration: 236 of 241\ttrain_loss: 5.6854\n",
      "Iteration: 238 of 241\ttrain_loss: 5.7911\n",
      "Iteration: 240 of 241\ttrain_loss: 5.5635\n",
      "Iteration: 241 of 241\ttrain_loss: 5.5563\n",
      "Average Score for this Epoch: 5.595888614654541\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 20 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.3060\n",
      "Iteration: 2 of 241\ttrain_loss: 5.4489\n",
      "Iteration: 4 of 241\ttrain_loss: 5.3194\n",
      "Iteration: 6 of 241\ttrain_loss: 5.3553\n",
      "Iteration: 8 of 241\ttrain_loss: 5.3450\n",
      "Iteration: 10 of 241\ttrain_loss: 5.3655\n",
      "Iteration: 12 of 241\ttrain_loss: 5.4977\n",
      "Iteration: 14 of 241\ttrain_loss: 5.4212\n",
      "Iteration: 16 of 241\ttrain_loss: 5.6714\n",
      "Iteration: 18 of 241\ttrain_loss: 5.7350\n",
      "Iteration: 20 of 241\ttrain_loss: 5.2840\n",
      "Iteration: 22 of 241\ttrain_loss: 5.2045\n",
      "Iteration: 24 of 241\ttrain_loss: 5.4717\n",
      "Iteration: 26 of 241\ttrain_loss: 5.5451\n",
      "Iteration: 28 of 241\ttrain_loss: 5.4724\n",
      "Iteration: 30 of 241\ttrain_loss: 5.6647\n",
      "Iteration: 32 of 241\ttrain_loss: 5.3946\n",
      "Iteration: 34 of 241\ttrain_loss: 5.5200\n",
      "Iteration: 36 of 241\ttrain_loss: 5.1396\n",
      "Iteration: 38 of 241\ttrain_loss: 5.6324\n",
      "Iteration: 40 of 241\ttrain_loss: 5.4978\n",
      "Iteration: 42 of 241\ttrain_loss: 5.5657\n",
      "Iteration: 44 of 241\ttrain_loss: 5.6657\n",
      "Iteration: 46 of 241\ttrain_loss: 5.5343\n",
      "Iteration: 48 of 241\ttrain_loss: 5.3590\n",
      "Iteration: 50 of 241\ttrain_loss: 5.3585\n",
      "Iteration: 52 of 241\ttrain_loss: 5.5502\n",
      "Iteration: 54 of 241\ttrain_loss: 5.4605\n",
      "Iteration: 56 of 241\ttrain_loss: 5.5380\n",
      "Iteration: 58 of 241\ttrain_loss: 5.5399\n",
      "Iteration: 60 of 241\ttrain_loss: 5.3888\n",
      "Iteration: 62 of 241\ttrain_loss: 5.4617\n",
      "Iteration: 64 of 241\ttrain_loss: 5.5255\n",
      "Iteration: 66 of 241\ttrain_loss: 5.3877\n",
      "Iteration: 68 of 241\ttrain_loss: 5.6414\n",
      "Iteration: 70 of 241\ttrain_loss: 5.4685\n",
      "Iteration: 72 of 241\ttrain_loss: 5.4076\n",
      "Iteration: 74 of 241\ttrain_loss: 5.6594\n",
      "Iteration: 76 of 241\ttrain_loss: 5.7483\n",
      "Iteration: 78 of 241\ttrain_loss: 5.7268\n",
      "Iteration: 80 of 241\ttrain_loss: 5.5028\n",
      "Iteration: 82 of 241\ttrain_loss: 5.7525\n",
      "Iteration: 84 of 241\ttrain_loss: 5.5371\n",
      "Iteration: 86 of 241\ttrain_loss: 5.4104\n",
      "Iteration: 88 of 241\ttrain_loss: 5.5489\n",
      "Iteration: 90 of 241\ttrain_loss: 5.3898\n",
      "Iteration: 92 of 241\ttrain_loss: 5.6506\n",
      "Iteration: 94 of 241\ttrain_loss: 5.6119\n",
      "Iteration: 96 of 241\ttrain_loss: 5.5366\n",
      "Iteration: 98 of 241\ttrain_loss: 5.6041\n",
      "Iteration: 100 of 241\ttrain_loss: 5.5386\n",
      "Iteration: 102 of 241\ttrain_loss: 5.7517\n",
      "Iteration: 104 of 241\ttrain_loss: 5.3949\n",
      "Iteration: 106 of 241\ttrain_loss: 5.3510\n",
      "Iteration: 108 of 241\ttrain_loss: 5.6390\n",
      "Iteration: 110 of 241\ttrain_loss: 5.4809\n",
      "Iteration: 112 of 241\ttrain_loss: 5.6173\n",
      "Iteration: 114 of 241\ttrain_loss: 5.4266\n",
      "Iteration: 116 of 241\ttrain_loss: 5.5559\n",
      "Iteration: 118 of 241\ttrain_loss: 5.4779\n",
      "Iteration: 120 of 241\ttrain_loss: 5.6782\n",
      "Iteration: 122 of 241\ttrain_loss: 5.4334\n",
      "Iteration: 124 of 241\ttrain_loss: 5.4373\n",
      "Iteration: 126 of 241\ttrain_loss: 5.4408\n",
      "Iteration: 128 of 241\ttrain_loss: 5.4451\n",
      "Iteration: 130 of 241\ttrain_loss: 5.3934\n",
      "Iteration: 132 of 241\ttrain_loss: 5.5299\n",
      "Iteration: 134 of 241\ttrain_loss: 5.4245\n",
      "Iteration: 136 of 241\ttrain_loss: 5.6110\n",
      "Iteration: 138 of 241\ttrain_loss: 5.5221\n",
      "Iteration: 140 of 241\ttrain_loss: 5.4445\n",
      "Iteration: 142 of 241\ttrain_loss: 5.6722\n",
      "Iteration: 144 of 241\ttrain_loss: 5.6073\n",
      "Iteration: 146 of 241\ttrain_loss: 5.6440\n",
      "Iteration: 148 of 241\ttrain_loss: 5.2632\n",
      "Iteration: 150 of 241\ttrain_loss: 5.4519\n",
      "Iteration: 152 of 241\ttrain_loss: 5.3058\n",
      "Iteration: 154 of 241\ttrain_loss: 5.7526\n",
      "Iteration: 156 of 241\ttrain_loss: 5.4905\n",
      "Iteration: 158 of 241\ttrain_loss: 5.4565\n",
      "Iteration: 160 of 241\ttrain_loss: 5.5409\n",
      "Iteration: 162 of 241\ttrain_loss: 5.3224\n",
      "Iteration: 164 of 241\ttrain_loss: 5.5332\n",
      "Iteration: 166 of 241\ttrain_loss: 5.6318\n",
      "Iteration: 168 of 241\ttrain_loss: 5.5092\n",
      "Iteration: 170 of 241\ttrain_loss: 5.5057\n",
      "Iteration: 172 of 241\ttrain_loss: 5.3535\n",
      "Iteration: 174 of 241\ttrain_loss: 5.5505\n",
      "Iteration: 176 of 241\ttrain_loss: 5.4169\n",
      "Iteration: 178 of 241\ttrain_loss: 5.6602\n",
      "Iteration: 180 of 241\ttrain_loss: 5.4498\n",
      "Iteration: 182 of 241\ttrain_loss: 5.4940\n",
      "Iteration: 184 of 241\ttrain_loss: 5.5751\n",
      "Iteration: 186 of 241\ttrain_loss: 5.3323\n",
      "Iteration: 188 of 241\ttrain_loss: 5.3117\n",
      "Iteration: 190 of 241\ttrain_loss: 5.3711\n",
      "Iteration: 192 of 241\ttrain_loss: 5.4204\n",
      "Iteration: 194 of 241\ttrain_loss: 5.4229\n",
      "Iteration: 196 of 241\ttrain_loss: 5.6138\n",
      "Iteration: 198 of 241\ttrain_loss: 5.6947\n",
      "Iteration: 200 of 241\ttrain_loss: 5.6439\n",
      "Iteration: 202 of 241\ttrain_loss: 5.4568\n",
      "Iteration: 204 of 241\ttrain_loss: 5.5024\n",
      "Iteration: 206 of 241\ttrain_loss: 5.5147\n",
      "Iteration: 208 of 241\ttrain_loss: 5.5026\n",
      "Iteration: 210 of 241\ttrain_loss: 5.3975\n",
      "Iteration: 212 of 241\ttrain_loss: 5.5065\n",
      "Iteration: 214 of 241\ttrain_loss: 5.2956\n",
      "Iteration: 216 of 241\ttrain_loss: 5.3958\n",
      "Iteration: 218 of 241\ttrain_loss: 5.3451\n",
      "Iteration: 220 of 241\ttrain_loss: 5.4569\n",
      "Iteration: 222 of 241\ttrain_loss: 5.5361\n",
      "Iteration: 224 of 241\ttrain_loss: 5.4562\n",
      "Iteration: 226 of 241\ttrain_loss: 5.2222\n",
      "Iteration: 228 of 241\ttrain_loss: 5.4794\n",
      "Iteration: 230 of 241\ttrain_loss: 5.2900\n",
      "Iteration: 232 of 241\ttrain_loss: 5.5316\n",
      "Iteration: 234 of 241\ttrain_loss: 5.4799\n",
      "Iteration: 236 of 241\ttrain_loss: 5.5748\n",
      "Iteration: 238 of 241\ttrain_loss: 5.4311\n",
      "Iteration: 240 of 241\ttrain_loss: 5.5728\n",
      "Iteration: 241 of 241\ttrain_loss: 5.4670\n",
      "Average Score for this Epoch: 5.4962944984436035\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 21 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.3796\n",
      "Iteration: 2 of 241\ttrain_loss: 5.1878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4 of 241\ttrain_loss: 5.3464\n",
      "Iteration: 6 of 241\ttrain_loss: 5.0985\n",
      "Iteration: 8 of 241\ttrain_loss: 5.2925\n",
      "Iteration: 10 of 241\ttrain_loss: 5.5477\n",
      "Iteration: 12 of 241\ttrain_loss: 5.3825\n",
      "Iteration: 14 of 241\ttrain_loss: 5.1239\n",
      "Iteration: 16 of 241\ttrain_loss: 5.1866\n",
      "Iteration: 18 of 241\ttrain_loss: 5.3057\n",
      "Iteration: 20 of 241\ttrain_loss: 5.1572\n",
      "Iteration: 22 of 241\ttrain_loss: 5.3009\n",
      "Iteration: 24 of 241\ttrain_loss: 5.5132\n",
      "Iteration: 26 of 241\ttrain_loss: 5.4804\n",
      "Iteration: 28 of 241\ttrain_loss: 5.5051\n",
      "Iteration: 30 of 241\ttrain_loss: 5.4431\n",
      "Iteration: 32 of 241\ttrain_loss: 5.2455\n",
      "Iteration: 34 of 241\ttrain_loss: 5.3796\n",
      "Iteration: 36 of 241\ttrain_loss: 5.5611\n",
      "Iteration: 38 of 241\ttrain_loss: 5.6130\n",
      "Iteration: 40 of 241\ttrain_loss: 5.3241\n",
      "Iteration: 42 of 241\ttrain_loss: 5.5732\n",
      "Iteration: 44 of 241\ttrain_loss: 5.3450\n",
      "Iteration: 46 of 241\ttrain_loss: 5.4394\n",
      "Iteration: 48 of 241\ttrain_loss: 5.4538\n",
      "Iteration: 50 of 241\ttrain_loss: 5.3839\n",
      "Iteration: 52 of 241\ttrain_loss: 5.4689\n",
      "Iteration: 54 of 241\ttrain_loss: 5.6068\n",
      "Iteration: 56 of 241\ttrain_loss: 5.3243\n",
      "Iteration: 58 of 241\ttrain_loss: 5.4054\n",
      "Iteration: 60 of 241\ttrain_loss: 5.4627\n",
      "Iteration: 62 of 241\ttrain_loss: 5.4135\n",
      "Iteration: 64 of 241\ttrain_loss: 5.3648\n",
      "Iteration: 66 of 241\ttrain_loss: 5.3193\n",
      "Iteration: 68 of 241\ttrain_loss: 5.6467\n",
      "Iteration: 70 of 241\ttrain_loss: 5.3464\n",
      "Iteration: 72 of 241\ttrain_loss: 5.5781\n",
      "Iteration: 74 of 241\ttrain_loss: 5.5548\n",
      "Iteration: 76 of 241\ttrain_loss: 5.2154\n",
      "Iteration: 78 of 241\ttrain_loss: 5.3298\n",
      "Iteration: 80 of 241\ttrain_loss: 5.4662\n",
      "Iteration: 82 of 241\ttrain_loss: 5.3168\n",
      "Iteration: 84 of 241\ttrain_loss: 5.6383\n",
      "Iteration: 86 of 241\ttrain_loss: 5.5019\n",
      "Iteration: 88 of 241\ttrain_loss: 5.4047\n",
      "Iteration: 90 of 241\ttrain_loss: 5.6240\n",
      "Iteration: 92 of 241\ttrain_loss: 5.3880\n",
      "Iteration: 94 of 241\ttrain_loss: 5.3854\n",
      "Iteration: 96 of 241\ttrain_loss: 5.4858\n",
      "Iteration: 98 of 241\ttrain_loss: 5.3511\n",
      "Iteration: 100 of 241\ttrain_loss: 5.3775\n",
      "Iteration: 102 of 241\ttrain_loss: 5.4111\n",
      "Iteration: 104 of 241\ttrain_loss: 5.4178\n",
      "Iteration: 106 of 241\ttrain_loss: 5.3980\n",
      "Iteration: 108 of 241\ttrain_loss: 5.4649\n",
      "Iteration: 110 of 241\ttrain_loss: 5.3924\n",
      "Iteration: 112 of 241\ttrain_loss: 5.3292\n",
      "Iteration: 114 of 241\ttrain_loss: 5.4407\n",
      "Iteration: 116 of 241\ttrain_loss: 5.3065\n",
      "Iteration: 118 of 241\ttrain_loss: 5.5073\n",
      "Iteration: 120 of 241\ttrain_loss: 5.1339\n",
      "Iteration: 122 of 241\ttrain_loss: 5.4687\n",
      "Iteration: 124 of 241\ttrain_loss: 5.4221\n",
      "Iteration: 126 of 241\ttrain_loss: 5.4727\n",
      "Iteration: 128 of 241\ttrain_loss: 5.5505\n",
      "Iteration: 130 of 241\ttrain_loss: 5.3046\n",
      "Iteration: 132 of 241\ttrain_loss: 5.2133\n",
      "Iteration: 134 of 241\ttrain_loss: 5.3283\n",
      "Iteration: 136 of 241\ttrain_loss: 5.2176\n",
      "Iteration: 138 of 241\ttrain_loss: 5.3886\n",
      "Iteration: 140 of 241\ttrain_loss: 5.4168\n",
      "Iteration: 142 of 241\ttrain_loss: 5.3130\n",
      "Iteration: 144 of 241\ttrain_loss: 5.5597\n",
      "Iteration: 146 of 241\ttrain_loss: 5.4112\n",
      "Iteration: 148 of 241\ttrain_loss: 5.4702\n",
      "Iteration: 150 of 241\ttrain_loss: 5.3518\n",
      "Iteration: 152 of 241\ttrain_loss: 5.3572\n",
      "Iteration: 154 of 241\ttrain_loss: 5.2997\n",
      "Iteration: 156 of 241\ttrain_loss: 5.3345\n",
      "Iteration: 158 of 241\ttrain_loss: 5.5917\n",
      "Iteration: 160 of 241\ttrain_loss: 5.3824\n",
      "Iteration: 162 of 241\ttrain_loss: 5.2489\n",
      "Iteration: 164 of 241\ttrain_loss: 5.4045\n",
      "Iteration: 166 of 241\ttrain_loss: 5.7314\n",
      "Iteration: 168 of 241\ttrain_loss: 5.2633\n",
      "Iteration: 170 of 241\ttrain_loss: 5.3220\n",
      "Iteration: 172 of 241\ttrain_loss: 5.5766\n",
      "Iteration: 174 of 241\ttrain_loss: 5.4232\n",
      "Iteration: 176 of 241\ttrain_loss: 5.5360\n",
      "Iteration: 178 of 241\ttrain_loss: 5.5827\n",
      "Iteration: 180 of 241\ttrain_loss: 5.3352\n",
      "Iteration: 182 of 241\ttrain_loss: 5.6250\n",
      "Iteration: 184 of 241\ttrain_loss: 5.5177\n",
      "Iteration: 186 of 241\ttrain_loss: 5.4379\n",
      "Iteration: 188 of 241\ttrain_loss: 5.3455\n",
      "Iteration: 190 of 241\ttrain_loss: 5.3573\n",
      "Iteration: 192 of 241\ttrain_loss: 5.3129\n",
      "Iteration: 194 of 241\ttrain_loss: 5.5205\n",
      "Iteration: 196 of 241\ttrain_loss: 5.3532\n",
      "Iteration: 198 of 241\ttrain_loss: 5.4207\n",
      "Iteration: 200 of 241\ttrain_loss: 5.2984\n",
      "Iteration: 202 of 241\ttrain_loss: 5.3680\n",
      "Iteration: 204 of 241\ttrain_loss: 5.4257\n",
      "Iteration: 206 of 241\ttrain_loss: 5.4863\n",
      "Iteration: 208 of 241\ttrain_loss: 5.4910\n",
      "Iteration: 210 of 241\ttrain_loss: 5.3670\n",
      "Iteration: 212 of 241\ttrain_loss: 5.5584\n",
      "Iteration: 214 of 241\ttrain_loss: 5.4761\n",
      "Iteration: 216 of 241\ttrain_loss: 5.4679\n",
      "Iteration: 218 of 241\ttrain_loss: 5.6197\n",
      "Iteration: 220 of 241\ttrain_loss: 5.3525\n",
      "Iteration: 222 of 241\ttrain_loss: 5.3886\n",
      "Iteration: 224 of 241\ttrain_loss: 5.2480\n",
      "Iteration: 226 of 241\ttrain_loss: 5.3290\n",
      "Iteration: 228 of 241\ttrain_loss: 5.1798\n",
      "Iteration: 230 of 241\ttrain_loss: 5.6083\n",
      "Iteration: 232 of 241\ttrain_loss: 5.6394\n",
      "Iteration: 234 of 241\ttrain_loss: 5.4554\n",
      "Iteration: 236 of 241\ttrain_loss: 5.4477\n",
      "Iteration: 238 of 241\ttrain_loss: 5.6377\n",
      "Iteration: 240 of 241\ttrain_loss: 5.5171\n",
      "Iteration: 241 of 241\ttrain_loss: 5.5224\n",
      "Average Score for this Epoch: 5.404534339904785\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 22 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.1164\n",
      "Iteration: 2 of 241\ttrain_loss: 5.2463\n",
      "Iteration: 4 of 241\ttrain_loss: 5.2764\n",
      "Iteration: 6 of 241\ttrain_loss: 5.2981\n",
      "Iteration: 8 of 241\ttrain_loss: 5.2825\n",
      "Iteration: 10 of 241\ttrain_loss: 5.2959\n",
      "Iteration: 12 of 241\ttrain_loss: 5.4923\n",
      "Iteration: 14 of 241\ttrain_loss: 5.2214\n",
      "Iteration: 16 of 241\ttrain_loss: 5.3517\n",
      "Iteration: 18 of 241\ttrain_loss: 5.2416\n",
      "Iteration: 20 of 241\ttrain_loss: 5.0402\n",
      "Iteration: 22 of 241\ttrain_loss: 5.5478\n",
      "Iteration: 24 of 241\ttrain_loss: 5.1146\n",
      "Iteration: 26 of 241\ttrain_loss: 5.2367\n",
      "Iteration: 28 of 241\ttrain_loss: 5.2409\n",
      "Iteration: 30 of 241\ttrain_loss: 5.1034\n",
      "Iteration: 32 of 241\ttrain_loss: 5.1668\n",
      "Iteration: 34 of 241\ttrain_loss: 5.0650\n",
      "Iteration: 36 of 241\ttrain_loss: 5.1786\n",
      "Iteration: 38 of 241\ttrain_loss: 5.2500\n",
      "Iteration: 40 of 241\ttrain_loss: 5.3922\n",
      "Iteration: 42 of 241\ttrain_loss: 5.4174\n",
      "Iteration: 44 of 241\ttrain_loss: 5.4282\n",
      "Iteration: 46 of 241\ttrain_loss: 5.2141\n",
      "Iteration: 48 of 241\ttrain_loss: 5.2855\n",
      "Iteration: 50 of 241\ttrain_loss: 5.2783\n",
      "Iteration: 52 of 241\ttrain_loss: 5.2594\n",
      "Iteration: 54 of 241\ttrain_loss: 5.1612\n",
      "Iteration: 56 of 241\ttrain_loss: 5.2743\n",
      "Iteration: 58 of 241\ttrain_loss: 5.1919\n",
      "Iteration: 60 of 241\ttrain_loss: 5.2416\n",
      "Iteration: 62 of 241\ttrain_loss: 5.1996\n",
      "Iteration: 64 of 241\ttrain_loss: 5.2088\n",
      "Iteration: 66 of 241\ttrain_loss: 5.5240\n",
      "Iteration: 68 of 241\ttrain_loss: 5.3602\n",
      "Iteration: 70 of 241\ttrain_loss: 5.3240\n",
      "Iteration: 72 of 241\ttrain_loss: 4.8374\n",
      "Iteration: 74 of 241\ttrain_loss: 5.2559\n",
      "Iteration: 76 of 241\ttrain_loss: 5.5295\n",
      "Iteration: 78 of 241\ttrain_loss: 5.3025\n",
      "Iteration: 80 of 241\ttrain_loss: 4.9275\n",
      "Iteration: 82 of 241\ttrain_loss: 5.2479\n",
      "Iteration: 84 of 241\ttrain_loss: 5.2373\n",
      "Iteration: 86 of 241\ttrain_loss: 5.2034\n",
      "Iteration: 88 of 241\ttrain_loss: 5.3135\n",
      "Iteration: 90 of 241\ttrain_loss: 5.4154\n",
      "Iteration: 92 of 241\ttrain_loss: 5.1554\n",
      "Iteration: 94 of 241\ttrain_loss: 5.2056\n",
      "Iteration: 96 of 241\ttrain_loss: 5.2516\n",
      "Iteration: 98 of 241\ttrain_loss: 5.2165\n",
      "Iteration: 100 of 241\ttrain_loss: 5.1295\n",
      "Iteration: 102 of 241\ttrain_loss: 5.0725\n",
      "Iteration: 104 of 241\ttrain_loss: 5.4966\n",
      "Iteration: 106 of 241\ttrain_loss: 5.1697\n",
      "Iteration: 108 of 241\ttrain_loss: 4.9728\n",
      "Iteration: 110 of 241\ttrain_loss: 5.1533\n",
      "Iteration: 112 of 241\ttrain_loss: 5.3254\n",
      "Iteration: 114 of 241\ttrain_loss: 5.2935\n",
      "Iteration: 116 of 241\ttrain_loss: 5.2577\n",
      "Iteration: 118 of 241\ttrain_loss: 5.2821\n",
      "Iteration: 120 of 241\ttrain_loss: 4.9212\n",
      "Iteration: 122 of 241\ttrain_loss: 5.2966\n",
      "Iteration: 124 of 241\ttrain_loss: 5.2496\n",
      "Iteration: 126 of 241\ttrain_loss: 5.1600\n",
      "Iteration: 128 of 241\ttrain_loss: 5.3097\n",
      "Iteration: 130 of 241\ttrain_loss: 5.3075\n",
      "Iteration: 132 of 241\ttrain_loss: 5.4833\n",
      "Iteration: 134 of 241\ttrain_loss: 5.2237\n",
      "Iteration: 136 of 241\ttrain_loss: 5.2784\n",
      "Iteration: 138 of 241\ttrain_loss: 5.0158\n",
      "Iteration: 140 of 241\ttrain_loss: 5.3243\n",
      "Iteration: 142 of 241\ttrain_loss: 5.2955\n",
      "Iteration: 144 of 241\ttrain_loss: 5.2577\n",
      "Iteration: 146 of 241\ttrain_loss: 5.3362\n",
      "Iteration: 148 of 241\ttrain_loss: 5.2944\n",
      "Iteration: 150 of 241\ttrain_loss: 5.1395\n",
      "Iteration: 152 of 241\ttrain_loss: 5.1475\n",
      "Iteration: 154 of 241\ttrain_loss: 5.2989\n",
      "Iteration: 156 of 241\ttrain_loss: 5.3591\n",
      "Iteration: 158 of 241\ttrain_loss: 5.6837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 160 of 241\ttrain_loss: 5.5470\n",
      "Iteration: 162 of 241\ttrain_loss: 5.3013\n",
      "Iteration: 164 of 241\ttrain_loss: 5.3608\n",
      "Iteration: 166 of 241\ttrain_loss: 5.3642\n",
      "Iteration: 168 of 241\ttrain_loss: 5.1084\n",
      "Iteration: 170 of 241\ttrain_loss: 5.4275\n",
      "Iteration: 172 of 241\ttrain_loss: 5.3331\n",
      "Iteration: 174 of 241\ttrain_loss: 5.1596\n",
      "Iteration: 176 of 241\ttrain_loss: 5.4219\n",
      "Iteration: 178 of 241\ttrain_loss: 5.4666\n",
      "Iteration: 180 of 241\ttrain_loss: 5.5221\n",
      "Iteration: 182 of 241\ttrain_loss: 5.3687\n",
      "Iteration: 184 of 241\ttrain_loss: 5.3887\n",
      "Iteration: 186 of 241\ttrain_loss: 5.3005\n",
      "Iteration: 188 of 241\ttrain_loss: 5.2510\n",
      "Iteration: 190 of 241\ttrain_loss: 5.1150\n",
      "Iteration: 192 of 241\ttrain_loss: 5.3808\n",
      "Iteration: 194 of 241\ttrain_loss: 5.2430\n",
      "Iteration: 196 of 241\ttrain_loss: 5.2736\n",
      "Iteration: 198 of 241\ttrain_loss: 5.4125\n",
      "Iteration: 200 of 241\ttrain_loss: 5.4090\n",
      "Iteration: 202 of 241\ttrain_loss: 5.2091\n",
      "Iteration: 204 of 241\ttrain_loss: 5.4690\n",
      "Iteration: 206 of 241\ttrain_loss: 5.4646\n",
      "Iteration: 208 of 241\ttrain_loss: 5.2562\n",
      "Iteration: 210 of 241\ttrain_loss: 5.2206\n",
      "Iteration: 212 of 241\ttrain_loss: 5.2840\n",
      "Iteration: 214 of 241\ttrain_loss: 5.3402\n",
      "Iteration: 216 of 241\ttrain_loss: 5.3846\n",
      "Iteration: 218 of 241\ttrain_loss: 5.2989\n",
      "Iteration: 220 of 241\ttrain_loss: 5.2890\n",
      "Iteration: 222 of 241\ttrain_loss: 5.2842\n",
      "Iteration: 224 of 241\ttrain_loss: 5.1324\n",
      "Iteration: 226 of 241\ttrain_loss: 5.3257\n",
      "Iteration: 228 of 241\ttrain_loss: 5.3246\n",
      "Iteration: 230 of 241\ttrain_loss: 5.3398\n",
      "Iteration: 232 of 241\ttrain_loss: 5.4338\n",
      "Iteration: 234 of 241\ttrain_loss: 5.3282\n",
      "Iteration: 236 of 241\ttrain_loss: 5.3759\n",
      "Iteration: 238 of 241\ttrain_loss: 5.3315\n",
      "Iteration: 240 of 241\ttrain_loss: 5.3089\n",
      "Iteration: 241 of 241\ttrain_loss: 5.4055\n",
      "Average Score for this Epoch: 5.281552791595459\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 23 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.9955\n",
      "Iteration: 2 of 241\ttrain_loss: 5.3001\n",
      "Iteration: 4 of 241\ttrain_loss: 4.9700\n",
      "Iteration: 6 of 241\ttrain_loss: 5.1527\n",
      "Iteration: 8 of 241\ttrain_loss: 5.0307\n",
      "Iteration: 10 of 241\ttrain_loss: 5.1585\n",
      "Iteration: 12 of 241\ttrain_loss: 5.2682\n",
      "Iteration: 14 of 241\ttrain_loss: 5.0974\n",
      "Iteration: 16 of 241\ttrain_loss: 5.0421\n",
      "Iteration: 18 of 241\ttrain_loss: 5.0744\n",
      "Iteration: 20 of 241\ttrain_loss: 5.0719\n",
      "Iteration: 22 of 241\ttrain_loss: 4.9426\n",
      "Iteration: 24 of 241\ttrain_loss: 4.6716\n",
      "Iteration: 26 of 241\ttrain_loss: 5.1808\n",
      "Iteration: 28 of 241\ttrain_loss: 5.3267\n",
      "Iteration: 30 of 241\ttrain_loss: 5.0202\n",
      "Iteration: 32 of 241\ttrain_loss: 5.1366\n",
      "Iteration: 34 of 241\ttrain_loss: 5.1097\n",
      "Iteration: 36 of 241\ttrain_loss: 4.8253\n",
      "Iteration: 38 of 241\ttrain_loss: 4.9769\n",
      "Iteration: 40 of 241\ttrain_loss: 4.9764\n",
      "Iteration: 42 of 241\ttrain_loss: 5.1911\n",
      "Iteration: 44 of 241\ttrain_loss: 5.0572\n",
      "Iteration: 46 of 241\ttrain_loss: 5.1131\n",
      "Iteration: 48 of 241\ttrain_loss: 5.1393\n",
      "Iteration: 50 of 241\ttrain_loss: 4.9280\n",
      "Iteration: 52 of 241\ttrain_loss: 4.9649\n",
      "Iteration: 54 of 241\ttrain_loss: 5.0829\n",
      "Iteration: 56 of 241\ttrain_loss: 4.9599\n",
      "Iteration: 58 of 241\ttrain_loss: 5.1777\n",
      "Iteration: 60 of 241\ttrain_loss: 5.1964\n",
      "Iteration: 62 of 241\ttrain_loss: 5.3000\n",
      "Iteration: 64 of 241\ttrain_loss: 5.1099\n",
      "Iteration: 66 of 241\ttrain_loss: 4.9969\n",
      "Iteration: 68 of 241\ttrain_loss: 5.1385\n",
      "Iteration: 70 of 241\ttrain_loss: 5.0681\n",
      "Iteration: 72 of 241\ttrain_loss: 5.1851\n",
      "Iteration: 74 of 241\ttrain_loss: 5.2345\n",
      "Iteration: 76 of 241\ttrain_loss: 5.1681\n",
      "Iteration: 78 of 241\ttrain_loss: 4.9276\n",
      "Iteration: 80 of 241\ttrain_loss: 5.0974\n",
      "Iteration: 82 of 241\ttrain_loss: 5.0982\n",
      "Iteration: 84 of 241\ttrain_loss: 5.0139\n",
      "Iteration: 86 of 241\ttrain_loss: 5.3115\n",
      "Iteration: 88 of 241\ttrain_loss: 5.2022\n",
      "Iteration: 90 of 241\ttrain_loss: 5.2259\n",
      "Iteration: 92 of 241\ttrain_loss: 5.1362\n",
      "Iteration: 94 of 241\ttrain_loss: 5.0372\n",
      "Iteration: 96 of 241\ttrain_loss: 4.9740\n",
      "Iteration: 98 of 241\ttrain_loss: 5.2803\n",
      "Iteration: 100 of 241\ttrain_loss: 5.1167\n",
      "Iteration: 102 of 241\ttrain_loss: 5.1532\n",
      "Iteration: 104 of 241\ttrain_loss: 4.9688\n",
      "Iteration: 106 of 241\ttrain_loss: 5.2237\n",
      "Iteration: 108 of 241\ttrain_loss: 5.2427\n",
      "Iteration: 110 of 241\ttrain_loss: 5.2225\n",
      "Iteration: 112 of 241\ttrain_loss: 5.2017\n",
      "Iteration: 114 of 241\ttrain_loss: 5.1785\n",
      "Iteration: 116 of 241\ttrain_loss: 5.1466\n",
      "Iteration: 118 of 241\ttrain_loss: 5.0363\n",
      "Iteration: 120 of 241\ttrain_loss: 5.3193\n",
      "Iteration: 122 of 241\ttrain_loss: 4.9851\n",
      "Iteration: 124 of 241\ttrain_loss: 5.0383\n",
      "Iteration: 126 of 241\ttrain_loss: 5.0156\n",
      "Iteration: 128 of 241\ttrain_loss: 5.2251\n",
      "Iteration: 130 of 241\ttrain_loss: 5.4089\n",
      "Iteration: 132 of 241\ttrain_loss: 4.9572\n",
      "Iteration: 134 of 241\ttrain_loss: 5.1516\n",
      "Iteration: 136 of 241\ttrain_loss: 5.1082\n",
      "Iteration: 138 of 241\ttrain_loss: 4.9559\n",
      "Iteration: 140 of 241\ttrain_loss: 5.4367\n",
      "Iteration: 142 of 241\ttrain_loss: 5.3363\n",
      "Iteration: 144 of 241\ttrain_loss: 5.1002\n",
      "Iteration: 146 of 241\ttrain_loss: 5.3384\n",
      "Iteration: 148 of 241\ttrain_loss: 5.3045\n",
      "Iteration: 150 of 241\ttrain_loss: 5.4001\n",
      "Iteration: 152 of 241\ttrain_loss: 5.2548\n",
      "Iteration: 154 of 241\ttrain_loss: 5.3066\n",
      "Iteration: 156 of 241\ttrain_loss: 5.2430\n",
      "Iteration: 158 of 241\ttrain_loss: 5.1210\n",
      "Iteration: 160 of 241\ttrain_loss: 5.3341\n",
      "Iteration: 162 of 241\ttrain_loss: 5.1728\n",
      "Iteration: 164 of 241\ttrain_loss: 5.2969\n",
      "Iteration: 166 of 241\ttrain_loss: 5.2997\n",
      "Iteration: 168 of 241\ttrain_loss: 5.2605\n",
      "Iteration: 170 of 241\ttrain_loss: 5.1824\n",
      "Iteration: 172 of 241\ttrain_loss: 5.1481\n",
      "Iteration: 174 of 241\ttrain_loss: 5.3291\n",
      "Iteration: 176 of 241\ttrain_loss: 5.3781\n",
      "Iteration: 178 of 241\ttrain_loss: 5.3294\n",
      "Iteration: 180 of 241\ttrain_loss: 5.2588\n",
      "Iteration: 182 of 241\ttrain_loss: 5.0207\n",
      "Iteration: 184 of 241\ttrain_loss: 5.2967\n",
      "Iteration: 186 of 241\ttrain_loss: 5.2229\n",
      "Iteration: 188 of 241\ttrain_loss: 5.3327\n",
      "Iteration: 190 of 241\ttrain_loss: 5.1218\n",
      "Iteration: 192 of 241\ttrain_loss: 5.1638\n",
      "Iteration: 194 of 241\ttrain_loss: 5.3085\n",
      "Iteration: 196 of 241\ttrain_loss: 5.3561\n",
      "Iteration: 198 of 241\ttrain_loss: 5.2241\n",
      "Iteration: 200 of 241\ttrain_loss: 5.1943\n",
      "Iteration: 202 of 241\ttrain_loss: 5.3295\n",
      "Iteration: 204 of 241\ttrain_loss: 5.1943\n",
      "Iteration: 206 of 241\ttrain_loss: 5.3108\n",
      "Iteration: 208 of 241\ttrain_loss: 5.1207\n",
      "Iteration: 210 of 241\ttrain_loss: 4.9459\n",
      "Iteration: 212 of 241\ttrain_loss: 5.2302\n",
      "Iteration: 214 of 241\ttrain_loss: 5.3895\n",
      "Iteration: 216 of 241\ttrain_loss: 5.1865\n",
      "Iteration: 218 of 241\ttrain_loss: 5.1702\n",
      "Iteration: 220 of 241\ttrain_loss: 5.4567\n",
      "Iteration: 222 of 241\ttrain_loss: 5.1229\n",
      "Iteration: 224 of 241\ttrain_loss: 4.9822\n",
      "Iteration: 226 of 241\ttrain_loss: 5.0802\n",
      "Iteration: 228 of 241\ttrain_loss: 5.1513\n",
      "Iteration: 230 of 241\ttrain_loss: 5.4025\n",
      "Iteration: 232 of 241\ttrain_loss: 5.2711\n",
      "Iteration: 234 of 241\ttrain_loss: 5.1438\n",
      "Iteration: 236 of 241\ttrain_loss: 5.1928\n",
      "Iteration: 238 of 241\ttrain_loss: 5.2417\n",
      "Iteration: 240 of 241\ttrain_loss: 5.2867\n",
      "Iteration: 241 of 241\ttrain_loss: 5.0144\n",
      "Average Score for this Epoch: 5.171065330505371\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 24 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.1414\n",
      "Iteration: 2 of 241\ttrain_loss: 4.7321\n",
      "Iteration: 4 of 241\ttrain_loss: 4.9998\n",
      "Iteration: 6 of 241\ttrain_loss: 4.7258\n",
      "Iteration: 8 of 241\ttrain_loss: 4.9051\n",
      "Iteration: 10 of 241\ttrain_loss: 4.7792\n",
      "Iteration: 12 of 241\ttrain_loss: 5.1219\n",
      "Iteration: 14 of 241\ttrain_loss: 4.9248\n",
      "Iteration: 16 of 241\ttrain_loss: 5.0779\n",
      "Iteration: 18 of 241\ttrain_loss: 4.9012\n",
      "Iteration: 20 of 241\ttrain_loss: 5.0261\n",
      "Iteration: 22 of 241\ttrain_loss: 4.6573\n",
      "Iteration: 24 of 241\ttrain_loss: 5.0471\n",
      "Iteration: 26 of 241\ttrain_loss: 4.8069\n",
      "Iteration: 28 of 241\ttrain_loss: 5.0694\n",
      "Iteration: 30 of 241\ttrain_loss: 5.1300\n",
      "Iteration: 32 of 241\ttrain_loss: 5.0753\n",
      "Iteration: 34 of 241\ttrain_loss: 4.9925\n",
      "Iteration: 36 of 241\ttrain_loss: 5.0027\n",
      "Iteration: 38 of 241\ttrain_loss: 5.1602\n",
      "Iteration: 40 of 241\ttrain_loss: 5.1475\n",
      "Iteration: 42 of 241\ttrain_loss: 5.0230\n",
      "Iteration: 44 of 241\ttrain_loss: 5.1785\n",
      "Iteration: 46 of 241\ttrain_loss: 5.1315\n",
      "Iteration: 48 of 241\ttrain_loss: 4.9029\n",
      "Iteration: 50 of 241\ttrain_loss: 4.9058\n",
      "Iteration: 52 of 241\ttrain_loss: 5.0115\n",
      "Iteration: 54 of 241\ttrain_loss: 5.2644\n",
      "Iteration: 56 of 241\ttrain_loss: 5.1765\n",
      "Iteration: 58 of 241\ttrain_loss: 5.1370\n",
      "Iteration: 60 of 241\ttrain_loss: 4.9761\n",
      "Iteration: 62 of 241\ttrain_loss: 4.8881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 64 of 241\ttrain_loss: 5.1879\n",
      "Iteration: 66 of 241\ttrain_loss: 4.8828\n",
      "Iteration: 68 of 241\ttrain_loss: 5.0183\n",
      "Iteration: 70 of 241\ttrain_loss: 5.0146\n",
      "Iteration: 72 of 241\ttrain_loss: 5.0832\n",
      "Iteration: 74 of 241\ttrain_loss: 5.1659\n",
      "Iteration: 76 of 241\ttrain_loss: 4.9816\n",
      "Iteration: 78 of 241\ttrain_loss: 5.0439\n",
      "Iteration: 80 of 241\ttrain_loss: 4.9788\n",
      "Iteration: 82 of 241\ttrain_loss: 4.7903\n",
      "Iteration: 84 of 241\ttrain_loss: 4.8310\n",
      "Iteration: 86 of 241\ttrain_loss: 5.2027\n",
      "Iteration: 88 of 241\ttrain_loss: 4.9786\n",
      "Iteration: 90 of 241\ttrain_loss: 5.1728\n",
      "Iteration: 92 of 241\ttrain_loss: 4.9697\n",
      "Iteration: 94 of 241\ttrain_loss: 5.1218\n",
      "Iteration: 96 of 241\ttrain_loss: 4.9664\n",
      "Iteration: 98 of 241\ttrain_loss: 5.1324\n",
      "Iteration: 100 of 241\ttrain_loss: 5.0311\n",
      "Iteration: 102 of 241\ttrain_loss: 5.1727\n",
      "Iteration: 104 of 241\ttrain_loss: 5.0866\n",
      "Iteration: 106 of 241\ttrain_loss: 5.1084\n",
      "Iteration: 108 of 241\ttrain_loss: 5.2366\n",
      "Iteration: 110 of 241\ttrain_loss: 5.1563\n",
      "Iteration: 112 of 241\ttrain_loss: 5.1898\n",
      "Iteration: 114 of 241\ttrain_loss: 5.1468\n",
      "Iteration: 116 of 241\ttrain_loss: 5.1560\n",
      "Iteration: 118 of 241\ttrain_loss: 5.2930\n",
      "Iteration: 120 of 241\ttrain_loss: 4.9916\n",
      "Iteration: 122 of 241\ttrain_loss: 5.1840\n",
      "Iteration: 124 of 241\ttrain_loss: 5.0924\n",
      "Iteration: 126 of 241\ttrain_loss: 5.0288\n",
      "Iteration: 128 of 241\ttrain_loss: 5.0768\n",
      "Iteration: 130 of 241\ttrain_loss: 5.2345\n",
      "Iteration: 132 of 241\ttrain_loss: 5.2540\n",
      "Iteration: 134 of 241\ttrain_loss: 5.1990\n",
      "Iteration: 136 of 241\ttrain_loss: 5.3087\n",
      "Iteration: 138 of 241\ttrain_loss: 5.1669\n",
      "Iteration: 140 of 241\ttrain_loss: 5.2191\n",
      "Iteration: 142 of 241\ttrain_loss: 5.0756\n",
      "Iteration: 144 of 241\ttrain_loss: 4.9741\n",
      "Iteration: 146 of 241\ttrain_loss: 5.1187\n",
      "Iteration: 148 of 241\ttrain_loss: 5.2321\n",
      "Iteration: 150 of 241\ttrain_loss: 5.0893\n",
      "Iteration: 152 of 241\ttrain_loss: 5.2663\n",
      "Iteration: 154 of 241\ttrain_loss: 5.1115\n",
      "Iteration: 156 of 241\ttrain_loss: 5.2175\n",
      "Iteration: 158 of 241\ttrain_loss: 5.0253\n",
      "Iteration: 160 of 241\ttrain_loss: 5.0855\n",
      "Iteration: 162 of 241\ttrain_loss: 5.3651\n",
      "Iteration: 164 of 241\ttrain_loss: 5.1749\n",
      "Iteration: 166 of 241\ttrain_loss: 5.2101\n",
      "Iteration: 168 of 241\ttrain_loss: 5.3685\n",
      "Iteration: 170 of 241\ttrain_loss: 5.2083\n",
      "Iteration: 172 of 241\ttrain_loss: 5.0837\n",
      "Iteration: 174 of 241\ttrain_loss: 5.1101\n",
      "Iteration: 176 of 241\ttrain_loss: 5.1454\n",
      "Iteration: 178 of 241\ttrain_loss: 5.2703\n",
      "Iteration: 180 of 241\ttrain_loss: 5.1050\n",
      "Iteration: 182 of 241\ttrain_loss: 5.0209\n",
      "Iteration: 184 of 241\ttrain_loss: 5.2202\n",
      "Iteration: 186 of 241\ttrain_loss: 5.0194\n",
      "Iteration: 188 of 241\ttrain_loss: 4.9275\n",
      "Iteration: 190 of 241\ttrain_loss: 5.0967\n",
      "Iteration: 192 of 241\ttrain_loss: 5.3013\n",
      "Iteration: 194 of 241\ttrain_loss: 4.9682\n",
      "Iteration: 196 of 241\ttrain_loss: 5.3379\n",
      "Iteration: 198 of 241\ttrain_loss: 5.0918\n",
      "Iteration: 200 of 241\ttrain_loss: 5.1613\n",
      "Iteration: 202 of 241\ttrain_loss: 4.9870\n",
      "Iteration: 204 of 241\ttrain_loss: 4.8554\n",
      "Iteration: 206 of 241\ttrain_loss: 4.9860\n",
      "Iteration: 208 of 241\ttrain_loss: 5.2143\n",
      "Iteration: 210 of 241\ttrain_loss: 5.3823\n",
      "Iteration: 212 of 241\ttrain_loss: 5.0627\n",
      "Iteration: 214 of 241\ttrain_loss: 5.2783\n",
      "Iteration: 216 of 241\ttrain_loss: 5.2253\n",
      "Iteration: 218 of 241\ttrain_loss: 5.0381\n",
      "Iteration: 220 of 241\ttrain_loss: 5.0465\n",
      "Iteration: 222 of 241\ttrain_loss: 5.1276\n",
      "Iteration: 224 of 241\ttrain_loss: 5.2382\n",
      "Iteration: 226 of 241\ttrain_loss: 5.4279\n",
      "Iteration: 228 of 241\ttrain_loss: 5.2621\n",
      "Iteration: 230 of 241\ttrain_loss: 5.1971\n",
      "Iteration: 232 of 241\ttrain_loss: 5.1383\n",
      "Iteration: 234 of 241\ttrain_loss: 5.2701\n",
      "Iteration: 236 of 241\ttrain_loss: 5.0003\n",
      "Iteration: 238 of 241\ttrain_loss: 5.0725\n",
      "Iteration: 240 of 241\ttrain_loss: 5.0984\n",
      "Iteration: 241 of 241\ttrain_loss: 5.1326\n",
      "Average Score for this Epoch: 5.084643363952637\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 25 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 5.1114\n",
      "Iteration: 2 of 241\ttrain_loss: 4.8346\n",
      "Iteration: 4 of 241\ttrain_loss: 4.7966\n",
      "Iteration: 6 of 241\ttrain_loss: 4.8406\n",
      "Iteration: 8 of 241\ttrain_loss: 4.9749\n",
      "Iteration: 10 of 241\ttrain_loss: 4.9660\n",
      "Iteration: 12 of 241\ttrain_loss: 4.9748\n",
      "Iteration: 14 of 241\ttrain_loss: 4.8926\n",
      "Iteration: 16 of 241\ttrain_loss: 4.9452\n",
      "Iteration: 18 of 241\ttrain_loss: 4.7010\n",
      "Iteration: 20 of 241\ttrain_loss: 4.9476\n",
      "Iteration: 22 of 241\ttrain_loss: 4.9485\n",
      "Iteration: 24 of 241\ttrain_loss: 4.7237\n",
      "Iteration: 26 of 241\ttrain_loss: 4.9937\n",
      "Iteration: 28 of 241\ttrain_loss: 4.6923\n",
      "Iteration: 30 of 241\ttrain_loss: 4.8244\n",
      "Iteration: 32 of 241\ttrain_loss: 5.0767\n",
      "Iteration: 34 of 241\ttrain_loss: 5.0058\n",
      "Iteration: 36 of 241\ttrain_loss: 5.1050\n",
      "Iteration: 38 of 241\ttrain_loss: 4.9666\n",
      "Iteration: 40 of 241\ttrain_loss: 4.9155\n",
      "Iteration: 42 of 241\ttrain_loss: 4.8133\n",
      "Iteration: 44 of 241\ttrain_loss: 4.7614\n",
      "Iteration: 46 of 241\ttrain_loss: 4.7903\n",
      "Iteration: 48 of 241\ttrain_loss: 4.5836\n",
      "Iteration: 50 of 241\ttrain_loss: 4.9312\n",
      "Iteration: 52 of 241\ttrain_loss: 4.9911\n",
      "Iteration: 54 of 241\ttrain_loss: 4.9945\n",
      "Iteration: 56 of 241\ttrain_loss: 4.7531\n",
      "Iteration: 58 of 241\ttrain_loss: 5.2481\n",
      "Iteration: 60 of 241\ttrain_loss: 4.9005\n",
      "Iteration: 62 of 241\ttrain_loss: 4.9299\n",
      "Iteration: 64 of 241\ttrain_loss: 5.0280\n",
      "Iteration: 66 of 241\ttrain_loss: 5.0415\n",
      "Iteration: 68 of 241\ttrain_loss: 4.9350\n",
      "Iteration: 70 of 241\ttrain_loss: 5.1266\n",
      "Iteration: 72 of 241\ttrain_loss: 4.8381\n",
      "Iteration: 74 of 241\ttrain_loss: 5.0430\n",
      "Iteration: 76 of 241\ttrain_loss: 5.0417\n",
      "Iteration: 78 of 241\ttrain_loss: 4.8569\n",
      "Iteration: 80 of 241\ttrain_loss: 5.1234\n",
      "Iteration: 82 of 241\ttrain_loss: 5.0050\n",
      "Iteration: 84 of 241\ttrain_loss: 5.1553\n",
      "Iteration: 86 of 241\ttrain_loss: 4.9944\n",
      "Iteration: 88 of 241\ttrain_loss: 4.9660\n",
      "Iteration: 90 of 241\ttrain_loss: 5.1411\n",
      "Iteration: 92 of 241\ttrain_loss: 5.1187\n",
      "Iteration: 94 of 241\ttrain_loss: 5.0541\n",
      "Iteration: 96 of 241\ttrain_loss: 4.9504\n",
      "Iteration: 98 of 241\ttrain_loss: 4.7986\n",
      "Iteration: 100 of 241\ttrain_loss: 5.0608\n",
      "Iteration: 102 of 241\ttrain_loss: 5.0477\n",
      "Iteration: 104 of 241\ttrain_loss: 4.9748\n",
      "Iteration: 106 of 241\ttrain_loss: 5.2507\n",
      "Iteration: 108 of 241\ttrain_loss: 5.1232\n",
      "Iteration: 110 of 241\ttrain_loss: 4.9984\n",
      "Iteration: 112 of 241\ttrain_loss: 5.1539\n",
      "Iteration: 114 of 241\ttrain_loss: 4.7627\n",
      "Iteration: 116 of 241\ttrain_loss: 5.0161\n",
      "Iteration: 118 of 241\ttrain_loss: 5.0211\n",
      "Iteration: 120 of 241\ttrain_loss: 5.0207\n",
      "Iteration: 122 of 241\ttrain_loss: 4.9780\n",
      "Iteration: 124 of 241\ttrain_loss: 4.8277\n",
      "Iteration: 126 of 241\ttrain_loss: 4.9481\n",
      "Iteration: 128 of 241\ttrain_loss: 4.9360\n",
      "Iteration: 130 of 241\ttrain_loss: 4.8941\n",
      "Iteration: 132 of 241\ttrain_loss: 5.1748\n",
      "Iteration: 134 of 241\ttrain_loss: 5.0537\n",
      "Iteration: 136 of 241\ttrain_loss: 5.0157\n",
      "Iteration: 138 of 241\ttrain_loss: 4.9783\n",
      "Iteration: 140 of 241\ttrain_loss: 5.0148\n",
      "Iteration: 142 of 241\ttrain_loss: 5.1670\n",
      "Iteration: 144 of 241\ttrain_loss: 5.0290\n",
      "Iteration: 146 of 241\ttrain_loss: 4.8843\n",
      "Iteration: 148 of 241\ttrain_loss: 5.1501\n",
      "Iteration: 150 of 241\ttrain_loss: 5.0308\n",
      "Iteration: 152 of 241\ttrain_loss: 4.9300\n",
      "Iteration: 154 of 241\ttrain_loss: 4.9109\n",
      "Iteration: 156 of 241\ttrain_loss: 4.7586\n",
      "Iteration: 158 of 241\ttrain_loss: 4.8555\n",
      "Iteration: 160 of 241\ttrain_loss: 4.8959\n",
      "Iteration: 162 of 241\ttrain_loss: 4.8251\n",
      "Iteration: 164 of 241\ttrain_loss: 4.9998\n",
      "Iteration: 166 of 241\ttrain_loss: 4.8935\n",
      "Iteration: 168 of 241\ttrain_loss: 5.0438\n",
      "Iteration: 170 of 241\ttrain_loss: 4.8554\n",
      "Iteration: 172 of 241\ttrain_loss: 4.8973\n",
      "Iteration: 174 of 241\ttrain_loss: 5.1349\n",
      "Iteration: 176 of 241\ttrain_loss: 4.8148\n",
      "Iteration: 178 of 241\ttrain_loss: 4.8008\n",
      "Iteration: 180 of 241\ttrain_loss: 4.8351\n",
      "Iteration: 182 of 241\ttrain_loss: 4.9254\n",
      "Iteration: 184 of 241\ttrain_loss: 5.0715\n",
      "Iteration: 186 of 241\ttrain_loss: 4.9667\n",
      "Iteration: 188 of 241\ttrain_loss: 4.9702\n",
      "Iteration: 190 of 241\ttrain_loss: 5.1182\n",
      "Iteration: 192 of 241\ttrain_loss: 4.7377\n",
      "Iteration: 194 of 241\ttrain_loss: 5.1859\n",
      "Iteration: 196 of 241\ttrain_loss: 5.1036\n",
      "Iteration: 198 of 241\ttrain_loss: 5.0185\n",
      "Iteration: 200 of 241\ttrain_loss: 5.1341\n",
      "Iteration: 202 of 241\ttrain_loss: 4.9095\n",
      "Iteration: 204 of 241\ttrain_loss: 4.8397\n",
      "Iteration: 206 of 241\ttrain_loss: 4.9757\n",
      "Iteration: 208 of 241\ttrain_loss: 5.0194\n",
      "Iteration: 210 of 241\ttrain_loss: 4.9024\n",
      "Iteration: 212 of 241\ttrain_loss: 4.9575\n",
      "Iteration: 214 of 241\ttrain_loss: 4.9154\n",
      "Iteration: 216 of 241\ttrain_loss: 5.1292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 218 of 241\ttrain_loss: 4.7978\n",
      "Iteration: 220 of 241\ttrain_loss: 5.1187\n",
      "Iteration: 222 of 241\ttrain_loss: 4.9219\n",
      "Iteration: 224 of 241\ttrain_loss: 5.0021\n",
      "Iteration: 226 of 241\ttrain_loss: 4.9326\n",
      "Iteration: 228 of 241\ttrain_loss: 4.9583\n",
      "Iteration: 230 of 241\ttrain_loss: 4.9798\n",
      "Iteration: 232 of 241\ttrain_loss: 5.1275\n",
      "Iteration: 234 of 241\ttrain_loss: 4.9981\n",
      "Iteration: 236 of 241\ttrain_loss: 5.0288\n",
      "Iteration: 238 of 241\ttrain_loss: 4.8560\n",
      "Iteration: 240 of 241\ttrain_loss: 5.3420\n",
      "Iteration: 241 of 241\ttrain_loss: 4.9903\n",
      "Average Score for this Epoch: 4.972450256347656\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 26 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.8589\n",
      "Iteration: 2 of 241\ttrain_loss: 4.8684\n",
      "Iteration: 4 of 241\ttrain_loss: 4.9946\n",
      "Iteration: 6 of 241\ttrain_loss: 4.8384\n",
      "Iteration: 8 of 241\ttrain_loss: 4.7305\n",
      "Iteration: 10 of 241\ttrain_loss: 4.7467\n",
      "Iteration: 12 of 241\ttrain_loss: 4.4667\n",
      "Iteration: 14 of 241\ttrain_loss: 4.7215\n",
      "Iteration: 16 of 241\ttrain_loss: 4.9540\n",
      "Iteration: 18 of 241\ttrain_loss: 4.8651\n",
      "Iteration: 20 of 241\ttrain_loss: 4.6549\n",
      "Iteration: 22 of 241\ttrain_loss: 4.7087\n",
      "Iteration: 24 of 241\ttrain_loss: 4.9739\n",
      "Iteration: 26 of 241\ttrain_loss: 4.7299\n",
      "Iteration: 28 of 241\ttrain_loss: 4.9241\n",
      "Iteration: 30 of 241\ttrain_loss: 4.7466\n",
      "Iteration: 32 of 241\ttrain_loss: 4.8102\n",
      "Iteration: 34 of 241\ttrain_loss: 4.7873\n",
      "Iteration: 36 of 241\ttrain_loss: 4.9707\n",
      "Iteration: 38 of 241\ttrain_loss: 4.8610\n",
      "Iteration: 40 of 241\ttrain_loss: 4.9888\n",
      "Iteration: 42 of 241\ttrain_loss: 4.7670\n",
      "Iteration: 44 of 241\ttrain_loss: 4.4666\n",
      "Iteration: 46 of 241\ttrain_loss: 4.8546\n",
      "Iteration: 48 of 241\ttrain_loss: 4.6807\n",
      "Iteration: 50 of 241\ttrain_loss: 4.7879\n",
      "Iteration: 52 of 241\ttrain_loss: 4.7361\n",
      "Iteration: 54 of 241\ttrain_loss: 4.8586\n",
      "Iteration: 56 of 241\ttrain_loss: 4.6374\n",
      "Iteration: 58 of 241\ttrain_loss: 4.7341\n",
      "Iteration: 60 of 241\ttrain_loss: 4.8620\n",
      "Iteration: 62 of 241\ttrain_loss: 4.9533\n",
      "Iteration: 64 of 241\ttrain_loss: 4.8019\n",
      "Iteration: 66 of 241\ttrain_loss: 4.9246\n",
      "Iteration: 68 of 241\ttrain_loss: 4.8154\n",
      "Iteration: 70 of 241\ttrain_loss: 4.8093\n",
      "Iteration: 72 of 241\ttrain_loss: 4.7211\n",
      "Iteration: 74 of 241\ttrain_loss: 4.7332\n",
      "Iteration: 76 of 241\ttrain_loss: 4.8374\n",
      "Iteration: 78 of 241\ttrain_loss: 4.9402\n",
      "Iteration: 80 of 241\ttrain_loss: 4.7914\n",
      "Iteration: 82 of 241\ttrain_loss: 4.9693\n",
      "Iteration: 84 of 241\ttrain_loss: 4.7849\n",
      "Iteration: 86 of 241\ttrain_loss: 4.8059\n",
      "Iteration: 88 of 241\ttrain_loss: 4.8544\n",
      "Iteration: 90 of 241\ttrain_loss: 5.0753\n",
      "Iteration: 92 of 241\ttrain_loss: 4.6962\n",
      "Iteration: 94 of 241\ttrain_loss: 4.6088\n",
      "Iteration: 96 of 241\ttrain_loss: 5.0410\n",
      "Iteration: 98 of 241\ttrain_loss: 4.7726\n",
      "Iteration: 100 of 241\ttrain_loss: 4.9044\n",
      "Iteration: 102 of 241\ttrain_loss: 5.0471\n",
      "Iteration: 104 of 241\ttrain_loss: 4.7483\n",
      "Iteration: 106 of 241\ttrain_loss: 4.7285\n",
      "Iteration: 108 of 241\ttrain_loss: 4.8095\n",
      "Iteration: 110 of 241\ttrain_loss: 4.7215\n",
      "Iteration: 112 of 241\ttrain_loss: 4.7983\n",
      "Iteration: 114 of 241\ttrain_loss: 4.7597\n",
      "Iteration: 116 of 241\ttrain_loss: 5.0538\n",
      "Iteration: 118 of 241\ttrain_loss: 5.1855\n",
      "Iteration: 120 of 241\ttrain_loss: 4.8091\n",
      "Iteration: 122 of 241\ttrain_loss: 4.8046\n",
      "Iteration: 124 of 241\ttrain_loss: 4.9253\n",
      "Iteration: 126 of 241\ttrain_loss: 4.8102\n",
      "Iteration: 128 of 241\ttrain_loss: 4.7573\n",
      "Iteration: 130 of 241\ttrain_loss: 4.6801\n",
      "Iteration: 132 of 241\ttrain_loss: 4.9040\n",
      "Iteration: 134 of 241\ttrain_loss: 4.8229\n",
      "Iteration: 136 of 241\ttrain_loss: 4.7412\n",
      "Iteration: 138 of 241\ttrain_loss: 4.7373\n",
      "Iteration: 140 of 241\ttrain_loss: 4.6599\n",
      "Iteration: 142 of 241\ttrain_loss: 4.8344\n",
      "Iteration: 144 of 241\ttrain_loss: 4.8678\n",
      "Iteration: 146 of 241\ttrain_loss: 4.7774\n",
      "Iteration: 148 of 241\ttrain_loss: 4.9584\n",
      "Iteration: 150 of 241\ttrain_loss: 4.9232\n",
      "Iteration: 152 of 241\ttrain_loss: 4.8577\n",
      "Iteration: 154 of 241\ttrain_loss: 4.8203\n",
      "Iteration: 156 of 241\ttrain_loss: 4.9843\n",
      "Iteration: 158 of 241\ttrain_loss: 4.7628\n",
      "Iteration: 160 of 241\ttrain_loss: 4.7812\n",
      "Iteration: 162 of 241\ttrain_loss: 4.9569\n",
      "Iteration: 164 of 241\ttrain_loss: 4.8410\n",
      "Iteration: 166 of 241\ttrain_loss: 4.8439\n",
      "Iteration: 168 of 241\ttrain_loss: 4.8174\n",
      "Iteration: 170 of 241\ttrain_loss: 4.6376\n",
      "Iteration: 172 of 241\ttrain_loss: 5.1189\n",
      "Iteration: 174 of 241\ttrain_loss: 4.7411\n",
      "Iteration: 176 of 241\ttrain_loss: 4.8864\n",
      "Iteration: 178 of 241\ttrain_loss: 4.9037\n",
      "Iteration: 180 of 241\ttrain_loss: 4.6516\n",
      "Iteration: 182 of 241\ttrain_loss: 4.8086\n",
      "Iteration: 184 of 241\ttrain_loss: 4.6493\n",
      "Iteration: 186 of 241\ttrain_loss: 4.9571\n",
      "Iteration: 188 of 241\ttrain_loss: 4.7664\n",
      "Iteration: 190 of 241\ttrain_loss: 5.0860\n",
      "Iteration: 192 of 241\ttrain_loss: 5.1544\n",
      "Iteration: 194 of 241\ttrain_loss: 4.6602\n",
      "Iteration: 196 of 241\ttrain_loss: 4.9939\n",
      "Iteration: 198 of 241\ttrain_loss: 4.9079\n",
      "Iteration: 200 of 241\ttrain_loss: 4.8384\n",
      "Iteration: 202 of 241\ttrain_loss: 4.8396\n",
      "Iteration: 204 of 241\ttrain_loss: 4.8822\n",
      "Iteration: 206 of 241\ttrain_loss: 4.8372\n",
      "Iteration: 208 of 241\ttrain_loss: 4.9743\n",
      "Iteration: 210 of 241\ttrain_loss: 4.9769\n",
      "Iteration: 212 of 241\ttrain_loss: 4.6643\n",
      "Iteration: 214 of 241\ttrain_loss: 4.7992\n",
      "Iteration: 216 of 241\ttrain_loss: 5.0538\n",
      "Iteration: 218 of 241\ttrain_loss: 5.1014\n",
      "Iteration: 220 of 241\ttrain_loss: 4.9315\n",
      "Iteration: 222 of 241\ttrain_loss: 4.8853\n",
      "Iteration: 224 of 241\ttrain_loss: 5.1034\n",
      "Iteration: 226 of 241\ttrain_loss: 4.8649\n",
      "Iteration: 228 of 241\ttrain_loss: 4.9504\n",
      "Iteration: 230 of 241\ttrain_loss: 4.9542\n",
      "Iteration: 232 of 241\ttrain_loss: 4.9161\n",
      "Iteration: 234 of 241\ttrain_loss: 5.0550\n",
      "Iteration: 236 of 241\ttrain_loss: 5.0897\n",
      "Iteration: 238 of 241\ttrain_loss: 4.7904\n",
      "Iteration: 240 of 241\ttrain_loss: 4.9551\n",
      "Iteration: 241 of 241\ttrain_loss: 4.9387\n",
      "Average Score for this Epoch: 4.843303203582764\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 27 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.7354\n",
      "Iteration: 2 of 241\ttrain_loss: 4.6715\n",
      "Iteration: 4 of 241\ttrain_loss: 4.5880\n",
      "Iteration: 6 of 241\ttrain_loss: 4.5766\n",
      "Iteration: 8 of 241\ttrain_loss: 4.6489\n",
      "Iteration: 10 of 241\ttrain_loss: 4.5594\n",
      "Iteration: 12 of 241\ttrain_loss: 4.6781\n",
      "Iteration: 14 of 241\ttrain_loss: 4.6555\n",
      "Iteration: 16 of 241\ttrain_loss: 4.6774\n",
      "Iteration: 18 of 241\ttrain_loss: 4.7031\n",
      "Iteration: 20 of 241\ttrain_loss: 4.3334\n",
      "Iteration: 22 of 241\ttrain_loss: 4.6210\n",
      "Iteration: 24 of 241\ttrain_loss: 4.9217\n",
      "Iteration: 26 of 241\ttrain_loss: 4.6139\n",
      "Iteration: 28 of 241\ttrain_loss: 4.5026\n",
      "Iteration: 30 of 241\ttrain_loss: 4.6610\n",
      "Iteration: 32 of 241\ttrain_loss: 4.5626\n",
      "Iteration: 34 of 241\ttrain_loss: 4.7741\n",
      "Iteration: 36 of 241\ttrain_loss: 4.6690\n",
      "Iteration: 38 of 241\ttrain_loss: 4.8152\n",
      "Iteration: 40 of 241\ttrain_loss: 4.7022\n",
      "Iteration: 42 of 241\ttrain_loss: 4.5057\n",
      "Iteration: 44 of 241\ttrain_loss: 4.8101\n",
      "Iteration: 46 of 241\ttrain_loss: 4.5695\n",
      "Iteration: 48 of 241\ttrain_loss: 4.6140\n",
      "Iteration: 50 of 241\ttrain_loss: 4.6306\n",
      "Iteration: 52 of 241\ttrain_loss: 4.8289\n",
      "Iteration: 54 of 241\ttrain_loss: 4.5950\n",
      "Iteration: 56 of 241\ttrain_loss: 4.7459\n",
      "Iteration: 58 of 241\ttrain_loss: 4.5697\n",
      "Iteration: 60 of 241\ttrain_loss: 4.7439\n",
      "Iteration: 62 of 241\ttrain_loss: 4.7960\n",
      "Iteration: 64 of 241\ttrain_loss: 4.7753\n",
      "Iteration: 66 of 241\ttrain_loss: 4.8246\n",
      "Iteration: 68 of 241\ttrain_loss: 4.6447\n",
      "Iteration: 70 of 241\ttrain_loss: 4.5860\n",
      "Iteration: 72 of 241\ttrain_loss: 4.5401\n",
      "Iteration: 74 of 241\ttrain_loss: 4.8667\n",
      "Iteration: 76 of 241\ttrain_loss: 4.6383\n",
      "Iteration: 78 of 241\ttrain_loss: 4.4111\n",
      "Iteration: 80 of 241\ttrain_loss: 4.6734\n",
      "Iteration: 82 of 241\ttrain_loss: 4.3826\n",
      "Iteration: 84 of 241\ttrain_loss: 4.5768\n",
      "Iteration: 86 of 241\ttrain_loss: 4.8136\n",
      "Iteration: 88 of 241\ttrain_loss: 4.7287\n",
      "Iteration: 90 of 241\ttrain_loss: 4.7592\n",
      "Iteration: 92 of 241\ttrain_loss: 4.6450\n",
      "Iteration: 94 of 241\ttrain_loss: 4.5177\n",
      "Iteration: 96 of 241\ttrain_loss: 5.1320\n",
      "Iteration: 98 of 241\ttrain_loss: 4.6281\n",
      "Iteration: 100 of 241\ttrain_loss: 4.5365\n",
      "Iteration: 102 of 241\ttrain_loss: 4.6869\n",
      "Iteration: 104 of 241\ttrain_loss: 4.7532\n",
      "Iteration: 106 of 241\ttrain_loss: 4.6199\n",
      "Iteration: 108 of 241\ttrain_loss: 4.5343\n",
      "Iteration: 110 of 241\ttrain_loss: 4.6606\n",
      "Iteration: 112 of 241\ttrain_loss: 4.7776\n",
      "Iteration: 114 of 241\ttrain_loss: 4.7056\n",
      "Iteration: 116 of 241\ttrain_loss: 4.8300\n",
      "Iteration: 118 of 241\ttrain_loss: 4.6470\n",
      "Iteration: 120 of 241\ttrain_loss: 4.5737\n",
      "Iteration: 122 of 241\ttrain_loss: 4.7138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 124 of 241\ttrain_loss: 4.6777\n",
      "Iteration: 126 of 241\ttrain_loss: 4.5799\n",
      "Iteration: 128 of 241\ttrain_loss: 4.5193\n",
      "Iteration: 130 of 241\ttrain_loss: 4.7993\n",
      "Iteration: 132 of 241\ttrain_loss: 4.8631\n",
      "Iteration: 134 of 241\ttrain_loss: 4.8018\n",
      "Iteration: 136 of 241\ttrain_loss: 4.7734\n",
      "Iteration: 138 of 241\ttrain_loss: 4.5378\n",
      "Iteration: 140 of 241\ttrain_loss: 4.5859\n",
      "Iteration: 142 of 241\ttrain_loss: 4.4892\n",
      "Iteration: 144 of 241\ttrain_loss: 4.5344\n",
      "Iteration: 146 of 241\ttrain_loss: 4.6736\n",
      "Iteration: 148 of 241\ttrain_loss: 4.7285\n",
      "Iteration: 150 of 241\ttrain_loss: 4.8136\n",
      "Iteration: 152 of 241\ttrain_loss: 4.6744\n",
      "Iteration: 154 of 241\ttrain_loss: 4.4778\n",
      "Iteration: 156 of 241\ttrain_loss: 4.6569\n",
      "Iteration: 158 of 241\ttrain_loss: 4.9868\n",
      "Iteration: 160 of 241\ttrain_loss: 4.8010\n",
      "Iteration: 162 of 241\ttrain_loss: 4.7550\n",
      "Iteration: 164 of 241\ttrain_loss: 4.9094\n",
      "Iteration: 166 of 241\ttrain_loss: 4.9000\n",
      "Iteration: 168 of 241\ttrain_loss: 4.7383\n",
      "Iteration: 170 of 241\ttrain_loss: 5.1785\n",
      "Iteration: 172 of 241\ttrain_loss: 4.6699\n",
      "Iteration: 174 of 241\ttrain_loss: 4.8146\n",
      "Iteration: 176 of 241\ttrain_loss: 4.6586\n",
      "Iteration: 178 of 241\ttrain_loss: 4.7223\n",
      "Iteration: 180 of 241\ttrain_loss: 4.8742\n",
      "Iteration: 182 of 241\ttrain_loss: 4.8165\n",
      "Iteration: 184 of 241\ttrain_loss: 4.8361\n",
      "Iteration: 186 of 241\ttrain_loss: 4.9562\n",
      "Iteration: 188 of 241\ttrain_loss: 4.7073\n",
      "Iteration: 190 of 241\ttrain_loss: 4.8223\n",
      "Iteration: 192 of 241\ttrain_loss: 4.7454\n",
      "Iteration: 194 of 241\ttrain_loss: 4.6979\n",
      "Iteration: 196 of 241\ttrain_loss: 4.6250\n",
      "Iteration: 198 of 241\ttrain_loss: 4.9249\n",
      "Iteration: 200 of 241\ttrain_loss: 4.9775\n",
      "Iteration: 202 of 241\ttrain_loss: 5.0167\n",
      "Iteration: 204 of 241\ttrain_loss: 4.7752\n",
      "Iteration: 206 of 241\ttrain_loss: 5.0978\n",
      "Iteration: 208 of 241\ttrain_loss: 4.6776\n",
      "Iteration: 210 of 241\ttrain_loss: 4.8604\n",
      "Iteration: 212 of 241\ttrain_loss: 4.8444\n",
      "Iteration: 214 of 241\ttrain_loss: 4.6410\n",
      "Iteration: 216 of 241\ttrain_loss: 4.6610\n",
      "Iteration: 218 of 241\ttrain_loss: 4.7233\n",
      "Iteration: 220 of 241\ttrain_loss: 4.6562\n",
      "Iteration: 222 of 241\ttrain_loss: 4.7801\n",
      "Iteration: 224 of 241\ttrain_loss: 4.9832\n",
      "Iteration: 226 of 241\ttrain_loss: 5.0067\n",
      "Iteration: 228 of 241\ttrain_loss: 4.7482\n",
      "Iteration: 230 of 241\ttrain_loss: 4.8929\n",
      "Iteration: 232 of 241\ttrain_loss: 4.6455\n",
      "Iteration: 234 of 241\ttrain_loss: 4.8123\n",
      "Iteration: 236 of 241\ttrain_loss: 4.6798\n",
      "Iteration: 238 of 241\ttrain_loss: 4.6827\n",
      "Iteration: 240 of 241\ttrain_loss: 4.9364\n",
      "Iteration: 241 of 241\ttrain_loss: 4.8037\n",
      "Average Score for this Epoch: 4.704107284545898\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 28 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.7155\n",
      "Iteration: 2 of 241\ttrain_loss: 4.4781\n",
      "Iteration: 4 of 241\ttrain_loss: 4.4436\n",
      "Iteration: 6 of 241\ttrain_loss: 4.3622\n",
      "Iteration: 8 of 241\ttrain_loss: 4.5112\n",
      "Iteration: 10 of 241\ttrain_loss: 4.7166\n",
      "Iteration: 12 of 241\ttrain_loss: 4.6823\n",
      "Iteration: 14 of 241\ttrain_loss: 4.5100\n",
      "Iteration: 16 of 241\ttrain_loss: 4.4932\n",
      "Iteration: 18 of 241\ttrain_loss: 4.5811\n",
      "Iteration: 20 of 241\ttrain_loss: 4.6675\n",
      "Iteration: 22 of 241\ttrain_loss: 4.4071\n",
      "Iteration: 24 of 241\ttrain_loss: 4.4456\n",
      "Iteration: 26 of 241\ttrain_loss: 4.5055\n",
      "Iteration: 28 of 241\ttrain_loss: 4.3458\n",
      "Iteration: 30 of 241\ttrain_loss: 4.6694\n",
      "Iteration: 32 of 241\ttrain_loss: 4.3852\n",
      "Iteration: 34 of 241\ttrain_loss: 4.4963\n",
      "Iteration: 36 of 241\ttrain_loss: 4.6731\n",
      "Iteration: 38 of 241\ttrain_loss: 4.4768\n",
      "Iteration: 40 of 241\ttrain_loss: 4.5877\n",
      "Iteration: 42 of 241\ttrain_loss: 4.5115\n",
      "Iteration: 44 of 241\ttrain_loss: 4.4307\n",
      "Iteration: 46 of 241\ttrain_loss: 4.4526\n",
      "Iteration: 48 of 241\ttrain_loss: 4.5407\n",
      "Iteration: 50 of 241\ttrain_loss: 4.6526\n",
      "Iteration: 52 of 241\ttrain_loss: 4.3202\n",
      "Iteration: 54 of 241\ttrain_loss: 4.6732\n",
      "Iteration: 56 of 241\ttrain_loss: 4.4348\n",
      "Iteration: 58 of 241\ttrain_loss: 4.3794\n",
      "Iteration: 60 of 241\ttrain_loss: 4.3870\n",
      "Iteration: 62 of 241\ttrain_loss: 4.5764\n",
      "Iteration: 64 of 241\ttrain_loss: 4.4589\n",
      "Iteration: 66 of 241\ttrain_loss: 4.4670\n",
      "Iteration: 68 of 241\ttrain_loss: 4.4243\n",
      "Iteration: 70 of 241\ttrain_loss: 4.4604\n",
      "Iteration: 72 of 241\ttrain_loss: 4.5845\n",
      "Iteration: 74 of 241\ttrain_loss: 4.5569\n",
      "Iteration: 76 of 241\ttrain_loss: 4.4260\n",
      "Iteration: 78 of 241\ttrain_loss: 4.6102\n",
      "Iteration: 80 of 241\ttrain_loss: 4.4628\n",
      "Iteration: 82 of 241\ttrain_loss: 4.5357\n",
      "Iteration: 84 of 241\ttrain_loss: 4.6535\n",
      "Iteration: 86 of 241\ttrain_loss: 4.6175\n",
      "Iteration: 88 of 241\ttrain_loss: 4.3640\n",
      "Iteration: 90 of 241\ttrain_loss: 4.4697\n",
      "Iteration: 92 of 241\ttrain_loss: 4.3533\n",
      "Iteration: 94 of 241\ttrain_loss: 4.6327\n",
      "Iteration: 96 of 241\ttrain_loss: 4.4566\n",
      "Iteration: 98 of 241\ttrain_loss: 4.5766\n",
      "Iteration: 100 of 241\ttrain_loss: 4.4239\n",
      "Iteration: 102 of 241\ttrain_loss: 4.6992\n",
      "Iteration: 104 of 241\ttrain_loss: 4.8444\n",
      "Iteration: 106 of 241\ttrain_loss: 4.8045\n",
      "Iteration: 108 of 241\ttrain_loss: 4.4704\n",
      "Iteration: 110 of 241\ttrain_loss: 4.5091\n",
      "Iteration: 112 of 241\ttrain_loss: 4.7468\n",
      "Iteration: 114 of 241\ttrain_loss: 4.6808\n",
      "Iteration: 116 of 241\ttrain_loss: 4.7116\n",
      "Iteration: 118 of 241\ttrain_loss: 4.8233\n",
      "Iteration: 120 of 241\ttrain_loss: 4.6368\n",
      "Iteration: 122 of 241\ttrain_loss: 4.7746\n",
      "Iteration: 124 of 241\ttrain_loss: 4.7216\n",
      "Iteration: 126 of 241\ttrain_loss: 4.6593\n",
      "Iteration: 128 of 241\ttrain_loss: 4.7471\n",
      "Iteration: 130 of 241\ttrain_loss: 4.5704\n",
      "Iteration: 132 of 241\ttrain_loss: 4.9678\n",
      "Iteration: 134 of 241\ttrain_loss: 4.7433\n",
      "Iteration: 136 of 241\ttrain_loss: 4.4624\n",
      "Iteration: 138 of 241\ttrain_loss: 4.5488\n",
      "Iteration: 140 of 241\ttrain_loss: 4.4937\n",
      "Iteration: 142 of 241\ttrain_loss: 4.7449\n",
      "Iteration: 144 of 241\ttrain_loss: 4.9361\n",
      "Iteration: 146 of 241\ttrain_loss: 4.6842\n",
      "Iteration: 148 of 241\ttrain_loss: 4.7886\n",
      "Iteration: 150 of 241\ttrain_loss: 4.6682\n",
      "Iteration: 152 of 241\ttrain_loss: 4.6915\n",
      "Iteration: 154 of 241\ttrain_loss: 4.7557\n",
      "Iteration: 156 of 241\ttrain_loss: 4.7686\n",
      "Iteration: 158 of 241\ttrain_loss: 4.7223\n",
      "Iteration: 160 of 241\ttrain_loss: 4.5938\n",
      "Iteration: 162 of 241\ttrain_loss: 4.6957\n",
      "Iteration: 164 of 241\ttrain_loss: 4.8730\n",
      "Iteration: 166 of 241\ttrain_loss: 4.5931\n",
      "Iteration: 168 of 241\ttrain_loss: 4.6527\n",
      "Iteration: 170 of 241\ttrain_loss: 4.7650\n",
      "Iteration: 172 of 241\ttrain_loss: 4.6369\n",
      "Iteration: 174 of 241\ttrain_loss: 4.5326\n",
      "Iteration: 176 of 241\ttrain_loss: 4.6001\n",
      "Iteration: 178 of 241\ttrain_loss: 4.6912\n",
      "Iteration: 180 of 241\ttrain_loss: 4.8655\n",
      "Iteration: 182 of 241\ttrain_loss: 4.6753\n",
      "Iteration: 184 of 241\ttrain_loss: 4.7457\n",
      "Iteration: 186 of 241\ttrain_loss: 4.5410\n",
      "Iteration: 188 of 241\ttrain_loss: 4.5592\n",
      "Iteration: 190 of 241\ttrain_loss: 4.6746\n",
      "Iteration: 192 of 241\ttrain_loss: 4.5183\n",
      "Iteration: 194 of 241\ttrain_loss: 4.5721\n",
      "Iteration: 196 of 241\ttrain_loss: 4.7094\n",
      "Iteration: 198 of 241\ttrain_loss: 4.5000\n",
      "Iteration: 200 of 241\ttrain_loss: 4.6283\n",
      "Iteration: 202 of 241\ttrain_loss: 4.4562\n",
      "Iteration: 204 of 241\ttrain_loss: 4.5461\n",
      "Iteration: 206 of 241\ttrain_loss: 4.8139\n",
      "Iteration: 208 of 241\ttrain_loss: 4.6404\n",
      "Iteration: 210 of 241\ttrain_loss: 4.7214\n",
      "Iteration: 212 of 241\ttrain_loss: 4.8033\n",
      "Iteration: 214 of 241\ttrain_loss: 4.7418\n",
      "Iteration: 216 of 241\ttrain_loss: 4.7838\n",
      "Iteration: 218 of 241\ttrain_loss: 4.4051\n",
      "Iteration: 220 of 241\ttrain_loss: 4.4279\n",
      "Iteration: 222 of 241\ttrain_loss: 4.7723\n",
      "Iteration: 224 of 241\ttrain_loss: 4.6363\n",
      "Iteration: 226 of 241\ttrain_loss: 4.6499\n",
      "Iteration: 228 of 241\ttrain_loss: 4.7722\n",
      "Iteration: 230 of 241\ttrain_loss: 4.5015\n",
      "Iteration: 232 of 241\ttrain_loss: 4.7703\n",
      "Iteration: 234 of 241\ttrain_loss: 4.8620\n",
      "Iteration: 236 of 241\ttrain_loss: 4.6645\n",
      "Iteration: 238 of 241\ttrain_loss: 4.5708\n",
      "Iteration: 240 of 241\ttrain_loss: 4.5167\n",
      "Iteration: 241 of 241\ttrain_loss: 4.6470\n",
      "Average Score for this Epoch: 4.6017303466796875\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 29 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.5545\n",
      "Iteration: 2 of 241\ttrain_loss: 4.5624\n",
      "Iteration: 4 of 241\ttrain_loss: 4.5852\n",
      "Iteration: 6 of 241\ttrain_loss: 4.4094\n",
      "Iteration: 8 of 241\ttrain_loss: 4.3848\n",
      "Iteration: 10 of 241\ttrain_loss: 4.4730\n",
      "Iteration: 12 of 241\ttrain_loss: 4.5367\n",
      "Iteration: 14 of 241\ttrain_loss: 4.1955\n",
      "Iteration: 16 of 241\ttrain_loss: 4.5353\n",
      "Iteration: 18 of 241\ttrain_loss: 4.4636\n",
      "Iteration: 20 of 241\ttrain_loss: 4.4088\n",
      "Iteration: 22 of 241\ttrain_loss: 4.1146\n",
      "Iteration: 24 of 241\ttrain_loss: 4.2187\n",
      "Iteration: 26 of 241\ttrain_loss: 4.4554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 28 of 241\ttrain_loss: 4.2133\n",
      "Iteration: 30 of 241\ttrain_loss: 4.5213\n",
      "Iteration: 32 of 241\ttrain_loss: 4.3409\n",
      "Iteration: 34 of 241\ttrain_loss: 4.6138\n",
      "Iteration: 36 of 241\ttrain_loss: 4.7145\n",
      "Iteration: 38 of 241\ttrain_loss: 3.9943\n",
      "Iteration: 40 of 241\ttrain_loss: 4.1822\n",
      "Iteration: 42 of 241\ttrain_loss: 4.5292\n",
      "Iteration: 44 of 241\ttrain_loss: 3.9924\n",
      "Iteration: 46 of 241\ttrain_loss: 4.4588\n",
      "Iteration: 48 of 241\ttrain_loss: 4.5451\n",
      "Iteration: 50 of 241\ttrain_loss: 4.3464\n",
      "Iteration: 52 of 241\ttrain_loss: 4.5093\n",
      "Iteration: 54 of 241\ttrain_loss: 4.2151\n",
      "Iteration: 56 of 241\ttrain_loss: 4.4796\n",
      "Iteration: 58 of 241\ttrain_loss: 4.4189\n",
      "Iteration: 60 of 241\ttrain_loss: 4.6628\n",
      "Iteration: 62 of 241\ttrain_loss: 4.5515\n",
      "Iteration: 64 of 241\ttrain_loss: 4.5294\n",
      "Iteration: 66 of 241\ttrain_loss: 4.6366\n",
      "Iteration: 68 of 241\ttrain_loss: 4.4408\n",
      "Iteration: 70 of 241\ttrain_loss: 4.6191\n",
      "Iteration: 72 of 241\ttrain_loss: 4.4255\n",
      "Iteration: 74 of 241\ttrain_loss: 4.4978\n",
      "Iteration: 76 of 241\ttrain_loss: 4.5518\n",
      "Iteration: 78 of 241\ttrain_loss: 4.4503\n",
      "Iteration: 80 of 241\ttrain_loss: 4.7104\n",
      "Iteration: 82 of 241\ttrain_loss: 4.5501\n",
      "Iteration: 84 of 241\ttrain_loss: 4.6311\n",
      "Iteration: 86 of 241\ttrain_loss: 4.5734\n",
      "Iteration: 88 of 241\ttrain_loss: 4.6373\n",
      "Iteration: 90 of 241\ttrain_loss: 4.4737\n",
      "Iteration: 92 of 241\ttrain_loss: 4.6562\n",
      "Iteration: 94 of 241\ttrain_loss: 4.4454\n",
      "Iteration: 96 of 241\ttrain_loss: 4.5570\n",
      "Iteration: 98 of 241\ttrain_loss: 4.6513\n",
      "Iteration: 100 of 241\ttrain_loss: 4.4128\n",
      "Iteration: 102 of 241\ttrain_loss: 4.3446\n",
      "Iteration: 104 of 241\ttrain_loss: 4.6523\n",
      "Iteration: 106 of 241\ttrain_loss: 4.4677\n",
      "Iteration: 108 of 241\ttrain_loss: 4.6612\n",
      "Iteration: 110 of 241\ttrain_loss: 4.6322\n",
      "Iteration: 112 of 241\ttrain_loss: 4.8702\n",
      "Iteration: 114 of 241\ttrain_loss: 4.5760\n",
      "Iteration: 116 of 241\ttrain_loss: 4.6853\n",
      "Iteration: 118 of 241\ttrain_loss: 4.7032\n",
      "Iteration: 120 of 241\ttrain_loss: 4.6606\n",
      "Iteration: 122 of 241\ttrain_loss: 4.5122\n",
      "Iteration: 124 of 241\ttrain_loss: 4.5256\n",
      "Iteration: 126 of 241\ttrain_loss: 4.7616\n",
      "Iteration: 128 of 241\ttrain_loss: 4.5493\n",
      "Iteration: 130 of 241\ttrain_loss: 4.7294\n",
      "Iteration: 132 of 241\ttrain_loss: 4.6310\n",
      "Iteration: 134 of 241\ttrain_loss: 4.2078\n",
      "Iteration: 136 of 241\ttrain_loss: 4.4350\n",
      "Iteration: 138 of 241\ttrain_loss: 4.7949\n",
      "Iteration: 140 of 241\ttrain_loss: 4.6613\n",
      "Iteration: 142 of 241\ttrain_loss: 4.6422\n",
      "Iteration: 144 of 241\ttrain_loss: 4.4951\n",
      "Iteration: 146 of 241\ttrain_loss: 4.6643\n",
      "Iteration: 148 of 241\ttrain_loss: 4.4206\n",
      "Iteration: 150 of 241\ttrain_loss: 4.5551\n",
      "Iteration: 152 of 241\ttrain_loss: 4.6168\n",
      "Iteration: 154 of 241\ttrain_loss: 4.7659\n",
      "Iteration: 156 of 241\ttrain_loss: 4.4924\n",
      "Iteration: 158 of 241\ttrain_loss: 4.7930\n",
      "Iteration: 160 of 241\ttrain_loss: 4.4100\n",
      "Iteration: 162 of 241\ttrain_loss: 4.6319\n",
      "Iteration: 164 of 241\ttrain_loss: 4.6856\n",
      "Iteration: 166 of 241\ttrain_loss: 4.5751\n",
      "Iteration: 168 of 241\ttrain_loss: 4.3114\n",
      "Iteration: 170 of 241\ttrain_loss: 4.5239\n",
      "Iteration: 172 of 241\ttrain_loss: 4.6597\n",
      "Iteration: 174 of 241\ttrain_loss: 4.3246\n",
      "Iteration: 176 of 241\ttrain_loss: 4.5136\n",
      "Iteration: 178 of 241\ttrain_loss: 4.4657\n",
      "Iteration: 180 of 241\ttrain_loss: 4.2695\n",
      "Iteration: 182 of 241\ttrain_loss: 4.5559\n",
      "Iteration: 184 of 241\ttrain_loss: 4.7069\n",
      "Iteration: 186 of 241\ttrain_loss: 4.6035\n",
      "Iteration: 188 of 241\ttrain_loss: 4.4709\n",
      "Iteration: 190 of 241\ttrain_loss: 4.6024\n",
      "Iteration: 192 of 241\ttrain_loss: 4.7282\n",
      "Iteration: 194 of 241\ttrain_loss: 4.7781\n",
      "Iteration: 196 of 241\ttrain_loss: 4.7521\n",
      "Iteration: 198 of 241\ttrain_loss: 4.6016\n",
      "Iteration: 200 of 241\ttrain_loss: 4.6225\n",
      "Iteration: 202 of 241\ttrain_loss: 4.5743\n",
      "Iteration: 204 of 241\ttrain_loss: 4.9636\n",
      "Iteration: 206 of 241\ttrain_loss: 4.6034\n",
      "Iteration: 208 of 241\ttrain_loss: 4.8111\n",
      "Iteration: 210 of 241\ttrain_loss: 4.4123\n",
      "Iteration: 212 of 241\ttrain_loss: 4.2894\n",
      "Iteration: 214 of 241\ttrain_loss: 4.7605\n",
      "Iteration: 216 of 241\ttrain_loss: 4.6727\n",
      "Iteration: 218 of 241\ttrain_loss: 4.3220\n",
      "Iteration: 220 of 241\ttrain_loss: 4.5463\n",
      "Iteration: 222 of 241\ttrain_loss: 4.5641\n",
      "Iteration: 224 of 241\ttrain_loss: 4.7404\n",
      "Iteration: 226 of 241\ttrain_loss: 4.3573\n",
      "Iteration: 228 of 241\ttrain_loss: 4.5954\n",
      "Iteration: 230 of 241\ttrain_loss: 4.5751\n",
      "Iteration: 232 of 241\ttrain_loss: 4.7346\n",
      "Iteration: 234 of 241\ttrain_loss: 4.4056\n",
      "Iteration: 236 of 241\ttrain_loss: 4.4113\n",
      "Iteration: 238 of 241\ttrain_loss: 4.5665\n",
      "Iteration: 240 of 241\ttrain_loss: 4.1988\n",
      "Iteration: 241 of 241\ttrain_loss: 4.4743\n",
      "Average Score for this Epoch: 4.509164333343506\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 30 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.3260\n",
      "Iteration: 2 of 241\ttrain_loss: 4.4720\n",
      "Iteration: 4 of 241\ttrain_loss: 4.3143\n",
      "Iteration: 6 of 241\ttrain_loss: 4.3361\n",
      "Iteration: 8 of 241\ttrain_loss: 4.1285\n",
      "Iteration: 10 of 241\ttrain_loss: 4.1132\n",
      "Iteration: 12 of 241\ttrain_loss: 4.3903\n",
      "Iteration: 14 of 241\ttrain_loss: 4.4384\n",
      "Iteration: 16 of 241\ttrain_loss: 4.3747\n",
      "Iteration: 18 of 241\ttrain_loss: 4.5788\n",
      "Iteration: 20 of 241\ttrain_loss: 4.2442\n",
      "Iteration: 22 of 241\ttrain_loss: 4.5258\n",
      "Iteration: 24 of 241\ttrain_loss: 4.1847\n",
      "Iteration: 26 of 241\ttrain_loss: 4.2633\n",
      "Iteration: 28 of 241\ttrain_loss: 4.4551\n",
      "Iteration: 30 of 241\ttrain_loss: 4.2759\n",
      "Iteration: 32 of 241\ttrain_loss: 4.0028\n",
      "Iteration: 34 of 241\ttrain_loss: 4.3331\n",
      "Iteration: 36 of 241\ttrain_loss: 4.2084\n",
      "Iteration: 38 of 241\ttrain_loss: 4.5209\n",
      "Iteration: 40 of 241\ttrain_loss: 4.4183\n",
      "Iteration: 42 of 241\ttrain_loss: 4.3769\n",
      "Iteration: 44 of 241\ttrain_loss: 4.3597\n",
      "Iteration: 46 of 241\ttrain_loss: 4.4327\n",
      "Iteration: 48 of 241\ttrain_loss: 4.5249\n",
      "Iteration: 50 of 241\ttrain_loss: 4.6308\n",
      "Iteration: 52 of 241\ttrain_loss: 4.4580\n",
      "Iteration: 54 of 241\ttrain_loss: 4.3789\n",
      "Iteration: 56 of 241\ttrain_loss: 4.2962\n",
      "Iteration: 58 of 241\ttrain_loss: 4.5885\n",
      "Iteration: 60 of 241\ttrain_loss: 4.6886\n",
      "Iteration: 62 of 241\ttrain_loss: 4.3868\n",
      "Iteration: 64 of 241\ttrain_loss: 4.5212\n",
      "Iteration: 66 of 241\ttrain_loss: 4.5390\n",
      "Iteration: 68 of 241\ttrain_loss: 4.5043\n",
      "Iteration: 70 of 241\ttrain_loss: 4.7715\n",
      "Iteration: 72 of 241\ttrain_loss: 4.3310\n",
      "Iteration: 74 of 241\ttrain_loss: 4.5477\n",
      "Iteration: 76 of 241\ttrain_loss: 4.4274\n",
      "Iteration: 78 of 241\ttrain_loss: 4.6451\n",
      "Iteration: 80 of 241\ttrain_loss: 4.4266\n",
      "Iteration: 82 of 241\ttrain_loss: 4.2709\n",
      "Iteration: 84 of 241\ttrain_loss: 4.4382\n",
      "Iteration: 86 of 241\ttrain_loss: 4.4450\n",
      "Iteration: 88 of 241\ttrain_loss: 4.5948\n",
      "Iteration: 90 of 241\ttrain_loss: 4.2725\n",
      "Iteration: 92 of 241\ttrain_loss: 4.5768\n",
      "Iteration: 94 of 241\ttrain_loss: 4.5985\n",
      "Iteration: 96 of 241\ttrain_loss: 4.4313\n",
      "Iteration: 98 of 241\ttrain_loss: 4.4467\n",
      "Iteration: 100 of 241\ttrain_loss: 4.5431\n",
      "Iteration: 102 of 241\ttrain_loss: 4.6656\n",
      "Iteration: 104 of 241\ttrain_loss: 4.4374\n",
      "Iteration: 106 of 241\ttrain_loss: 4.5669\n",
      "Iteration: 108 of 241\ttrain_loss: 4.3723\n",
      "Iteration: 110 of 241\ttrain_loss: 4.3638\n",
      "Iteration: 112 of 241\ttrain_loss: 4.4812\n",
      "Iteration: 114 of 241\ttrain_loss: 4.3376\n",
      "Iteration: 116 of 241\ttrain_loss: 4.4769\n",
      "Iteration: 118 of 241\ttrain_loss: 4.6888\n",
      "Iteration: 120 of 241\ttrain_loss: 4.0617\n",
      "Iteration: 122 of 241\ttrain_loss: 4.5944\n",
      "Iteration: 124 of 241\ttrain_loss: 4.4583\n",
      "Iteration: 126 of 241\ttrain_loss: 4.2056\n",
      "Iteration: 128 of 241\ttrain_loss: 4.5099\n",
      "Iteration: 130 of 241\ttrain_loss: 4.1103\n",
      "Iteration: 132 of 241\ttrain_loss: 4.3659\n",
      "Iteration: 134 of 241\ttrain_loss: 4.2198\n",
      "Iteration: 136 of 241\ttrain_loss: 4.2265\n",
      "Iteration: 138 of 241\ttrain_loss: 4.4171\n",
      "Iteration: 140 of 241\ttrain_loss: 4.3273\n",
      "Iteration: 142 of 241\ttrain_loss: 4.3365\n",
      "Iteration: 144 of 241\ttrain_loss: 4.2873\n",
      "Iteration: 146 of 241\ttrain_loss: 4.2322\n",
      "Iteration: 148 of 241\ttrain_loss: 4.6316\n",
      "Iteration: 150 of 241\ttrain_loss: 4.3559\n",
      "Iteration: 152 of 241\ttrain_loss: 4.4405\n",
      "Iteration: 154 of 241\ttrain_loss: 4.1910\n",
      "Iteration: 156 of 241\ttrain_loss: 4.1948\n",
      "Iteration: 158 of 241\ttrain_loss: 4.3814\n",
      "Iteration: 160 of 241\ttrain_loss: 4.6235\n",
      "Iteration: 162 of 241\ttrain_loss: 4.4207\n",
      "Iteration: 164 of 241\ttrain_loss: 4.3818\n",
      "Iteration: 166 of 241\ttrain_loss: 4.5693\n",
      "Iteration: 168 of 241\ttrain_loss: 4.5620\n",
      "Iteration: 170 of 241\ttrain_loss: 4.5682\n",
      "Iteration: 172 of 241\ttrain_loss: 4.4441\n",
      "Iteration: 174 of 241\ttrain_loss: 4.4650\n",
      "Iteration: 176 of 241\ttrain_loss: 4.3011\n",
      "Iteration: 178 of 241\ttrain_loss: 4.3334\n",
      "Iteration: 180 of 241\ttrain_loss: 4.5806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 182 of 241\ttrain_loss: 4.6348\n",
      "Iteration: 184 of 241\ttrain_loss: 4.2743\n",
      "Iteration: 186 of 241\ttrain_loss: 4.4381\n",
      "Iteration: 188 of 241\ttrain_loss: 4.2897\n",
      "Iteration: 190 of 241\ttrain_loss: 4.1310\n",
      "Iteration: 192 of 241\ttrain_loss: 4.6502\n",
      "Iteration: 194 of 241\ttrain_loss: 4.5600\n",
      "Iteration: 196 of 241\ttrain_loss: 4.5017\n",
      "Iteration: 198 of 241\ttrain_loss: 4.5536\n",
      "Iteration: 200 of 241\ttrain_loss: 4.5851\n",
      "Iteration: 202 of 241\ttrain_loss: 4.3731\n",
      "Iteration: 204 of 241\ttrain_loss: 4.4720\n",
      "Iteration: 206 of 241\ttrain_loss: 4.3198\n",
      "Iteration: 208 of 241\ttrain_loss: 4.5133\n",
      "Iteration: 210 of 241\ttrain_loss: 4.2590\n",
      "Iteration: 212 of 241\ttrain_loss: 4.5670\n",
      "Iteration: 214 of 241\ttrain_loss: 4.3829\n",
      "Iteration: 216 of 241\ttrain_loss: 4.4577\n",
      "Iteration: 218 of 241\ttrain_loss: 4.3960\n",
      "Iteration: 220 of 241\ttrain_loss: 4.4799\n",
      "Iteration: 222 of 241\ttrain_loss: 4.4568\n",
      "Iteration: 224 of 241\ttrain_loss: 4.5465\n",
      "Iteration: 226 of 241\ttrain_loss: 4.5670\n",
      "Iteration: 228 of 241\ttrain_loss: 4.4308\n",
      "Iteration: 230 of 241\ttrain_loss: 4.3262\n",
      "Iteration: 232 of 241\ttrain_loss: 4.7165\n",
      "Iteration: 234 of 241\ttrain_loss: 4.6254\n",
      "Iteration: 236 of 241\ttrain_loss: 4.4610\n",
      "Iteration: 238 of 241\ttrain_loss: 4.5747\n",
      "Iteration: 240 of 241\ttrain_loss: 4.3465\n",
      "Iteration: 241 of 241\ttrain_loss: 4.7726\n",
      "Average Score for this Epoch: 4.410089492797852\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 31 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.3348\n",
      "Iteration: 2 of 241\ttrain_loss: 4.2925\n",
      "Iteration: 4 of 241\ttrain_loss: 4.0224\n",
      "Iteration: 6 of 241\ttrain_loss: 4.0166\n",
      "Iteration: 8 of 241\ttrain_loss: 4.0371\n",
      "Iteration: 10 of 241\ttrain_loss: 4.2006\n",
      "Iteration: 12 of 241\ttrain_loss: 4.1460\n",
      "Iteration: 14 of 241\ttrain_loss: 4.2601\n",
      "Iteration: 16 of 241\ttrain_loss: 4.1754\n",
      "Iteration: 18 of 241\ttrain_loss: 3.8781\n",
      "Iteration: 20 of 241\ttrain_loss: 4.1874\n",
      "Iteration: 22 of 241\ttrain_loss: 4.0283\n",
      "Iteration: 24 of 241\ttrain_loss: 4.4332\n",
      "Iteration: 26 of 241\ttrain_loss: 4.4246\n",
      "Iteration: 28 of 241\ttrain_loss: 4.0112\n",
      "Iteration: 30 of 241\ttrain_loss: 4.3426\n",
      "Iteration: 32 of 241\ttrain_loss: 3.9998\n",
      "Iteration: 34 of 241\ttrain_loss: 4.2118\n",
      "Iteration: 36 of 241\ttrain_loss: 3.8447\n",
      "Iteration: 38 of 241\ttrain_loss: 4.1501\n",
      "Iteration: 40 of 241\ttrain_loss: 4.4375\n",
      "Iteration: 42 of 241\ttrain_loss: 4.2574\n",
      "Iteration: 44 of 241\ttrain_loss: 4.3869\n",
      "Iteration: 46 of 241\ttrain_loss: 4.1198\n",
      "Iteration: 48 of 241\ttrain_loss: 4.0785\n",
      "Iteration: 50 of 241\ttrain_loss: 4.0898\n",
      "Iteration: 52 of 241\ttrain_loss: 4.4886\n",
      "Iteration: 54 of 241\ttrain_loss: 4.1944\n",
      "Iteration: 56 of 241\ttrain_loss: 4.3353\n",
      "Iteration: 58 of 241\ttrain_loss: 4.1061\n",
      "Iteration: 60 of 241\ttrain_loss: 4.3742\n",
      "Iteration: 62 of 241\ttrain_loss: 4.3763\n",
      "Iteration: 64 of 241\ttrain_loss: 4.3894\n",
      "Iteration: 66 of 241\ttrain_loss: 4.3192\n",
      "Iteration: 68 of 241\ttrain_loss: 4.1937\n",
      "Iteration: 70 of 241\ttrain_loss: 4.0928\n",
      "Iteration: 72 of 241\ttrain_loss: 4.3394\n",
      "Iteration: 74 of 241\ttrain_loss: 4.0589\n",
      "Iteration: 76 of 241\ttrain_loss: 4.0795\n",
      "Iteration: 78 of 241\ttrain_loss: 4.5564\n",
      "Iteration: 80 of 241\ttrain_loss: 4.2985\n",
      "Iteration: 82 of 241\ttrain_loss: 4.1499\n",
      "Iteration: 84 of 241\ttrain_loss: 4.1277\n",
      "Iteration: 86 of 241\ttrain_loss: 4.2236\n",
      "Iteration: 88 of 241\ttrain_loss: 4.0705\n",
      "Iteration: 90 of 241\ttrain_loss: 4.2502\n",
      "Iteration: 92 of 241\ttrain_loss: 4.3533\n",
      "Iteration: 94 of 241\ttrain_loss: 4.3109\n",
      "Iteration: 96 of 241\ttrain_loss: 4.1963\n",
      "Iteration: 98 of 241\ttrain_loss: 3.9830\n",
      "Iteration: 100 of 241\ttrain_loss: 4.0322\n",
      "Iteration: 102 of 241\ttrain_loss: 4.4285\n",
      "Iteration: 104 of 241\ttrain_loss: 4.3957\n",
      "Iteration: 106 of 241\ttrain_loss: 4.3751\n",
      "Iteration: 108 of 241\ttrain_loss: 4.2161\n",
      "Iteration: 110 of 241\ttrain_loss: 4.3346\n",
      "Iteration: 112 of 241\ttrain_loss: 4.2170\n",
      "Iteration: 114 of 241\ttrain_loss: 4.2616\n",
      "Iteration: 116 of 241\ttrain_loss: 4.2627\n",
      "Iteration: 118 of 241\ttrain_loss: 4.2066\n",
      "Iteration: 120 of 241\ttrain_loss: 4.4044\n",
      "Iteration: 122 of 241\ttrain_loss: 4.3047\n",
      "Iteration: 124 of 241\ttrain_loss: 4.2222\n",
      "Iteration: 126 of 241\ttrain_loss: 4.3684\n",
      "Iteration: 128 of 241\ttrain_loss: 4.3810\n",
      "Iteration: 130 of 241\ttrain_loss: 4.4640\n",
      "Iteration: 132 of 241\ttrain_loss: 4.1839\n",
      "Iteration: 134 of 241\ttrain_loss: 4.3049\n",
      "Iteration: 136 of 241\ttrain_loss: 4.1815\n",
      "Iteration: 138 of 241\ttrain_loss: 4.6912\n",
      "Iteration: 140 of 241\ttrain_loss: 4.2970\n",
      "Iteration: 142 of 241\ttrain_loss: 4.4077\n",
      "Iteration: 144 of 241\ttrain_loss: 4.2020\n",
      "Iteration: 146 of 241\ttrain_loss: 4.3353\n",
      "Iteration: 148 of 241\ttrain_loss: 4.0397\n",
      "Iteration: 150 of 241\ttrain_loss: 4.3497\n",
      "Iteration: 152 of 241\ttrain_loss: 4.3835\n",
      "Iteration: 154 of 241\ttrain_loss: 4.2564\n",
      "Iteration: 156 of 241\ttrain_loss: 4.0286\n",
      "Iteration: 158 of 241\ttrain_loss: 4.3524\n",
      "Iteration: 160 of 241\ttrain_loss: 4.1917\n",
      "Iteration: 162 of 241\ttrain_loss: 4.0508\n",
      "Iteration: 164 of 241\ttrain_loss: 4.3999\n",
      "Iteration: 166 of 241\ttrain_loss: 4.4486\n",
      "Iteration: 168 of 241\ttrain_loss: 3.9311\n",
      "Iteration: 170 of 241\ttrain_loss: 4.3116\n",
      "Iteration: 172 of 241\ttrain_loss: 4.3886\n",
      "Iteration: 174 of 241\ttrain_loss: 4.3154\n",
      "Iteration: 176 of 241\ttrain_loss: 4.2490\n",
      "Iteration: 178 of 241\ttrain_loss: 4.2580\n",
      "Iteration: 180 of 241\ttrain_loss: 4.3597\n",
      "Iteration: 182 of 241\ttrain_loss: 4.1954\n",
      "Iteration: 184 of 241\ttrain_loss: 4.2126\n",
      "Iteration: 186 of 241\ttrain_loss: 4.3068\n",
      "Iteration: 188 of 241\ttrain_loss: 4.2977\n",
      "Iteration: 190 of 241\ttrain_loss: 4.4620\n",
      "Iteration: 192 of 241\ttrain_loss: 4.3632\n",
      "Iteration: 194 of 241\ttrain_loss: 4.2481\n",
      "Iteration: 196 of 241\ttrain_loss: 4.0211\n",
      "Iteration: 198 of 241\ttrain_loss: 4.3615\n",
      "Iteration: 200 of 241\ttrain_loss: 4.3404\n",
      "Iteration: 202 of 241\ttrain_loss: 4.3098\n",
      "Iteration: 204 of 241\ttrain_loss: 4.3655\n",
      "Iteration: 206 of 241\ttrain_loss: 4.1472\n",
      "Iteration: 208 of 241\ttrain_loss: 4.5088\n",
      "Iteration: 210 of 241\ttrain_loss: 4.1174\n",
      "Iteration: 212 of 241\ttrain_loss: 4.6021\n",
      "Iteration: 214 of 241\ttrain_loss: 4.3589\n",
      "Iteration: 216 of 241\ttrain_loss: 4.4026\n",
      "Iteration: 218 of 241\ttrain_loss: 4.3120\n",
      "Iteration: 220 of 241\ttrain_loss: 4.4474\n",
      "Iteration: 222 of 241\ttrain_loss: 4.3885\n",
      "Iteration: 224 of 241\ttrain_loss: 4.6910\n",
      "Iteration: 226 of 241\ttrain_loss: 4.2940\n",
      "Iteration: 228 of 241\ttrain_loss: 4.2960\n",
      "Iteration: 230 of 241\ttrain_loss: 4.2615\n",
      "Iteration: 232 of 241\ttrain_loss: 4.3228\n",
      "Iteration: 234 of 241\ttrain_loss: 4.3106\n",
      "Iteration: 236 of 241\ttrain_loss: 4.5501\n",
      "Iteration: 238 of 241\ttrain_loss: 4.4700\n",
      "Iteration: 240 of 241\ttrain_loss: 4.5946\n",
      "Iteration: 241 of 241\ttrain_loss: 4.3954\n",
      "Average Score for this Epoch: 4.284936904907227\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 32 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 4.0749\n",
      "Iteration: 2 of 241\ttrain_loss: 3.9751\n",
      "Iteration: 4 of 241\ttrain_loss: 4.3806\n",
      "Iteration: 6 of 241\ttrain_loss: 3.9127\n",
      "Iteration: 8 of 241\ttrain_loss: 4.5076\n",
      "Iteration: 10 of 241\ttrain_loss: 4.0956\n",
      "Iteration: 12 of 241\ttrain_loss: 4.1168\n",
      "Iteration: 14 of 241\ttrain_loss: 4.1075\n",
      "Iteration: 16 of 241\ttrain_loss: 4.1819\n",
      "Iteration: 18 of 241\ttrain_loss: 4.0137\n",
      "Iteration: 20 of 241\ttrain_loss: 3.8318\n",
      "Iteration: 22 of 241\ttrain_loss: 4.0962\n",
      "Iteration: 24 of 241\ttrain_loss: 4.3124\n",
      "Iteration: 26 of 241\ttrain_loss: 4.1735\n",
      "Iteration: 28 of 241\ttrain_loss: 3.9682\n",
      "Iteration: 30 of 241\ttrain_loss: 4.3850\n",
      "Iteration: 32 of 241\ttrain_loss: 4.1921\n",
      "Iteration: 34 of 241\ttrain_loss: 4.0904\n",
      "Iteration: 36 of 241\ttrain_loss: 4.0239\n",
      "Iteration: 38 of 241\ttrain_loss: 4.1245\n",
      "Iteration: 40 of 241\ttrain_loss: 4.1001\n",
      "Iteration: 42 of 241\ttrain_loss: 3.9624\n",
      "Iteration: 44 of 241\ttrain_loss: 3.9609\n",
      "Iteration: 46 of 241\ttrain_loss: 3.8665\n",
      "Iteration: 48 of 241\ttrain_loss: 4.4133\n",
      "Iteration: 50 of 241\ttrain_loss: 4.0693\n",
      "Iteration: 52 of 241\ttrain_loss: 4.1858\n",
      "Iteration: 54 of 241\ttrain_loss: 3.8535\n",
      "Iteration: 56 of 241\ttrain_loss: 4.2943\n",
      "Iteration: 58 of 241\ttrain_loss: 3.9190\n",
      "Iteration: 60 of 241\ttrain_loss: 4.0996\n",
      "Iteration: 62 of 241\ttrain_loss: 4.3942\n",
      "Iteration: 64 of 241\ttrain_loss: 4.2163\n",
      "Iteration: 66 of 241\ttrain_loss: 3.7594\n",
      "Iteration: 68 of 241\ttrain_loss: 4.1078\n",
      "Iteration: 70 of 241\ttrain_loss: 4.1985\n",
      "Iteration: 72 of 241\ttrain_loss: 4.1136\n",
      "Iteration: 74 of 241\ttrain_loss: 4.2501\n",
      "Iteration: 76 of 241\ttrain_loss: 4.0286\n",
      "Iteration: 78 of 241\ttrain_loss: 4.2769\n",
      "Iteration: 80 of 241\ttrain_loss: 4.1247\n",
      "Iteration: 82 of 241\ttrain_loss: 3.9988\n",
      "Iteration: 84 of 241\ttrain_loss: 3.8244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 86 of 241\ttrain_loss: 3.8437\n",
      "Iteration: 88 of 241\ttrain_loss: 3.9679\n",
      "Iteration: 90 of 241\ttrain_loss: 4.3526\n",
      "Iteration: 92 of 241\ttrain_loss: 4.5847\n",
      "Iteration: 94 of 241\ttrain_loss: 4.1986\n",
      "Iteration: 96 of 241\ttrain_loss: 4.1812\n",
      "Iteration: 98 of 241\ttrain_loss: 4.0793\n",
      "Iteration: 100 of 241\ttrain_loss: 4.0243\n",
      "Iteration: 102 of 241\ttrain_loss: 4.1585\n",
      "Iteration: 104 of 241\ttrain_loss: 3.9789\n",
      "Iteration: 106 of 241\ttrain_loss: 3.9749\n",
      "Iteration: 108 of 241\ttrain_loss: 3.9969\n",
      "Iteration: 110 of 241\ttrain_loss: 4.2133\n",
      "Iteration: 112 of 241\ttrain_loss: 4.0389\n",
      "Iteration: 114 of 241\ttrain_loss: 4.2098\n",
      "Iteration: 116 of 241\ttrain_loss: 4.2013\n",
      "Iteration: 118 of 241\ttrain_loss: 4.4682\n",
      "Iteration: 120 of 241\ttrain_loss: 3.9722\n",
      "Iteration: 122 of 241\ttrain_loss: 4.0910\n",
      "Iteration: 124 of 241\ttrain_loss: 4.1762\n",
      "Iteration: 126 of 241\ttrain_loss: 4.3068\n",
      "Iteration: 128 of 241\ttrain_loss: 4.0313\n",
      "Iteration: 130 of 241\ttrain_loss: 4.2231\n",
      "Iteration: 132 of 241\ttrain_loss: 4.2194\n",
      "Iteration: 134 of 241\ttrain_loss: 3.9460\n",
      "Iteration: 136 of 241\ttrain_loss: 4.1298\n",
      "Iteration: 138 of 241\ttrain_loss: 4.1318\n",
      "Iteration: 140 of 241\ttrain_loss: 4.3246\n",
      "Iteration: 142 of 241\ttrain_loss: 4.1034\n",
      "Iteration: 144 of 241\ttrain_loss: 3.8965\n",
      "Iteration: 146 of 241\ttrain_loss: 3.9517\n",
      "Iteration: 148 of 241\ttrain_loss: 4.2582\n",
      "Iteration: 150 of 241\ttrain_loss: 4.2081\n",
      "Iteration: 152 of 241\ttrain_loss: 4.1004\n",
      "Iteration: 154 of 241\ttrain_loss: 4.1839\n",
      "Iteration: 156 of 241\ttrain_loss: 4.7377\n",
      "Iteration: 158 of 241\ttrain_loss: 4.1765\n",
      "Iteration: 160 of 241\ttrain_loss: 4.5443\n",
      "Iteration: 162 of 241\ttrain_loss: 4.0624\n",
      "Iteration: 164 of 241\ttrain_loss: 4.1526\n",
      "Iteration: 166 of 241\ttrain_loss: 4.0901\n",
      "Iteration: 168 of 241\ttrain_loss: 4.2082\n",
      "Iteration: 170 of 241\ttrain_loss: 4.3175\n",
      "Iteration: 172 of 241\ttrain_loss: 4.1333\n",
      "Iteration: 174 of 241\ttrain_loss: 4.1374\n",
      "Iteration: 176 of 241\ttrain_loss: 4.3639\n",
      "Iteration: 178 of 241\ttrain_loss: 4.2746\n",
      "Iteration: 180 of 241\ttrain_loss: 4.0703\n",
      "Iteration: 182 of 241\ttrain_loss: 4.1676\n",
      "Iteration: 184 of 241\ttrain_loss: 4.2049\n",
      "Iteration: 186 of 241\ttrain_loss: 3.8008\n",
      "Iteration: 188 of 241\ttrain_loss: 4.1348\n",
      "Iteration: 190 of 241\ttrain_loss: 4.2009\n",
      "Iteration: 192 of 241\ttrain_loss: 4.4092\n",
      "Iteration: 194 of 241\ttrain_loss: 4.3933\n",
      "Iteration: 196 of 241\ttrain_loss: 4.3617\n",
      "Iteration: 198 of 241\ttrain_loss: 4.4581\n",
      "Iteration: 200 of 241\ttrain_loss: 4.1848\n",
      "Iteration: 202 of 241\ttrain_loss: 4.2915\n",
      "Iteration: 204 of 241\ttrain_loss: 4.6171\n",
      "Iteration: 206 of 241\ttrain_loss: 4.2726\n",
      "Iteration: 208 of 241\ttrain_loss: 4.3531\n",
      "Iteration: 210 of 241\ttrain_loss: 4.1675\n",
      "Iteration: 212 of 241\ttrain_loss: 4.3724\n",
      "Iteration: 214 of 241\ttrain_loss: 4.3282\n",
      "Iteration: 216 of 241\ttrain_loss: 4.3511\n",
      "Iteration: 218 of 241\ttrain_loss: 4.5868\n",
      "Iteration: 220 of 241\ttrain_loss: 4.0960\n",
      "Iteration: 222 of 241\ttrain_loss: 4.1947\n",
      "Iteration: 224 of 241\ttrain_loss: 4.0347\n",
      "Iteration: 226 of 241\ttrain_loss: 4.4390\n",
      "Iteration: 228 of 241\ttrain_loss: 3.7988\n",
      "Iteration: 230 of 241\ttrain_loss: 4.1637\n",
      "Iteration: 232 of 241\ttrain_loss: 4.3642\n",
      "Iteration: 234 of 241\ttrain_loss: 3.9669\n",
      "Iteration: 236 of 241\ttrain_loss: 4.2089\n",
      "Iteration: 238 of 241\ttrain_loss: 4.0609\n",
      "Iteration: 240 of 241\ttrain_loss: 4.2820\n",
      "Iteration: 241 of 241\ttrain_loss: 4.2392\n",
      "Average Score for this Epoch: 4.1581501960754395\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 33 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.9303\n",
      "Iteration: 2 of 241\ttrain_loss: 4.1418\n",
      "Iteration: 4 of 241\ttrain_loss: 3.9960\n",
      "Iteration: 6 of 241\ttrain_loss: 3.8488\n",
      "Iteration: 8 of 241\ttrain_loss: 3.7092\n",
      "Iteration: 10 of 241\ttrain_loss: 3.7849\n",
      "Iteration: 12 of 241\ttrain_loss: 4.0581\n",
      "Iteration: 14 of 241\ttrain_loss: 3.9053\n",
      "Iteration: 16 of 241\ttrain_loss: 4.0424\n",
      "Iteration: 18 of 241\ttrain_loss: 4.1147\n",
      "Iteration: 20 of 241\ttrain_loss: 3.6910\n",
      "Iteration: 22 of 241\ttrain_loss: 4.0994\n",
      "Iteration: 24 of 241\ttrain_loss: 3.8023\n",
      "Iteration: 26 of 241\ttrain_loss: 4.0059\n",
      "Iteration: 28 of 241\ttrain_loss: 4.0091\n",
      "Iteration: 30 of 241\ttrain_loss: 3.8618\n",
      "Iteration: 32 of 241\ttrain_loss: 4.0364\n",
      "Iteration: 34 of 241\ttrain_loss: 3.8371\n",
      "Iteration: 36 of 241\ttrain_loss: 3.8327\n",
      "Iteration: 38 of 241\ttrain_loss: 3.9338\n",
      "Iteration: 40 of 241\ttrain_loss: 3.8712\n",
      "Iteration: 42 of 241\ttrain_loss: 4.0400\n",
      "Iteration: 44 of 241\ttrain_loss: 3.8647\n",
      "Iteration: 46 of 241\ttrain_loss: 3.9611\n",
      "Iteration: 48 of 241\ttrain_loss: 4.2071\n",
      "Iteration: 50 of 241\ttrain_loss: 3.8709\n",
      "Iteration: 52 of 241\ttrain_loss: 3.9761\n",
      "Iteration: 54 of 241\ttrain_loss: 3.9212\n",
      "Iteration: 56 of 241\ttrain_loss: 3.7533\n",
      "Iteration: 58 of 241\ttrain_loss: 3.9555\n",
      "Iteration: 60 of 241\ttrain_loss: 3.8856\n",
      "Iteration: 62 of 241\ttrain_loss: 3.8495\n",
      "Iteration: 64 of 241\ttrain_loss: 3.9670\n",
      "Iteration: 66 of 241\ttrain_loss: 3.6035\n",
      "Iteration: 68 of 241\ttrain_loss: 4.1509\n",
      "Iteration: 70 of 241\ttrain_loss: 4.1383\n",
      "Iteration: 72 of 241\ttrain_loss: 4.1321\n",
      "Iteration: 74 of 241\ttrain_loss: 4.1403\n",
      "Iteration: 76 of 241\ttrain_loss: 3.9404\n",
      "Iteration: 78 of 241\ttrain_loss: 4.0270\n",
      "Iteration: 80 of 241\ttrain_loss: 3.9645\n",
      "Iteration: 82 of 241\ttrain_loss: 4.0223\n",
      "Iteration: 84 of 241\ttrain_loss: 3.9528\n",
      "Iteration: 86 of 241\ttrain_loss: 4.2257\n",
      "Iteration: 88 of 241\ttrain_loss: 3.9639\n",
      "Iteration: 90 of 241\ttrain_loss: 3.8285\n",
      "Iteration: 92 of 241\ttrain_loss: 3.8187\n",
      "Iteration: 94 of 241\ttrain_loss: 4.2629\n",
      "Iteration: 96 of 241\ttrain_loss: 3.9169\n",
      "Iteration: 98 of 241\ttrain_loss: 3.9972\n",
      "Iteration: 100 of 241\ttrain_loss: 4.2220\n",
      "Iteration: 102 of 241\ttrain_loss: 4.1606\n",
      "Iteration: 104 of 241\ttrain_loss: 4.3043\n",
      "Iteration: 106 of 241\ttrain_loss: 4.1010\n",
      "Iteration: 108 of 241\ttrain_loss: 4.1834\n",
      "Iteration: 110 of 241\ttrain_loss: 3.9585\n",
      "Iteration: 112 of 241\ttrain_loss: 3.7989\n",
      "Iteration: 114 of 241\ttrain_loss: 4.1100\n",
      "Iteration: 116 of 241\ttrain_loss: 4.0942\n",
      "Iteration: 118 of 241\ttrain_loss: 4.1870\n",
      "Iteration: 120 of 241\ttrain_loss: 4.2300\n",
      "Iteration: 122 of 241\ttrain_loss: 3.9038\n",
      "Iteration: 124 of 241\ttrain_loss: 4.2224\n",
      "Iteration: 126 of 241\ttrain_loss: 4.1248\n",
      "Iteration: 128 of 241\ttrain_loss: 4.2159\n",
      "Iteration: 130 of 241\ttrain_loss: 4.1297\n",
      "Iteration: 132 of 241\ttrain_loss: 4.1265\n",
      "Iteration: 134 of 241\ttrain_loss: 4.2030\n",
      "Iteration: 136 of 241\ttrain_loss: 4.3455\n",
      "Iteration: 138 of 241\ttrain_loss: 4.0390\n",
      "Iteration: 140 of 241\ttrain_loss: 4.2008\n",
      "Iteration: 142 of 241\ttrain_loss: 4.1234\n",
      "Iteration: 144 of 241\ttrain_loss: 4.2956\n",
      "Iteration: 146 of 241\ttrain_loss: 4.3935\n",
      "Iteration: 148 of 241\ttrain_loss: 3.8120\n",
      "Iteration: 150 of 241\ttrain_loss: 4.0981\n",
      "Iteration: 152 of 241\ttrain_loss: 4.4655\n",
      "Iteration: 154 of 241\ttrain_loss: 4.3306\n",
      "Iteration: 156 of 241\ttrain_loss: 4.2537\n",
      "Iteration: 158 of 241\ttrain_loss: 3.8323\n",
      "Iteration: 160 of 241\ttrain_loss: 3.9777\n",
      "Iteration: 162 of 241\ttrain_loss: 3.9963\n",
      "Iteration: 164 of 241\ttrain_loss: 3.9994\n",
      "Iteration: 166 of 241\ttrain_loss: 3.8286\n",
      "Iteration: 168 of 241\ttrain_loss: 4.2917\n",
      "Iteration: 170 of 241\ttrain_loss: 4.0268\n",
      "Iteration: 172 of 241\ttrain_loss: 4.2225\n",
      "Iteration: 174 of 241\ttrain_loss: 4.2822\n",
      "Iteration: 176 of 241\ttrain_loss: 4.2627\n",
      "Iteration: 178 of 241\ttrain_loss: 4.0238\n",
      "Iteration: 180 of 241\ttrain_loss: 4.2898\n",
      "Iteration: 182 of 241\ttrain_loss: 4.3259\n",
      "Iteration: 184 of 241\ttrain_loss: 4.1463\n",
      "Iteration: 186 of 241\ttrain_loss: 3.9257\n",
      "Iteration: 188 of 241\ttrain_loss: 4.1040\n",
      "Iteration: 190 of 241\ttrain_loss: 4.0352\n",
      "Iteration: 192 of 241\ttrain_loss: 4.1948\n",
      "Iteration: 194 of 241\ttrain_loss: 3.8211\n",
      "Iteration: 196 of 241\ttrain_loss: 3.9350\n",
      "Iteration: 198 of 241\ttrain_loss: 3.8648\n",
      "Iteration: 200 of 241\ttrain_loss: 4.2128\n",
      "Iteration: 202 of 241\ttrain_loss: 4.4106\n",
      "Iteration: 204 of 241\ttrain_loss: 4.2593\n",
      "Iteration: 206 of 241\ttrain_loss: 4.1738\n",
      "Iteration: 208 of 241\ttrain_loss: 3.8326\n",
      "Iteration: 210 of 241\ttrain_loss: 3.8275\n",
      "Iteration: 212 of 241\ttrain_loss: 3.9975\n",
      "Iteration: 214 of 241\ttrain_loss: 4.0409\n",
      "Iteration: 216 of 241\ttrain_loss: 3.9531\n",
      "Iteration: 218 of 241\ttrain_loss: 3.9209\n",
      "Iteration: 220 of 241\ttrain_loss: 4.0066\n",
      "Iteration: 222 of 241\ttrain_loss: 4.1081\n",
      "Iteration: 224 of 241\ttrain_loss: 3.9250\n",
      "Iteration: 226 of 241\ttrain_loss: 4.0514\n",
      "Iteration: 228 of 241\ttrain_loss: 4.3157\n",
      "Iteration: 230 of 241\ttrain_loss: 4.1796\n",
      "Iteration: 232 of 241\ttrain_loss: 3.7883\n",
      "Iteration: 234 of 241\ttrain_loss: 3.8862\n",
      "Iteration: 236 of 241\ttrain_loss: 4.0275\n",
      "Iteration: 238 of 241\ttrain_loss: 4.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 240 of 241\ttrain_loss: 4.0427\n",
      "Iteration: 241 of 241\ttrain_loss: 3.8102\n",
      "Average Score for this Epoch: 4.045661449432373\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 34 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.7955\n",
      "Iteration: 2 of 241\ttrain_loss: 3.9546\n",
      "Iteration: 4 of 241\ttrain_loss: 3.6836\n",
      "Iteration: 6 of 241\ttrain_loss: 3.7054\n",
      "Iteration: 8 of 241\ttrain_loss: 3.6633\n",
      "Iteration: 10 of 241\ttrain_loss: 3.6264\n",
      "Iteration: 12 of 241\ttrain_loss: 3.7309\n",
      "Iteration: 14 of 241\ttrain_loss: 3.6683\n",
      "Iteration: 16 of 241\ttrain_loss: 3.8629\n",
      "Iteration: 18 of 241\ttrain_loss: 3.7079\n",
      "Iteration: 20 of 241\ttrain_loss: 4.0693\n",
      "Iteration: 22 of 241\ttrain_loss: 3.9396\n",
      "Iteration: 24 of 241\ttrain_loss: 4.3223\n",
      "Iteration: 26 of 241\ttrain_loss: 4.0811\n",
      "Iteration: 28 of 241\ttrain_loss: 4.0436\n",
      "Iteration: 30 of 241\ttrain_loss: 3.8297\n",
      "Iteration: 32 of 241\ttrain_loss: 3.8257\n",
      "Iteration: 34 of 241\ttrain_loss: 3.9140\n",
      "Iteration: 36 of 241\ttrain_loss: 3.9928\n",
      "Iteration: 38 of 241\ttrain_loss: 3.6131\n",
      "Iteration: 40 of 241\ttrain_loss: 3.9768\n",
      "Iteration: 42 of 241\ttrain_loss: 3.8994\n",
      "Iteration: 44 of 241\ttrain_loss: 3.9284\n",
      "Iteration: 46 of 241\ttrain_loss: 3.9324\n",
      "Iteration: 48 of 241\ttrain_loss: 4.1745\n",
      "Iteration: 50 of 241\ttrain_loss: 4.1285\n",
      "Iteration: 52 of 241\ttrain_loss: 4.0046\n",
      "Iteration: 54 of 241\ttrain_loss: 3.9930\n",
      "Iteration: 56 of 241\ttrain_loss: 4.0589\n",
      "Iteration: 58 of 241\ttrain_loss: 3.8993\n",
      "Iteration: 60 of 241\ttrain_loss: 3.8619\n",
      "Iteration: 62 of 241\ttrain_loss: 3.8969\n",
      "Iteration: 64 of 241\ttrain_loss: 3.5358\n",
      "Iteration: 66 of 241\ttrain_loss: 4.2555\n",
      "Iteration: 68 of 241\ttrain_loss: 3.8403\n",
      "Iteration: 70 of 241\ttrain_loss: 3.9051\n",
      "Iteration: 72 of 241\ttrain_loss: 4.1183\n",
      "Iteration: 74 of 241\ttrain_loss: 4.2833\n",
      "Iteration: 76 of 241\ttrain_loss: 4.0809\n",
      "Iteration: 78 of 241\ttrain_loss: 3.8961\n",
      "Iteration: 80 of 241\ttrain_loss: 4.2336\n",
      "Iteration: 82 of 241\ttrain_loss: 4.2958\n",
      "Iteration: 84 of 241\ttrain_loss: 3.8157\n",
      "Iteration: 86 of 241\ttrain_loss: 4.0480\n",
      "Iteration: 88 of 241\ttrain_loss: 3.8746\n",
      "Iteration: 90 of 241\ttrain_loss: 3.9051\n",
      "Iteration: 92 of 241\ttrain_loss: 4.1016\n",
      "Iteration: 94 of 241\ttrain_loss: 4.0484\n",
      "Iteration: 96 of 241\ttrain_loss: 3.9784\n",
      "Iteration: 98 of 241\ttrain_loss: 4.2863\n",
      "Iteration: 100 of 241\ttrain_loss: 4.1469\n",
      "Iteration: 102 of 241\ttrain_loss: 3.9207\n",
      "Iteration: 104 of 241\ttrain_loss: 3.9890\n",
      "Iteration: 106 of 241\ttrain_loss: 4.2207\n",
      "Iteration: 108 of 241\ttrain_loss: 4.2137\n",
      "Iteration: 110 of 241\ttrain_loss: 4.0610\n",
      "Iteration: 112 of 241\ttrain_loss: 4.0204\n",
      "Iteration: 114 of 241\ttrain_loss: 3.9404\n",
      "Iteration: 116 of 241\ttrain_loss: 4.0793\n",
      "Iteration: 118 of 241\ttrain_loss: 4.0298\n",
      "Iteration: 120 of 241\ttrain_loss: 3.6475\n",
      "Iteration: 122 of 241\ttrain_loss: 4.2451\n",
      "Iteration: 124 of 241\ttrain_loss: 3.9924\n",
      "Iteration: 126 of 241\ttrain_loss: 4.1950\n",
      "Iteration: 128 of 241\ttrain_loss: 3.8281\n",
      "Iteration: 130 of 241\ttrain_loss: 4.0853\n",
      "Iteration: 132 of 241\ttrain_loss: 3.5633\n",
      "Iteration: 134 of 241\ttrain_loss: 4.1509\n",
      "Iteration: 136 of 241\ttrain_loss: 4.0883\n",
      "Iteration: 138 of 241\ttrain_loss: 3.9994\n",
      "Iteration: 140 of 241\ttrain_loss: 4.0580\n",
      "Iteration: 142 of 241\ttrain_loss: 3.6846\n",
      "Iteration: 144 of 241\ttrain_loss: 4.1385\n",
      "Iteration: 146 of 241\ttrain_loss: 4.0433\n",
      "Iteration: 148 of 241\ttrain_loss: 3.8698\n",
      "Iteration: 150 of 241\ttrain_loss: 4.1974\n",
      "Iteration: 152 of 241\ttrain_loss: 4.0084\n",
      "Iteration: 154 of 241\ttrain_loss: 3.9749\n",
      "Iteration: 156 of 241\ttrain_loss: 4.1142\n",
      "Iteration: 158 of 241\ttrain_loss: 3.9901\n",
      "Iteration: 160 of 241\ttrain_loss: 4.1503\n",
      "Iteration: 162 of 241\ttrain_loss: 3.9547\n",
      "Iteration: 164 of 241\ttrain_loss: 4.1143\n",
      "Iteration: 166 of 241\ttrain_loss: 3.9752\n",
      "Iteration: 168 of 241\ttrain_loss: 3.9721\n",
      "Iteration: 170 of 241\ttrain_loss: 4.6443\n",
      "Iteration: 172 of 241\ttrain_loss: 3.8332\n",
      "Iteration: 174 of 241\ttrain_loss: 3.7928\n",
      "Iteration: 176 of 241\ttrain_loss: 4.0372\n",
      "Iteration: 178 of 241\ttrain_loss: 3.8686\n",
      "Iteration: 180 of 241\ttrain_loss: 3.6243\n",
      "Iteration: 182 of 241\ttrain_loss: 3.7577\n",
      "Iteration: 184 of 241\ttrain_loss: 3.9334\n",
      "Iteration: 186 of 241\ttrain_loss: 4.0085\n",
      "Iteration: 188 of 241\ttrain_loss: 4.0284\n",
      "Iteration: 190 of 241\ttrain_loss: 4.0812\n",
      "Iteration: 192 of 241\ttrain_loss: 4.2803\n",
      "Iteration: 194 of 241\ttrain_loss: 3.8312\n",
      "Iteration: 196 of 241\ttrain_loss: 3.7883\n",
      "Iteration: 198 of 241\ttrain_loss: 3.8154\n",
      "Iteration: 200 of 241\ttrain_loss: 3.7992\n",
      "Iteration: 202 of 241\ttrain_loss: 3.8582\n",
      "Iteration: 204 of 241\ttrain_loss: 4.0983\n",
      "Iteration: 206 of 241\ttrain_loss: 3.7035\n",
      "Iteration: 208 of 241\ttrain_loss: 4.0164\n",
      "Iteration: 210 of 241\ttrain_loss: 3.9171\n",
      "Iteration: 212 of 241\ttrain_loss: 3.6164\n",
      "Iteration: 214 of 241\ttrain_loss: 3.8563\n",
      "Iteration: 216 of 241\ttrain_loss: 3.7534\n",
      "Iteration: 218 of 241\ttrain_loss: 3.9443\n",
      "Iteration: 220 of 241\ttrain_loss: 4.0784\n",
      "Iteration: 222 of 241\ttrain_loss: 3.9854\n",
      "Iteration: 224 of 241\ttrain_loss: 3.9332\n",
      "Iteration: 226 of 241\ttrain_loss: 3.8872\n",
      "Iteration: 228 of 241\ttrain_loss: 3.8013\n",
      "Iteration: 230 of 241\ttrain_loss: 4.0120\n",
      "Iteration: 232 of 241\ttrain_loss: 3.6178\n",
      "Iteration: 234 of 241\ttrain_loss: 3.8898\n",
      "Iteration: 236 of 241\ttrain_loss: 4.3798\n",
      "Iteration: 238 of 241\ttrain_loss: 3.8334\n",
      "Iteration: 240 of 241\ttrain_loss: 3.9101\n",
      "Iteration: 241 of 241\ttrain_loss: 4.3809\n",
      "Average Score for this Epoch: 3.9724700450897217\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 35 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.8016\n",
      "Iteration: 2 of 241\ttrain_loss: 3.5615\n",
      "Iteration: 4 of 241\ttrain_loss: 3.6100\n",
      "Iteration: 6 of 241\ttrain_loss: 3.8832\n",
      "Iteration: 8 of 241\ttrain_loss: 3.7746\n",
      "Iteration: 10 of 241\ttrain_loss: 3.5821\n",
      "Iteration: 12 of 241\ttrain_loss: 3.8817\n",
      "Iteration: 14 of 241\ttrain_loss: 3.5943\n",
      "Iteration: 16 of 241\ttrain_loss: 3.5178\n",
      "Iteration: 18 of 241\ttrain_loss: 3.9795\n",
      "Iteration: 20 of 241\ttrain_loss: 3.6486\n",
      "Iteration: 22 of 241\ttrain_loss: 3.8634\n",
      "Iteration: 24 of 241\ttrain_loss: 3.6288\n",
      "Iteration: 26 of 241\ttrain_loss: 3.8599\n",
      "Iteration: 28 of 241\ttrain_loss: 3.4593\n",
      "Iteration: 30 of 241\ttrain_loss: 3.6579\n",
      "Iteration: 32 of 241\ttrain_loss: 3.9093\n",
      "Iteration: 34 of 241\ttrain_loss: 3.8943\n",
      "Iteration: 36 of 241\ttrain_loss: 3.9586\n",
      "Iteration: 38 of 241\ttrain_loss: 3.6566\n",
      "Iteration: 40 of 241\ttrain_loss: 3.7691\n",
      "Iteration: 42 of 241\ttrain_loss: 3.5759\n",
      "Iteration: 44 of 241\ttrain_loss: 3.8166\n",
      "Iteration: 46 of 241\ttrain_loss: 3.8012\n",
      "Iteration: 48 of 241\ttrain_loss: 3.7674\n",
      "Iteration: 50 of 241\ttrain_loss: 3.9754\n",
      "Iteration: 52 of 241\ttrain_loss: 4.0080\n",
      "Iteration: 54 of 241\ttrain_loss: 3.6286\n",
      "Iteration: 56 of 241\ttrain_loss: 3.8225\n",
      "Iteration: 58 of 241\ttrain_loss: 3.8612\n",
      "Iteration: 60 of 241\ttrain_loss: 3.8843\n",
      "Iteration: 62 of 241\ttrain_loss: 3.9476\n",
      "Iteration: 64 of 241\ttrain_loss: 3.6752\n",
      "Iteration: 66 of 241\ttrain_loss: 3.6339\n",
      "Iteration: 68 of 241\ttrain_loss: 3.6808\n",
      "Iteration: 70 of 241\ttrain_loss: 3.7221\n",
      "Iteration: 72 of 241\ttrain_loss: 3.9083\n",
      "Iteration: 74 of 241\ttrain_loss: 3.7511\n",
      "Iteration: 76 of 241\ttrain_loss: 3.8879\n",
      "Iteration: 78 of 241\ttrain_loss: 3.8567\n",
      "Iteration: 80 of 241\ttrain_loss: 3.9918\n",
      "Iteration: 82 of 241\ttrain_loss: 3.7698\n",
      "Iteration: 84 of 241\ttrain_loss: 4.1659\n",
      "Iteration: 86 of 241\ttrain_loss: 3.9309\n",
      "Iteration: 88 of 241\ttrain_loss: 3.7799\n",
      "Iteration: 90 of 241\ttrain_loss: 3.9436\n",
      "Iteration: 92 of 241\ttrain_loss: 3.8374\n",
      "Iteration: 94 of 241\ttrain_loss: 3.8896\n",
      "Iteration: 96 of 241\ttrain_loss: 3.8962\n",
      "Iteration: 98 of 241\ttrain_loss: 4.1473\n",
      "Iteration: 100 of 241\ttrain_loss: 3.7168\n",
      "Iteration: 102 of 241\ttrain_loss: 3.5663\n",
      "Iteration: 104 of 241\ttrain_loss: 3.9688\n",
      "Iteration: 106 of 241\ttrain_loss: 3.9059\n",
      "Iteration: 108 of 241\ttrain_loss: 3.8233\n",
      "Iteration: 110 of 241\ttrain_loss: 3.9760\n",
      "Iteration: 112 of 241\ttrain_loss: 4.0595\n",
      "Iteration: 114 of 241\ttrain_loss: 4.0116\n",
      "Iteration: 116 of 241\ttrain_loss: 3.6482\n",
      "Iteration: 118 of 241\ttrain_loss: 4.0613\n",
      "Iteration: 120 of 241\ttrain_loss: 3.9572\n",
      "Iteration: 122 of 241\ttrain_loss: 3.5333\n",
      "Iteration: 124 of 241\ttrain_loss: 3.8252\n",
      "Iteration: 126 of 241\ttrain_loss: 3.7882\n",
      "Iteration: 128 of 241\ttrain_loss: 3.9313\n",
      "Iteration: 130 of 241\ttrain_loss: 4.0142\n",
      "Iteration: 132 of 241\ttrain_loss: 3.8454\n",
      "Iteration: 134 of 241\ttrain_loss: 3.8112\n",
      "Iteration: 136 of 241\ttrain_loss: 3.7181\n",
      "Iteration: 138 of 241\ttrain_loss: 3.8341\n",
      "Iteration: 140 of 241\ttrain_loss: 3.8481\n",
      "Iteration: 142 of 241\ttrain_loss: 3.9788\n",
      "Iteration: 144 of 241\ttrain_loss: 3.9701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 146 of 241\ttrain_loss: 3.8561\n",
      "Iteration: 148 of 241\ttrain_loss: 4.0532\n",
      "Iteration: 150 of 241\ttrain_loss: 3.7654\n",
      "Iteration: 152 of 241\ttrain_loss: 3.6245\n",
      "Iteration: 154 of 241\ttrain_loss: 3.7500\n",
      "Iteration: 156 of 241\ttrain_loss: 3.9332\n",
      "Iteration: 158 of 241\ttrain_loss: 3.7965\n",
      "Iteration: 160 of 241\ttrain_loss: 3.7013\n",
      "Iteration: 162 of 241\ttrain_loss: 3.9884\n",
      "Iteration: 164 of 241\ttrain_loss: 4.1019\n",
      "Iteration: 166 of 241\ttrain_loss: 3.8982\n",
      "Iteration: 168 of 241\ttrain_loss: 3.8291\n",
      "Iteration: 170 of 241\ttrain_loss: 3.9651\n",
      "Iteration: 172 of 241\ttrain_loss: 3.7958\n",
      "Iteration: 174 of 241\ttrain_loss: 3.8214\n",
      "Iteration: 176 of 241\ttrain_loss: 4.0310\n",
      "Iteration: 178 of 241\ttrain_loss: 3.9475\n",
      "Iteration: 180 of 241\ttrain_loss: 3.9560\n",
      "Iteration: 182 of 241\ttrain_loss: 3.8261\n",
      "Iteration: 184 of 241\ttrain_loss: 3.5866\n",
      "Iteration: 186 of 241\ttrain_loss: 4.1449\n",
      "Iteration: 188 of 241\ttrain_loss: 4.1127\n",
      "Iteration: 190 of 241\ttrain_loss: 3.8979\n",
      "Iteration: 192 of 241\ttrain_loss: 4.3108\n",
      "Iteration: 194 of 241\ttrain_loss: 3.8754\n",
      "Iteration: 196 of 241\ttrain_loss: 3.8535\n",
      "Iteration: 198 of 241\ttrain_loss: 3.8846\n",
      "Iteration: 200 of 241\ttrain_loss: 3.9062\n",
      "Iteration: 202 of 241\ttrain_loss: 3.7542\n",
      "Iteration: 204 of 241\ttrain_loss: 3.9191\n",
      "Iteration: 206 of 241\ttrain_loss: 4.0710\n",
      "Iteration: 208 of 241\ttrain_loss: 3.9543\n",
      "Iteration: 210 of 241\ttrain_loss: 3.9450\n",
      "Iteration: 212 of 241\ttrain_loss: 3.6539\n",
      "Iteration: 214 of 241\ttrain_loss: 3.7876\n",
      "Iteration: 216 of 241\ttrain_loss: 3.7782\n",
      "Iteration: 218 of 241\ttrain_loss: 3.7440\n",
      "Iteration: 220 of 241\ttrain_loss: 4.0023\n",
      "Iteration: 222 of 241\ttrain_loss: 4.1562\n",
      "Iteration: 224 of 241\ttrain_loss: 4.0110\n",
      "Iteration: 226 of 241\ttrain_loss: 4.0810\n",
      "Iteration: 228 of 241\ttrain_loss: 3.8259\n",
      "Iteration: 230 of 241\ttrain_loss: 4.0307\n",
      "Iteration: 232 of 241\ttrain_loss: 4.2162\n",
      "Iteration: 234 of 241\ttrain_loss: 3.7335\n",
      "Iteration: 236 of 241\ttrain_loss: 4.0250\n",
      "Iteration: 238 of 241\ttrain_loss: 3.9057\n",
      "Iteration: 240 of 241\ttrain_loss: 4.0763\n",
      "Iteration: 241 of 241\ttrain_loss: 3.5088\n",
      "Average Score for this Epoch: 3.859119415283203\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 36 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.6230\n",
      "Iteration: 2 of 241\ttrain_loss: 3.5388\n",
      "Iteration: 4 of 241\ttrain_loss: 3.4921\n",
      "Iteration: 6 of 241\ttrain_loss: 3.7703\n",
      "Iteration: 8 of 241\ttrain_loss: 3.9385\n",
      "Iteration: 10 of 241\ttrain_loss: 3.9102\n",
      "Iteration: 12 of 241\ttrain_loss: 3.9007\n",
      "Iteration: 14 of 241\ttrain_loss: 3.6990\n",
      "Iteration: 16 of 241\ttrain_loss: 3.8242\n",
      "Iteration: 18 of 241\ttrain_loss: 3.3440\n",
      "Iteration: 20 of 241\ttrain_loss: 3.3624\n",
      "Iteration: 22 of 241\ttrain_loss: 3.7259\n",
      "Iteration: 24 of 241\ttrain_loss: 3.9573\n",
      "Iteration: 26 of 241\ttrain_loss: 3.5751\n",
      "Iteration: 28 of 241\ttrain_loss: 3.5994\n",
      "Iteration: 30 of 241\ttrain_loss: 3.7009\n",
      "Iteration: 32 of 241\ttrain_loss: 3.7209\n",
      "Iteration: 34 of 241\ttrain_loss: 3.5764\n",
      "Iteration: 36 of 241\ttrain_loss: 3.6954\n",
      "Iteration: 38 of 241\ttrain_loss: 3.6720\n",
      "Iteration: 40 of 241\ttrain_loss: 3.7214\n",
      "Iteration: 42 of 241\ttrain_loss: 3.6716\n",
      "Iteration: 44 of 241\ttrain_loss: 3.8247\n",
      "Iteration: 46 of 241\ttrain_loss: 3.8110\n",
      "Iteration: 48 of 241\ttrain_loss: 3.8200\n",
      "Iteration: 50 of 241\ttrain_loss: 3.4324\n",
      "Iteration: 52 of 241\ttrain_loss: 3.9820\n",
      "Iteration: 54 of 241\ttrain_loss: 3.7160\n",
      "Iteration: 56 of 241\ttrain_loss: 3.6053\n",
      "Iteration: 58 of 241\ttrain_loss: 3.6894\n",
      "Iteration: 60 of 241\ttrain_loss: 3.7159\n",
      "Iteration: 62 of 241\ttrain_loss: 3.7728\n",
      "Iteration: 64 of 241\ttrain_loss: 3.5543\n",
      "Iteration: 66 of 241\ttrain_loss: 3.8951\n",
      "Iteration: 68 of 241\ttrain_loss: 3.5395\n",
      "Iteration: 70 of 241\ttrain_loss: 3.8042\n",
      "Iteration: 72 of 241\ttrain_loss: 3.5568\n",
      "Iteration: 74 of 241\ttrain_loss: 3.9641\n",
      "Iteration: 76 of 241\ttrain_loss: 3.7134\n",
      "Iteration: 78 of 241\ttrain_loss: 3.9552\n",
      "Iteration: 80 of 241\ttrain_loss: 3.7551\n",
      "Iteration: 82 of 241\ttrain_loss: 4.0208\n",
      "Iteration: 84 of 241\ttrain_loss: 3.4250\n",
      "Iteration: 86 of 241\ttrain_loss: 4.0744\n",
      "Iteration: 88 of 241\ttrain_loss: 3.5482\n",
      "Iteration: 90 of 241\ttrain_loss: 3.7519\n",
      "Iteration: 92 of 241\ttrain_loss: 3.5568\n",
      "Iteration: 94 of 241\ttrain_loss: 3.9251\n",
      "Iteration: 96 of 241\ttrain_loss: 3.6587\n",
      "Iteration: 98 of 241\ttrain_loss: 3.7751\n",
      "Iteration: 100 of 241\ttrain_loss: 3.8836\n",
      "Iteration: 102 of 241\ttrain_loss: 3.6745\n",
      "Iteration: 104 of 241\ttrain_loss: 3.7971\n",
      "Iteration: 106 of 241\ttrain_loss: 3.3618\n",
      "Iteration: 108 of 241\ttrain_loss: 3.6962\n",
      "Iteration: 110 of 241\ttrain_loss: 3.6849\n",
      "Iteration: 112 of 241\ttrain_loss: 3.8294\n",
      "Iteration: 114 of 241\ttrain_loss: 3.7486\n",
      "Iteration: 116 of 241\ttrain_loss: 3.6558\n",
      "Iteration: 118 of 241\ttrain_loss: 3.7105\n",
      "Iteration: 120 of 241\ttrain_loss: 3.7566\n",
      "Iteration: 122 of 241\ttrain_loss: 3.9087\n",
      "Iteration: 124 of 241\ttrain_loss: 3.8148\n",
      "Iteration: 126 of 241\ttrain_loss: 3.5663\n",
      "Iteration: 128 of 241\ttrain_loss: 3.6699\n",
      "Iteration: 130 of 241\ttrain_loss: 3.5759\n",
      "Iteration: 132 of 241\ttrain_loss: 3.8022\n",
      "Iteration: 134 of 241\ttrain_loss: 3.8253\n",
      "Iteration: 136 of 241\ttrain_loss: 3.7442\n",
      "Iteration: 138 of 241\ttrain_loss: 3.6533\n",
      "Iteration: 140 of 241\ttrain_loss: 3.6400\n",
      "Iteration: 142 of 241\ttrain_loss: 3.9096\n",
      "Iteration: 144 of 241\ttrain_loss: 3.8743\n",
      "Iteration: 146 of 241\ttrain_loss: 4.2156\n",
      "Iteration: 148 of 241\ttrain_loss: 3.5519\n",
      "Iteration: 150 of 241\ttrain_loss: 3.7183\n",
      "Iteration: 152 of 241\ttrain_loss: 3.4923\n",
      "Iteration: 154 of 241\ttrain_loss: 3.5522\n",
      "Iteration: 156 of 241\ttrain_loss: 4.0035\n",
      "Iteration: 158 of 241\ttrain_loss: 3.6240\n",
      "Iteration: 160 of 241\ttrain_loss: 3.8127\n",
      "Iteration: 162 of 241\ttrain_loss: 3.7791\n",
      "Iteration: 164 of 241\ttrain_loss: 3.7390\n",
      "Iteration: 166 of 241\ttrain_loss: 3.9723\n",
      "Iteration: 168 of 241\ttrain_loss: 3.7333\n",
      "Iteration: 170 of 241\ttrain_loss: 3.7064\n",
      "Iteration: 172 of 241\ttrain_loss: 3.7408\n",
      "Iteration: 174 of 241\ttrain_loss: 3.7431\n",
      "Iteration: 176 of 241\ttrain_loss: 3.6947\n",
      "Iteration: 178 of 241\ttrain_loss: 3.8769\n",
      "Iteration: 180 of 241\ttrain_loss: 3.8023\n",
      "Iteration: 182 of 241\ttrain_loss: 4.1137\n",
      "Iteration: 184 of 241\ttrain_loss: 3.5742\n",
      "Iteration: 186 of 241\ttrain_loss: 3.7964\n",
      "Iteration: 188 of 241\ttrain_loss: 3.4724\n",
      "Iteration: 190 of 241\ttrain_loss: 3.8672\n",
      "Iteration: 192 of 241\ttrain_loss: 3.5772\n",
      "Iteration: 194 of 241\ttrain_loss: 3.6956\n",
      "Iteration: 196 of 241\ttrain_loss: 3.9092\n",
      "Iteration: 198 of 241\ttrain_loss: 3.7666\n",
      "Iteration: 200 of 241\ttrain_loss: 3.9633\n",
      "Iteration: 202 of 241\ttrain_loss: 3.9142\n",
      "Iteration: 204 of 241\ttrain_loss: 3.7482\n",
      "Iteration: 206 of 241\ttrain_loss: 3.5184\n",
      "Iteration: 208 of 241\ttrain_loss: 3.9918\n",
      "Iteration: 210 of 241\ttrain_loss: 3.7770\n",
      "Iteration: 212 of 241\ttrain_loss: 3.9159\n",
      "Iteration: 214 of 241\ttrain_loss: 3.5769\n",
      "Iteration: 216 of 241\ttrain_loss: 3.7048\n",
      "Iteration: 218 of 241\ttrain_loss: 4.2833\n",
      "Iteration: 220 of 241\ttrain_loss: 3.7960\n",
      "Iteration: 222 of 241\ttrain_loss: 3.7485\n",
      "Iteration: 224 of 241\ttrain_loss: 3.7091\n",
      "Iteration: 226 of 241\ttrain_loss: 3.8642\n",
      "Iteration: 228 of 241\ttrain_loss: 3.4033\n",
      "Iteration: 230 of 241\ttrain_loss: 3.6132\n",
      "Iteration: 232 of 241\ttrain_loss: 3.7854\n",
      "Iteration: 234 of 241\ttrain_loss: 3.6842\n",
      "Iteration: 236 of 241\ttrain_loss: 3.6481\n",
      "Iteration: 238 of 241\ttrain_loss: 3.8876\n",
      "Iteration: 240 of 241\ttrain_loss: 3.6901\n",
      "Iteration: 241 of 241\ttrain_loss: 3.5974\n",
      "Average Score for this Epoch: 3.7372443675994873\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 37 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.2821\n",
      "Iteration: 2 of 241\ttrain_loss: 3.7709\n",
      "Iteration: 4 of 241\ttrain_loss: 3.5249\n",
      "Iteration: 6 of 241\ttrain_loss: 3.6510\n",
      "Iteration: 8 of 241\ttrain_loss: 3.6302\n",
      "Iteration: 10 of 241\ttrain_loss: 3.8334\n",
      "Iteration: 12 of 241\ttrain_loss: 3.4171\n",
      "Iteration: 14 of 241\ttrain_loss: 3.6608\n",
      "Iteration: 16 of 241\ttrain_loss: 3.2572\n",
      "Iteration: 18 of 241\ttrain_loss: 3.4981\n",
      "Iteration: 20 of 241\ttrain_loss: 3.0624\n",
      "Iteration: 22 of 241\ttrain_loss: 3.4527\n",
      "Iteration: 24 of 241\ttrain_loss: 3.6820\n",
      "Iteration: 26 of 241\ttrain_loss: 3.7201\n",
      "Iteration: 28 of 241\ttrain_loss: 3.5018\n",
      "Iteration: 30 of 241\ttrain_loss: 3.5695\n",
      "Iteration: 32 of 241\ttrain_loss: 3.3710\n",
      "Iteration: 34 of 241\ttrain_loss: 3.4949\n",
      "Iteration: 36 of 241\ttrain_loss: 3.5629\n",
      "Iteration: 38 of 241\ttrain_loss: 3.3829\n",
      "Iteration: 40 of 241\ttrain_loss: 4.0338\n",
      "Iteration: 42 of 241\ttrain_loss: 3.5754\n",
      "Iteration: 44 of 241\ttrain_loss: 3.7809\n",
      "Iteration: 46 of 241\ttrain_loss: 3.4964\n",
      "Iteration: 48 of 241\ttrain_loss: 3.2681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50 of 241\ttrain_loss: 3.5994\n",
      "Iteration: 52 of 241\ttrain_loss: 3.1003\n",
      "Iteration: 54 of 241\ttrain_loss: 3.5591\n",
      "Iteration: 56 of 241\ttrain_loss: 3.4735\n",
      "Iteration: 58 of 241\ttrain_loss: 3.3517\n",
      "Iteration: 60 of 241\ttrain_loss: 3.3908\n",
      "Iteration: 62 of 241\ttrain_loss: 3.8084\n",
      "Iteration: 64 of 241\ttrain_loss: 3.4440\n",
      "Iteration: 66 of 241\ttrain_loss: 3.5122\n",
      "Iteration: 68 of 241\ttrain_loss: 3.5799\n",
      "Iteration: 70 of 241\ttrain_loss: 3.4804\n",
      "Iteration: 72 of 241\ttrain_loss: 3.6625\n",
      "Iteration: 74 of 241\ttrain_loss: 3.5603\n",
      "Iteration: 76 of 241\ttrain_loss: 3.4401\n",
      "Iteration: 78 of 241\ttrain_loss: 3.2760\n",
      "Iteration: 80 of 241\ttrain_loss: 3.8103\n",
      "Iteration: 82 of 241\ttrain_loss: 3.9540\n",
      "Iteration: 84 of 241\ttrain_loss: 3.5233\n",
      "Iteration: 86 of 241\ttrain_loss: 3.6140\n",
      "Iteration: 88 of 241\ttrain_loss: 3.8626\n",
      "Iteration: 90 of 241\ttrain_loss: 3.2278\n",
      "Iteration: 92 of 241\ttrain_loss: 3.6407\n",
      "Iteration: 94 of 241\ttrain_loss: 3.6592\n",
      "Iteration: 96 of 241\ttrain_loss: 3.6695\n",
      "Iteration: 98 of 241\ttrain_loss: 3.3418\n",
      "Iteration: 100 of 241\ttrain_loss: 3.6800\n",
      "Iteration: 102 of 241\ttrain_loss: 3.9862\n",
      "Iteration: 104 of 241\ttrain_loss: 3.4979\n",
      "Iteration: 106 of 241\ttrain_loss: 3.4628\n",
      "Iteration: 108 of 241\ttrain_loss: 3.5098\n",
      "Iteration: 110 of 241\ttrain_loss: 3.4690\n",
      "Iteration: 112 of 241\ttrain_loss: 3.7117\n",
      "Iteration: 114 of 241\ttrain_loss: 3.8349\n",
      "Iteration: 116 of 241\ttrain_loss: 3.4492\n",
      "Iteration: 118 of 241\ttrain_loss: 3.7375\n",
      "Iteration: 120 of 241\ttrain_loss: 4.1713\n",
      "Iteration: 122 of 241\ttrain_loss: 3.7507\n",
      "Iteration: 124 of 241\ttrain_loss: 3.6174\n",
      "Iteration: 126 of 241\ttrain_loss: 3.4909\n",
      "Iteration: 128 of 241\ttrain_loss: 3.8170\n",
      "Iteration: 130 of 241\ttrain_loss: 3.3306\n",
      "Iteration: 132 of 241\ttrain_loss: 3.8355\n",
      "Iteration: 134 of 241\ttrain_loss: 3.5029\n",
      "Iteration: 136 of 241\ttrain_loss: 3.6228\n",
      "Iteration: 138 of 241\ttrain_loss: 3.4660\n",
      "Iteration: 140 of 241\ttrain_loss: 3.8989\n",
      "Iteration: 142 of 241\ttrain_loss: 3.6855\n",
      "Iteration: 144 of 241\ttrain_loss: 3.5377\n",
      "Iteration: 146 of 241\ttrain_loss: 3.4397\n",
      "Iteration: 148 of 241\ttrain_loss: 3.8245\n",
      "Iteration: 150 of 241\ttrain_loss: 3.5602\n",
      "Iteration: 152 of 241\ttrain_loss: 3.5401\n",
      "Iteration: 154 of 241\ttrain_loss: 3.9706\n",
      "Iteration: 156 of 241\ttrain_loss: 3.5933\n",
      "Iteration: 158 of 241\ttrain_loss: 3.6907\n",
      "Iteration: 160 of 241\ttrain_loss: 3.4998\n",
      "Iteration: 162 of 241\ttrain_loss: 3.4294\n",
      "Iteration: 164 of 241\ttrain_loss: 3.7100\n",
      "Iteration: 166 of 241\ttrain_loss: 3.6708\n",
      "Iteration: 168 of 241\ttrain_loss: 3.5891\n",
      "Iteration: 170 of 241\ttrain_loss: 3.6519\n",
      "Iteration: 172 of 241\ttrain_loss: 3.6095\n",
      "Iteration: 174 of 241\ttrain_loss: 3.6569\n",
      "Iteration: 176 of 241\ttrain_loss: 4.1472\n",
      "Iteration: 178 of 241\ttrain_loss: 3.8065\n",
      "Iteration: 180 of 241\ttrain_loss: 3.5905\n",
      "Iteration: 182 of 241\ttrain_loss: 3.5420\n",
      "Iteration: 184 of 241\ttrain_loss: 3.7769\n",
      "Iteration: 186 of 241\ttrain_loss: 3.8347\n",
      "Iteration: 188 of 241\ttrain_loss: 4.1403\n",
      "Iteration: 190 of 241\ttrain_loss: 3.5091\n",
      "Iteration: 192 of 241\ttrain_loss: 3.9562\n",
      "Iteration: 194 of 241\ttrain_loss: 3.6115\n",
      "Iteration: 196 of 241\ttrain_loss: 3.3878\n",
      "Iteration: 198 of 241\ttrain_loss: 3.7952\n",
      "Iteration: 200 of 241\ttrain_loss: 3.6908\n",
      "Iteration: 202 of 241\ttrain_loss: 3.7726\n",
      "Iteration: 204 of 241\ttrain_loss: 3.5866\n",
      "Iteration: 206 of 241\ttrain_loss: 3.6153\n",
      "Iteration: 208 of 241\ttrain_loss: 3.7517\n",
      "Iteration: 210 of 241\ttrain_loss: 3.6301\n",
      "Iteration: 212 of 241\ttrain_loss: 3.8624\n",
      "Iteration: 214 of 241\ttrain_loss: 3.6118\n",
      "Iteration: 216 of 241\ttrain_loss: 3.7086\n",
      "Iteration: 218 of 241\ttrain_loss: 3.7252\n",
      "Iteration: 220 of 241\ttrain_loss: 3.4816\n",
      "Iteration: 222 of 241\ttrain_loss: 3.4218\n",
      "Iteration: 224 of 241\ttrain_loss: 3.9429\n",
      "Iteration: 226 of 241\ttrain_loss: 3.1975\n",
      "Iteration: 228 of 241\ttrain_loss: 3.7315\n",
      "Iteration: 230 of 241\ttrain_loss: 3.4006\n",
      "Iteration: 232 of 241\ttrain_loss: 3.7344\n",
      "Iteration: 234 of 241\ttrain_loss: 3.8386\n",
      "Iteration: 236 of 241\ttrain_loss: 3.6370\n",
      "Iteration: 238 of 241\ttrain_loss: 3.6396\n",
      "Iteration: 240 of 241\ttrain_loss: 3.6979\n",
      "Iteration: 241 of 241\ttrain_loss: 3.6789\n",
      "Average Score for this Epoch: 3.6098673343658447\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 38 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.4870\n",
      "Iteration: 2 of 241\ttrain_loss: 3.6347\n",
      "Iteration: 4 of 241\ttrain_loss: 3.3882\n",
      "Iteration: 6 of 241\ttrain_loss: 3.6131\n",
      "Iteration: 8 of 241\ttrain_loss: 3.3866\n",
      "Iteration: 10 of 241\ttrain_loss: 3.5311\n",
      "Iteration: 12 of 241\ttrain_loss: 3.3658\n",
      "Iteration: 14 of 241\ttrain_loss: 3.3479\n",
      "Iteration: 16 of 241\ttrain_loss: 3.2329\n",
      "Iteration: 18 of 241\ttrain_loss: 3.3164\n",
      "Iteration: 20 of 241\ttrain_loss: 3.5002\n",
      "Iteration: 22 of 241\ttrain_loss: 3.4816\n",
      "Iteration: 24 of 241\ttrain_loss: 3.5621\n",
      "Iteration: 26 of 241\ttrain_loss: 3.3571\n",
      "Iteration: 28 of 241\ttrain_loss: 3.4542\n",
      "Iteration: 30 of 241\ttrain_loss: 3.9784\n",
      "Iteration: 32 of 241\ttrain_loss: 3.6504\n",
      "Iteration: 34 of 241\ttrain_loss: 3.6318\n",
      "Iteration: 36 of 241\ttrain_loss: 3.2589\n",
      "Iteration: 38 of 241\ttrain_loss: 3.2300\n",
      "Iteration: 40 of 241\ttrain_loss: 3.2630\n",
      "Iteration: 42 of 241\ttrain_loss: 3.3378\n",
      "Iteration: 44 of 241\ttrain_loss: 3.3730\n",
      "Iteration: 46 of 241\ttrain_loss: 3.4113\n",
      "Iteration: 48 of 241\ttrain_loss: 3.4237\n",
      "Iteration: 50 of 241\ttrain_loss: 3.1707\n",
      "Iteration: 52 of 241\ttrain_loss: 3.4900\n",
      "Iteration: 54 of 241\ttrain_loss: 3.2720\n",
      "Iteration: 56 of 241\ttrain_loss: 3.3878\n",
      "Iteration: 58 of 241\ttrain_loss: 3.1316\n",
      "Iteration: 60 of 241\ttrain_loss: 3.4706\n",
      "Iteration: 62 of 241\ttrain_loss: 3.3799\n",
      "Iteration: 64 of 241\ttrain_loss: 3.4442\n",
      "Iteration: 66 of 241\ttrain_loss: 3.3675\n",
      "Iteration: 68 of 241\ttrain_loss: 3.3087\n",
      "Iteration: 70 of 241\ttrain_loss: 3.4199\n",
      "Iteration: 72 of 241\ttrain_loss: 3.0037\n",
      "Iteration: 74 of 241\ttrain_loss: 3.4384\n",
      "Iteration: 76 of 241\ttrain_loss: 3.7462\n",
      "Iteration: 78 of 241\ttrain_loss: 3.5407\n",
      "Iteration: 80 of 241\ttrain_loss: 3.2838\n",
      "Iteration: 82 of 241\ttrain_loss: 3.7082\n",
      "Iteration: 84 of 241\ttrain_loss: 3.3972\n",
      "Iteration: 86 of 241\ttrain_loss: 3.6309\n",
      "Iteration: 88 of 241\ttrain_loss: 3.3825\n",
      "Iteration: 90 of 241\ttrain_loss: 3.5886\n",
      "Iteration: 92 of 241\ttrain_loss: 3.3789\n",
      "Iteration: 94 of 241\ttrain_loss: 3.7453\n",
      "Iteration: 96 of 241\ttrain_loss: 3.2855\n",
      "Iteration: 98 of 241\ttrain_loss: 3.5635\n",
      "Iteration: 100 of 241\ttrain_loss: 3.4063\n",
      "Iteration: 102 of 241\ttrain_loss: 4.0596\n",
      "Iteration: 104 of 241\ttrain_loss: 3.6590\n",
      "Iteration: 106 of 241\ttrain_loss: 3.5633\n",
      "Iteration: 108 of 241\ttrain_loss: 3.3453\n",
      "Iteration: 110 of 241\ttrain_loss: 3.7043\n",
      "Iteration: 112 of 241\ttrain_loss: 3.1949\n",
      "Iteration: 114 of 241\ttrain_loss: 3.8140\n",
      "Iteration: 116 of 241\ttrain_loss: 3.5907\n",
      "Iteration: 118 of 241\ttrain_loss: 3.3599\n",
      "Iteration: 120 of 241\ttrain_loss: 3.6569\n",
      "Iteration: 122 of 241\ttrain_loss: 3.4010\n",
      "Iteration: 124 of 241\ttrain_loss: 3.3727\n",
      "Iteration: 126 of 241\ttrain_loss: 3.6801\n",
      "Iteration: 128 of 241\ttrain_loss: 3.5482\n",
      "Iteration: 130 of 241\ttrain_loss: 3.5640\n",
      "Iteration: 132 of 241\ttrain_loss: 3.5916\n",
      "Iteration: 134 of 241\ttrain_loss: 3.6113\n",
      "Iteration: 136 of 241\ttrain_loss: 3.9708\n",
      "Iteration: 138 of 241\ttrain_loss: 3.7448\n",
      "Iteration: 140 of 241\ttrain_loss: 3.4620\n",
      "Iteration: 142 of 241\ttrain_loss: 4.3735\n",
      "Iteration: 144 of 241\ttrain_loss: 3.6846\n",
      "Iteration: 146 of 241\ttrain_loss: 3.6602\n",
      "Iteration: 148 of 241\ttrain_loss: 3.8110\n",
      "Iteration: 150 of 241\ttrain_loss: 3.5889\n",
      "Iteration: 152 of 241\ttrain_loss: 3.6110\n",
      "Iteration: 154 of 241\ttrain_loss: 3.5714\n",
      "Iteration: 156 of 241\ttrain_loss: 3.6606\n",
      "Iteration: 158 of 241\ttrain_loss: 3.5492\n",
      "Iteration: 160 of 241\ttrain_loss: 3.4791\n",
      "Iteration: 162 of 241\ttrain_loss: 3.6169\n",
      "Iteration: 164 of 241\ttrain_loss: 3.6530\n",
      "Iteration: 166 of 241\ttrain_loss: 3.6659\n",
      "Iteration: 168 of 241\ttrain_loss: 3.5552\n",
      "Iteration: 170 of 241\ttrain_loss: 3.7628\n",
      "Iteration: 172 of 241\ttrain_loss: 3.5764\n",
      "Iteration: 174 of 241\ttrain_loss: 3.4106\n",
      "Iteration: 176 of 241\ttrain_loss: 3.4538\n",
      "Iteration: 178 of 241\ttrain_loss: 3.6399\n",
      "Iteration: 180 of 241\ttrain_loss: 3.4983\n",
      "Iteration: 182 of 241\ttrain_loss: 3.6698\n",
      "Iteration: 184 of 241\ttrain_loss: 3.6594\n",
      "Iteration: 186 of 241\ttrain_loss: 3.7760\n",
      "Iteration: 188 of 241\ttrain_loss: 3.4990\n",
      "Iteration: 190 of 241\ttrain_loss: 3.4777\n",
      "Iteration: 192 of 241\ttrain_loss: 3.4107\n",
      "Iteration: 194 of 241\ttrain_loss: 3.3181\n",
      "Iteration: 196 of 241\ttrain_loss: 3.7176\n",
      "Iteration: 198 of 241\ttrain_loss: 3.9450\n",
      "Iteration: 200 of 241\ttrain_loss: 3.9895\n",
      "Iteration: 202 of 241\ttrain_loss: 3.4901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 204 of 241\ttrain_loss: 3.7081\n",
      "Iteration: 206 of 241\ttrain_loss: 3.6823\n",
      "Iteration: 208 of 241\ttrain_loss: 3.6275\n",
      "Iteration: 210 of 241\ttrain_loss: 3.4461\n",
      "Iteration: 212 of 241\ttrain_loss: 3.8074\n",
      "Iteration: 214 of 241\ttrain_loss: 3.0915\n",
      "Iteration: 216 of 241\ttrain_loss: 3.4969\n",
      "Iteration: 218 of 241\ttrain_loss: 3.4107\n",
      "Iteration: 220 of 241\ttrain_loss: 3.5045\n",
      "Iteration: 222 of 241\ttrain_loss: 3.2342\n",
      "Iteration: 224 of 241\ttrain_loss: 3.5101\n",
      "Iteration: 226 of 241\ttrain_loss: 3.7145\n",
      "Iteration: 228 of 241\ttrain_loss: 3.4243\n",
      "Iteration: 230 of 241\ttrain_loss: 3.5193\n",
      "Iteration: 232 of 241\ttrain_loss: 3.7759\n",
      "Iteration: 234 of 241\ttrain_loss: 3.8209\n",
      "Iteration: 236 of 241\ttrain_loss: 3.6056\n",
      "Iteration: 238 of 241\ttrain_loss: 3.6809\n",
      "Iteration: 240 of 241\ttrain_loss: 3.5611\n",
      "Iteration: 241 of 241\ttrain_loss: 3.6124\n",
      "Average Score for this Epoch: 3.534726619720459\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 39 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.5657\n",
      "Iteration: 2 of 241\ttrain_loss: 3.2716\n",
      "Iteration: 4 of 241\ttrain_loss: 3.3159\n",
      "Iteration: 6 of 241\ttrain_loss: 3.0806\n",
      "Iteration: 8 of 241\ttrain_loss: 3.0743\n",
      "Iteration: 10 of 241\ttrain_loss: 3.0689\n",
      "Iteration: 12 of 241\ttrain_loss: 3.1597\n",
      "Iteration: 14 of 241\ttrain_loss: 3.3763\n",
      "Iteration: 16 of 241\ttrain_loss: 3.6499\n",
      "Iteration: 18 of 241\ttrain_loss: 3.6947\n",
      "Iteration: 20 of 241\ttrain_loss: 3.5225\n",
      "Iteration: 22 of 241\ttrain_loss: 3.5114\n",
      "Iteration: 24 of 241\ttrain_loss: 3.3167\n",
      "Iteration: 26 of 241\ttrain_loss: 2.8812\n",
      "Iteration: 28 of 241\ttrain_loss: 3.7080\n",
      "Iteration: 30 of 241\ttrain_loss: 3.2491\n",
      "Iteration: 32 of 241\ttrain_loss: 3.1326\n",
      "Iteration: 34 of 241\ttrain_loss: 3.6108\n",
      "Iteration: 36 of 241\ttrain_loss: 3.3985\n",
      "Iteration: 38 of 241\ttrain_loss: 3.6308\n",
      "Iteration: 40 of 241\ttrain_loss: 3.7466\n",
      "Iteration: 42 of 241\ttrain_loss: 3.4196\n",
      "Iteration: 44 of 241\ttrain_loss: 3.2428\n",
      "Iteration: 46 of 241\ttrain_loss: 3.4412\n",
      "Iteration: 48 of 241\ttrain_loss: 3.2817\n",
      "Iteration: 50 of 241\ttrain_loss: 3.4439\n",
      "Iteration: 52 of 241\ttrain_loss: 3.5337\n",
      "Iteration: 54 of 241\ttrain_loss: 2.9001\n",
      "Iteration: 56 of 241\ttrain_loss: 3.5229\n",
      "Iteration: 58 of 241\ttrain_loss: 3.6065\n",
      "Iteration: 60 of 241\ttrain_loss: 3.3138\n",
      "Iteration: 62 of 241\ttrain_loss: 3.4985\n",
      "Iteration: 64 of 241\ttrain_loss: 3.5435\n",
      "Iteration: 66 of 241\ttrain_loss: 3.3178\n",
      "Iteration: 68 of 241\ttrain_loss: 3.0162\n",
      "Iteration: 70 of 241\ttrain_loss: 3.1155\n",
      "Iteration: 72 of 241\ttrain_loss: 3.4343\n",
      "Iteration: 74 of 241\ttrain_loss: 3.3853\n",
      "Iteration: 76 of 241\ttrain_loss: 3.6864\n",
      "Iteration: 78 of 241\ttrain_loss: 3.4471\n",
      "Iteration: 80 of 241\ttrain_loss: 3.4517\n",
      "Iteration: 82 of 241\ttrain_loss: 3.1080\n",
      "Iteration: 84 of 241\ttrain_loss: 3.3350\n",
      "Iteration: 86 of 241\ttrain_loss: 3.3572\n",
      "Iteration: 88 of 241\ttrain_loss: 3.8473\n",
      "Iteration: 90 of 241\ttrain_loss: 3.4915\n",
      "Iteration: 92 of 241\ttrain_loss: 3.5254\n",
      "Iteration: 94 of 241\ttrain_loss: 3.4595\n",
      "Iteration: 96 of 241\ttrain_loss: 3.6767\n",
      "Iteration: 98 of 241\ttrain_loss: 4.2095\n",
      "Iteration: 100 of 241\ttrain_loss: 3.3985\n",
      "Iteration: 102 of 241\ttrain_loss: 3.3888\n",
      "Iteration: 104 of 241\ttrain_loss: 3.4927\n",
      "Iteration: 106 of 241\ttrain_loss: 3.4621\n",
      "Iteration: 108 of 241\ttrain_loss: 3.5646\n",
      "Iteration: 110 of 241\ttrain_loss: 3.6436\n",
      "Iteration: 112 of 241\ttrain_loss: 3.5184\n",
      "Iteration: 114 of 241\ttrain_loss: 3.2392\n",
      "Iteration: 116 of 241\ttrain_loss: 3.7120\n",
      "Iteration: 118 of 241\ttrain_loss: 3.5112\n",
      "Iteration: 120 of 241\ttrain_loss: 3.6464\n",
      "Iteration: 122 of 241\ttrain_loss: 3.4796\n",
      "Iteration: 124 of 241\ttrain_loss: 3.6192\n",
      "Iteration: 126 of 241\ttrain_loss: 3.8741\n",
      "Iteration: 128 of 241\ttrain_loss: 3.2175\n",
      "Iteration: 130 of 241\ttrain_loss: 3.8579\n",
      "Iteration: 132 of 241\ttrain_loss: 3.5363\n",
      "Iteration: 134 of 241\ttrain_loss: 3.2255\n",
      "Iteration: 136 of 241\ttrain_loss: 3.1773\n",
      "Iteration: 138 of 241\ttrain_loss: 3.5089\n",
      "Iteration: 140 of 241\ttrain_loss: 3.2572\n",
      "Iteration: 142 of 241\ttrain_loss: 3.5984\n",
      "Iteration: 144 of 241\ttrain_loss: 3.4394\n",
      "Iteration: 146 of 241\ttrain_loss: 3.2894\n",
      "Iteration: 148 of 241\ttrain_loss: 3.5875\n",
      "Iteration: 150 of 241\ttrain_loss: 3.3902\n",
      "Iteration: 152 of 241\ttrain_loss: 3.5922\n",
      "Iteration: 154 of 241\ttrain_loss: 3.6599\n",
      "Iteration: 156 of 241\ttrain_loss: 4.0612\n",
      "Iteration: 158 of 241\ttrain_loss: 3.5564\n",
      "Iteration: 160 of 241\ttrain_loss: 3.5474\n",
      "Iteration: 162 of 241\ttrain_loss: 3.3876\n",
      "Iteration: 164 of 241\ttrain_loss: 3.2878\n",
      "Iteration: 166 of 241\ttrain_loss: 3.3597\n",
      "Iteration: 168 of 241\ttrain_loss: 3.5442\n",
      "Iteration: 170 of 241\ttrain_loss: 3.6111\n",
      "Iteration: 172 of 241\ttrain_loss: 3.6615\n",
      "Iteration: 174 of 241\ttrain_loss: 3.5105\n",
      "Iteration: 176 of 241\ttrain_loss: 3.9608\n",
      "Iteration: 178 of 241\ttrain_loss: 3.4763\n",
      "Iteration: 180 of 241\ttrain_loss: 3.4236\n",
      "Iteration: 182 of 241\ttrain_loss: 3.4825\n",
      "Iteration: 184 of 241\ttrain_loss: 3.5794\n",
      "Iteration: 186 of 241\ttrain_loss: 3.6197\n",
      "Iteration: 188 of 241\ttrain_loss: 3.5775\n",
      "Iteration: 190 of 241\ttrain_loss: 3.2919\n",
      "Iteration: 192 of 241\ttrain_loss: 3.2189\n",
      "Iteration: 194 of 241\ttrain_loss: 3.7166\n",
      "Iteration: 196 of 241\ttrain_loss: 3.3753\n",
      "Iteration: 198 of 241\ttrain_loss: 3.3313\n",
      "Iteration: 200 of 241\ttrain_loss: 3.4529\n",
      "Iteration: 202 of 241\ttrain_loss: 3.4606\n",
      "Iteration: 204 of 241\ttrain_loss: 3.5764\n",
      "Iteration: 206 of 241\ttrain_loss: 3.0039\n",
      "Iteration: 208 of 241\ttrain_loss: 3.2607\n",
      "Iteration: 210 of 241\ttrain_loss: 3.2696\n",
      "Iteration: 212 of 241\ttrain_loss: 3.3896\n",
      "Iteration: 214 of 241\ttrain_loss: 3.5832\n",
      "Iteration: 216 of 241\ttrain_loss: 3.3197\n",
      "Iteration: 218 of 241\ttrain_loss: 3.4573\n",
      "Iteration: 220 of 241\ttrain_loss: 3.3725\n",
      "Iteration: 222 of 241\ttrain_loss: 3.6465\n",
      "Iteration: 224 of 241\ttrain_loss: 3.2237\n",
      "Iteration: 226 of 241\ttrain_loss: 3.6736\n",
      "Iteration: 228 of 241\ttrain_loss: 3.3764\n",
      "Iteration: 230 of 241\ttrain_loss: 3.4669\n",
      "Iteration: 232 of 241\ttrain_loss: 3.2952\n",
      "Iteration: 234 of 241\ttrain_loss: 3.5017\n",
      "Iteration: 236 of 241\ttrain_loss: 3.5263\n",
      "Iteration: 238 of 241\ttrain_loss: 3.4609\n",
      "Iteration: 240 of 241\ttrain_loss: 3.1714\n",
      "Iteration: 241 of 241\ttrain_loss: 3.1613\n",
      "Average Score for this Epoch: 3.453627586364746\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 40 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.3662\n",
      "Iteration: 2 of 241\ttrain_loss: 3.1243\n",
      "Iteration: 4 of 241\ttrain_loss: 2.9100\n",
      "Iteration: 6 of 241\ttrain_loss: 3.5419\n",
      "Iteration: 8 of 241\ttrain_loss: 3.3057\n",
      "Iteration: 10 of 241\ttrain_loss: 3.3233\n",
      "Iteration: 12 of 241\ttrain_loss: 2.9940\n",
      "Iteration: 14 of 241\ttrain_loss: 3.1741\n",
      "Iteration: 16 of 241\ttrain_loss: 3.3471\n",
      "Iteration: 18 of 241\ttrain_loss: 3.0840\n",
      "Iteration: 20 of 241\ttrain_loss: 3.5688\n",
      "Iteration: 22 of 241\ttrain_loss: 3.3596\n",
      "Iteration: 24 of 241\ttrain_loss: 3.1644\n",
      "Iteration: 26 of 241\ttrain_loss: 3.3965\n",
      "Iteration: 28 of 241\ttrain_loss: 3.1802\n",
      "Iteration: 30 of 241\ttrain_loss: 3.5488\n",
      "Iteration: 32 of 241\ttrain_loss: 3.2065\n",
      "Iteration: 34 of 241\ttrain_loss: 3.5990\n",
      "Iteration: 36 of 241\ttrain_loss: 3.6224\n",
      "Iteration: 38 of 241\ttrain_loss: 3.2471\n",
      "Iteration: 40 of 241\ttrain_loss: 3.4516\n",
      "Iteration: 42 of 241\ttrain_loss: 3.5082\n",
      "Iteration: 44 of 241\ttrain_loss: 3.4277\n",
      "Iteration: 46 of 241\ttrain_loss: 3.5299\n",
      "Iteration: 48 of 241\ttrain_loss: 3.6674\n",
      "Iteration: 50 of 241\ttrain_loss: 3.3747\n",
      "Iteration: 52 of 241\ttrain_loss: 3.2965\n",
      "Iteration: 54 of 241\ttrain_loss: 3.4351\n",
      "Iteration: 56 of 241\ttrain_loss: 3.0708\n",
      "Iteration: 58 of 241\ttrain_loss: 3.6025\n",
      "Iteration: 60 of 241\ttrain_loss: 3.0633\n",
      "Iteration: 62 of 241\ttrain_loss: 3.4101\n",
      "Iteration: 64 of 241\ttrain_loss: 3.5820\n",
      "Iteration: 66 of 241\ttrain_loss: 3.2361\n",
      "Iteration: 68 of 241\ttrain_loss: 3.2042\n",
      "Iteration: 70 of 241\ttrain_loss: 3.4616\n",
      "Iteration: 72 of 241\ttrain_loss: 3.2479\n",
      "Iteration: 74 of 241\ttrain_loss: 3.3635\n",
      "Iteration: 76 of 241\ttrain_loss: 3.3957\n",
      "Iteration: 78 of 241\ttrain_loss: 3.5865\n",
      "Iteration: 80 of 241\ttrain_loss: 3.4874\n",
      "Iteration: 82 of 241\ttrain_loss: 3.3952\n",
      "Iteration: 84 of 241\ttrain_loss: 3.3410\n",
      "Iteration: 86 of 241\ttrain_loss: 3.1100\n",
      "Iteration: 88 of 241\ttrain_loss: 3.1521\n",
      "Iteration: 90 of 241\ttrain_loss: 3.4944\n",
      "Iteration: 92 of 241\ttrain_loss: 3.2893\n",
      "Iteration: 94 of 241\ttrain_loss: 3.3583\n",
      "Iteration: 96 of 241\ttrain_loss: 3.6470\n",
      "Iteration: 98 of 241\ttrain_loss: 3.4443\n",
      "Iteration: 100 of 241\ttrain_loss: 3.2137\n",
      "Iteration: 102 of 241\ttrain_loss: 3.1235\n",
      "Iteration: 104 of 241\ttrain_loss: 3.3577\n",
      "Iteration: 106 of 241\ttrain_loss: 3.2878\n",
      "Iteration: 108 of 241\ttrain_loss: 3.7291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 110 of 241\ttrain_loss: 3.1544\n",
      "Iteration: 112 of 241\ttrain_loss: 3.3299\n",
      "Iteration: 114 of 241\ttrain_loss: 3.1463\n",
      "Iteration: 116 of 241\ttrain_loss: 3.8401\n",
      "Iteration: 118 of 241\ttrain_loss: 3.1632\n",
      "Iteration: 120 of 241\ttrain_loss: 3.0622\n",
      "Iteration: 122 of 241\ttrain_loss: 3.2154\n",
      "Iteration: 124 of 241\ttrain_loss: 2.8624\n",
      "Iteration: 126 of 241\ttrain_loss: 3.2515\n",
      "Iteration: 128 of 241\ttrain_loss: 3.3379\n",
      "Iteration: 130 of 241\ttrain_loss: 3.2437\n",
      "Iteration: 132 of 241\ttrain_loss: 3.4217\n",
      "Iteration: 134 of 241\ttrain_loss: 3.5163\n",
      "Iteration: 136 of 241\ttrain_loss: 3.7772\n",
      "Iteration: 138 of 241\ttrain_loss: 3.4044\n",
      "Iteration: 140 of 241\ttrain_loss: 3.0678\n",
      "Iteration: 142 of 241\ttrain_loss: 3.3408\n",
      "Iteration: 144 of 241\ttrain_loss: 2.9791\n",
      "Iteration: 146 of 241\ttrain_loss: 3.2531\n",
      "Iteration: 148 of 241\ttrain_loss: 3.2769\n",
      "Iteration: 150 of 241\ttrain_loss: 3.3863\n",
      "Iteration: 152 of 241\ttrain_loss: 3.1489\n",
      "Iteration: 154 of 241\ttrain_loss: 3.4004\n",
      "Iteration: 156 of 241\ttrain_loss: 3.1745\n",
      "Iteration: 158 of 241\ttrain_loss: 3.6292\n",
      "Iteration: 160 of 241\ttrain_loss: 3.1916\n",
      "Iteration: 162 of 241\ttrain_loss: 3.3645\n",
      "Iteration: 164 of 241\ttrain_loss: 3.3708\n",
      "Iteration: 166 of 241\ttrain_loss: 3.4627\n",
      "Iteration: 168 of 241\ttrain_loss: 3.2627\n",
      "Iteration: 170 of 241\ttrain_loss: 3.1962\n",
      "Iteration: 172 of 241\ttrain_loss: 3.2655\n",
      "Iteration: 174 of 241\ttrain_loss: 3.4157\n",
      "Iteration: 176 of 241\ttrain_loss: 3.1202\n",
      "Iteration: 178 of 241\ttrain_loss: 2.9489\n",
      "Iteration: 180 of 241\ttrain_loss: 3.2050\n",
      "Iteration: 182 of 241\ttrain_loss: 3.5504\n",
      "Iteration: 184 of 241\ttrain_loss: 3.2473\n",
      "Iteration: 186 of 241\ttrain_loss: 3.2147\n",
      "Iteration: 188 of 241\ttrain_loss: 3.5197\n",
      "Iteration: 190 of 241\ttrain_loss: 3.4270\n",
      "Iteration: 192 of 241\ttrain_loss: 3.1660\n",
      "Iteration: 194 of 241\ttrain_loss: 3.5541\n",
      "Iteration: 196 of 241\ttrain_loss: 3.4211\n",
      "Iteration: 198 of 241\ttrain_loss: 3.6707\n",
      "Iteration: 200 of 241\ttrain_loss: 3.1095\n",
      "Iteration: 202 of 241\ttrain_loss: 3.6147\n",
      "Iteration: 204 of 241\ttrain_loss: 3.8209\n",
      "Iteration: 206 of 241\ttrain_loss: 3.4008\n",
      "Iteration: 208 of 241\ttrain_loss: 3.7310\n",
      "Iteration: 210 of 241\ttrain_loss: 3.3033\n",
      "Iteration: 212 of 241\ttrain_loss: 3.5704\n",
      "Iteration: 214 of 241\ttrain_loss: 3.4018\n",
      "Iteration: 216 of 241\ttrain_loss: 3.6687\n",
      "Iteration: 218 of 241\ttrain_loss: 3.6184\n",
      "Iteration: 220 of 241\ttrain_loss: 3.3635\n",
      "Iteration: 222 of 241\ttrain_loss: 3.3846\n",
      "Iteration: 224 of 241\ttrain_loss: 3.2588\n",
      "Iteration: 226 of 241\ttrain_loss: 3.5572\n",
      "Iteration: 228 of 241\ttrain_loss: 3.6667\n",
      "Iteration: 230 of 241\ttrain_loss: 3.1234\n",
      "Iteration: 232 of 241\ttrain_loss: 3.3662\n",
      "Iteration: 234 of 241\ttrain_loss: 3.6309\n",
      "Iteration: 236 of 241\ttrain_loss: 3.3042\n",
      "Iteration: 238 of 241\ttrain_loss: 3.6799\n",
      "Iteration: 240 of 241\ttrain_loss: 3.4337\n",
      "Iteration: 241 of 241\ttrain_loss: 3.8946\n",
      "Average Score for this Epoch: 3.370464563369751\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 41 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.0815\n",
      "Iteration: 2 of 241\ttrain_loss: 3.1280\n",
      "Iteration: 4 of 241\ttrain_loss: 2.8929\n",
      "Iteration: 6 of 241\ttrain_loss: 2.7326\n",
      "Iteration: 8 of 241\ttrain_loss: 3.1721\n",
      "Iteration: 10 of 241\ttrain_loss: 3.2145\n",
      "Iteration: 12 of 241\ttrain_loss: 3.0237\n",
      "Iteration: 14 of 241\ttrain_loss: 2.9638\n",
      "Iteration: 16 of 241\ttrain_loss: 3.3965\n",
      "Iteration: 18 of 241\ttrain_loss: 3.2587\n",
      "Iteration: 20 of 241\ttrain_loss: 3.2315\n",
      "Iteration: 22 of 241\ttrain_loss: 3.3305\n",
      "Iteration: 24 of 241\ttrain_loss: 3.5183\n",
      "Iteration: 26 of 241\ttrain_loss: 3.4848\n",
      "Iteration: 28 of 241\ttrain_loss: 3.5277\n",
      "Iteration: 30 of 241\ttrain_loss: 3.3881\n",
      "Iteration: 32 of 241\ttrain_loss: 3.1996\n",
      "Iteration: 34 of 241\ttrain_loss: 2.8105\n",
      "Iteration: 36 of 241\ttrain_loss: 3.2794\n",
      "Iteration: 38 of 241\ttrain_loss: 3.3747\n",
      "Iteration: 40 of 241\ttrain_loss: 2.9571\n",
      "Iteration: 42 of 241\ttrain_loss: 3.3630\n",
      "Iteration: 44 of 241\ttrain_loss: 3.2380\n",
      "Iteration: 46 of 241\ttrain_loss: 3.0090\n",
      "Iteration: 48 of 241\ttrain_loss: 3.1942\n",
      "Iteration: 50 of 241\ttrain_loss: 3.5707\n",
      "Iteration: 52 of 241\ttrain_loss: 3.4652\n",
      "Iteration: 54 of 241\ttrain_loss: 3.2632\n",
      "Iteration: 56 of 241\ttrain_loss: 3.1181\n",
      "Iteration: 58 of 241\ttrain_loss: 3.0883\n",
      "Iteration: 60 of 241\ttrain_loss: 3.2234\n",
      "Iteration: 62 of 241\ttrain_loss: 2.9784\n",
      "Iteration: 64 of 241\ttrain_loss: 2.7786\n",
      "Iteration: 66 of 241\ttrain_loss: 3.1295\n",
      "Iteration: 68 of 241\ttrain_loss: 3.5906\n",
      "Iteration: 70 of 241\ttrain_loss: 3.0839\n",
      "Iteration: 72 of 241\ttrain_loss: 3.0992\n",
      "Iteration: 74 of 241\ttrain_loss: 3.2984\n",
      "Iteration: 76 of 241\ttrain_loss: 3.8074\n",
      "Iteration: 78 of 241\ttrain_loss: 3.1356\n",
      "Iteration: 80 of 241\ttrain_loss: 3.3353\n",
      "Iteration: 82 of 241\ttrain_loss: 3.9389\n",
      "Iteration: 84 of 241\ttrain_loss: 3.5518\n",
      "Iteration: 86 of 241\ttrain_loss: 3.2591\n",
      "Iteration: 88 of 241\ttrain_loss: 3.2171\n",
      "Iteration: 90 of 241\ttrain_loss: 3.3595\n",
      "Iteration: 92 of 241\ttrain_loss: 3.5140\n",
      "Iteration: 94 of 241\ttrain_loss: 3.3755\n",
      "Iteration: 96 of 241\ttrain_loss: 3.4625\n",
      "Iteration: 98 of 241\ttrain_loss: 3.0966\n",
      "Iteration: 100 of 241\ttrain_loss: 3.3601\n",
      "Iteration: 102 of 241\ttrain_loss: 2.9649\n",
      "Iteration: 104 of 241\ttrain_loss: 2.9969\n",
      "Iteration: 106 of 241\ttrain_loss: 3.1010\n",
      "Iteration: 108 of 241\ttrain_loss: 3.2763\n",
      "Iteration: 110 of 241\ttrain_loss: 3.1394\n",
      "Iteration: 112 of 241\ttrain_loss: 3.0120\n",
      "Iteration: 114 of 241\ttrain_loss: 3.5626\n",
      "Iteration: 116 of 241\ttrain_loss: 3.3004\n",
      "Iteration: 118 of 241\ttrain_loss: 3.3255\n",
      "Iteration: 120 of 241\ttrain_loss: 3.3437\n",
      "Iteration: 122 of 241\ttrain_loss: 3.2022\n",
      "Iteration: 124 of 241\ttrain_loss: 3.5278\n",
      "Iteration: 126 of 241\ttrain_loss: 3.1024\n",
      "Iteration: 128 of 241\ttrain_loss: 3.0369\n",
      "Iteration: 130 of 241\ttrain_loss: 3.5440\n",
      "Iteration: 132 of 241\ttrain_loss: 3.5636\n",
      "Iteration: 134 of 241\ttrain_loss: 3.0601\n",
      "Iteration: 136 of 241\ttrain_loss: 3.0692\n",
      "Iteration: 138 of 241\ttrain_loss: 3.5291\n",
      "Iteration: 140 of 241\ttrain_loss: 2.9664\n",
      "Iteration: 142 of 241\ttrain_loss: 2.9624\n",
      "Iteration: 144 of 241\ttrain_loss: 3.3694\n",
      "Iteration: 146 of 241\ttrain_loss: 3.4631\n",
      "Iteration: 148 of 241\ttrain_loss: 3.0861\n",
      "Iteration: 150 of 241\ttrain_loss: 3.4071\n",
      "Iteration: 152 of 241\ttrain_loss: 3.2449\n",
      "Iteration: 154 of 241\ttrain_loss: 3.6626\n",
      "Iteration: 156 of 241\ttrain_loss: 2.7864\n",
      "Iteration: 158 of 241\ttrain_loss: 3.3220\n",
      "Iteration: 160 of 241\ttrain_loss: 3.3123\n",
      "Iteration: 162 of 241\ttrain_loss: 3.0230\n",
      "Iteration: 164 of 241\ttrain_loss: 3.7307\n",
      "Iteration: 166 of 241\ttrain_loss: 3.1197\n",
      "Iteration: 168 of 241\ttrain_loss: 3.0104\n",
      "Iteration: 170 of 241\ttrain_loss: 3.3299\n",
      "Iteration: 172 of 241\ttrain_loss: 3.4865\n",
      "Iteration: 174 of 241\ttrain_loss: 3.1932\n",
      "Iteration: 176 of 241\ttrain_loss: 3.2846\n",
      "Iteration: 178 of 241\ttrain_loss: 3.3188\n",
      "Iteration: 180 of 241\ttrain_loss: 3.0109\n",
      "Iteration: 182 of 241\ttrain_loss: 3.2344\n",
      "Iteration: 184 of 241\ttrain_loss: 3.4810\n",
      "Iteration: 186 of 241\ttrain_loss: 3.5223\n",
      "Iteration: 188 of 241\ttrain_loss: 3.5647\n",
      "Iteration: 190 of 241\ttrain_loss: 3.4121\n",
      "Iteration: 192 of 241\ttrain_loss: 3.2015\n",
      "Iteration: 194 of 241\ttrain_loss: 3.7196\n",
      "Iteration: 196 of 241\ttrain_loss: 3.5010\n",
      "Iteration: 198 of 241\ttrain_loss: 3.5856\n",
      "Iteration: 200 of 241\ttrain_loss: 3.4860\n",
      "Iteration: 202 of 241\ttrain_loss: 3.9357\n",
      "Iteration: 204 of 241\ttrain_loss: 3.5541\n",
      "Iteration: 206 of 241\ttrain_loss: 3.2809\n",
      "Iteration: 208 of 241\ttrain_loss: 3.4135\n",
      "Iteration: 210 of 241\ttrain_loss: 3.3337\n",
      "Iteration: 212 of 241\ttrain_loss: 3.3889\n",
      "Iteration: 214 of 241\ttrain_loss: 3.5378\n",
      "Iteration: 216 of 241\ttrain_loss: 2.9975\n",
      "Iteration: 218 of 241\ttrain_loss: 3.5536\n",
      "Iteration: 220 of 241\ttrain_loss: 3.2083\n",
      "Iteration: 222 of 241\ttrain_loss: 3.4520\n",
      "Iteration: 224 of 241\ttrain_loss: 3.1975\n",
      "Iteration: 226 of 241\ttrain_loss: 3.5283\n",
      "Iteration: 228 of 241\ttrain_loss: 3.6788\n",
      "Iteration: 230 of 241\ttrain_loss: 3.5228\n",
      "Iteration: 232 of 241\ttrain_loss: 3.0855\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9628\n",
      "Iteration: 236 of 241\ttrain_loss: 3.4571\n",
      "Iteration: 238 of 241\ttrain_loss: 3.4073\n",
      "Iteration: 240 of 241\ttrain_loss: 3.8160\n",
      "Iteration: 241 of 241\ttrain_loss: 3.5550\n",
      "Average Score for this Epoch: 3.264617443084717\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 42 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.1346\n",
      "Iteration: 2 of 241\ttrain_loss: 3.1634\n",
      "Iteration: 4 of 241\ttrain_loss: 2.9181\n",
      "Iteration: 6 of 241\ttrain_loss: 2.9277\n",
      "Iteration: 8 of 241\ttrain_loss: 3.3355\n",
      "Iteration: 10 of 241\ttrain_loss: 2.9537\n",
      "Iteration: 12 of 241\ttrain_loss: 3.3462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 14 of 241\ttrain_loss: 2.9714\n",
      "Iteration: 16 of 241\ttrain_loss: 3.3600\n",
      "Iteration: 18 of 241\ttrain_loss: 3.1188\n",
      "Iteration: 20 of 241\ttrain_loss: 3.3235\n",
      "Iteration: 22 of 241\ttrain_loss: 3.2869\n",
      "Iteration: 24 of 241\ttrain_loss: 2.9962\n",
      "Iteration: 26 of 241\ttrain_loss: 3.1099\n",
      "Iteration: 28 of 241\ttrain_loss: 2.8581\n",
      "Iteration: 30 of 241\ttrain_loss: 2.8604\n",
      "Iteration: 32 of 241\ttrain_loss: 3.1121\n",
      "Iteration: 34 of 241\ttrain_loss: 3.5705\n",
      "Iteration: 36 of 241\ttrain_loss: 3.0737\n",
      "Iteration: 38 of 241\ttrain_loss: 3.1134\n",
      "Iteration: 40 of 241\ttrain_loss: 2.8829\n",
      "Iteration: 42 of 241\ttrain_loss: 3.2375\n",
      "Iteration: 44 of 241\ttrain_loss: 3.1058\n",
      "Iteration: 46 of 241\ttrain_loss: 2.7105\n",
      "Iteration: 48 of 241\ttrain_loss: 2.9525\n",
      "Iteration: 50 of 241\ttrain_loss: 2.7000\n",
      "Iteration: 52 of 241\ttrain_loss: 2.9080\n",
      "Iteration: 54 of 241\ttrain_loss: 3.0931\n",
      "Iteration: 56 of 241\ttrain_loss: 3.1389\n",
      "Iteration: 58 of 241\ttrain_loss: 3.2152\n",
      "Iteration: 60 of 241\ttrain_loss: 2.8302\n",
      "Iteration: 62 of 241\ttrain_loss: 3.4045\n",
      "Iteration: 64 of 241\ttrain_loss: 3.0684\n",
      "Iteration: 66 of 241\ttrain_loss: 3.3752\n",
      "Iteration: 68 of 241\ttrain_loss: 2.7585\n",
      "Iteration: 70 of 241\ttrain_loss: 3.0670\n",
      "Iteration: 72 of 241\ttrain_loss: 3.1139\n",
      "Iteration: 74 of 241\ttrain_loss: 3.3700\n",
      "Iteration: 76 of 241\ttrain_loss: 3.2201\n",
      "Iteration: 78 of 241\ttrain_loss: 2.8140\n",
      "Iteration: 80 of 241\ttrain_loss: 2.9054\n",
      "Iteration: 82 of 241\ttrain_loss: 3.0842\n",
      "Iteration: 84 of 241\ttrain_loss: 2.8894\n",
      "Iteration: 86 of 241\ttrain_loss: 3.0948\n",
      "Iteration: 88 of 241\ttrain_loss: 2.8567\n",
      "Iteration: 90 of 241\ttrain_loss: 2.8823\n",
      "Iteration: 92 of 241\ttrain_loss: 3.0990\n",
      "Iteration: 94 of 241\ttrain_loss: 3.1200\n",
      "Iteration: 96 of 241\ttrain_loss: 3.0708\n",
      "Iteration: 98 of 241\ttrain_loss: 2.8732\n",
      "Iteration: 100 of 241\ttrain_loss: 2.9402\n",
      "Iteration: 102 of 241\ttrain_loss: 2.9272\n",
      "Iteration: 104 of 241\ttrain_loss: 3.0140\n",
      "Iteration: 106 of 241\ttrain_loss: 3.7077\n",
      "Iteration: 108 of 241\ttrain_loss: 3.2304\n",
      "Iteration: 110 of 241\ttrain_loss: 3.3415\n",
      "Iteration: 112 of 241\ttrain_loss: 3.7508\n",
      "Iteration: 114 of 241\ttrain_loss: 3.3575\n",
      "Iteration: 116 of 241\ttrain_loss: 3.2321\n",
      "Iteration: 118 of 241\ttrain_loss: 3.1945\n",
      "Iteration: 120 of 241\ttrain_loss: 3.1868\n",
      "Iteration: 122 of 241\ttrain_loss: 3.0845\n",
      "Iteration: 124 of 241\ttrain_loss: 3.3207\n",
      "Iteration: 126 of 241\ttrain_loss: 2.8037\n",
      "Iteration: 128 of 241\ttrain_loss: 3.2511\n",
      "Iteration: 130 of 241\ttrain_loss: 3.1986\n",
      "Iteration: 132 of 241\ttrain_loss: 4.0831\n",
      "Iteration: 134 of 241\ttrain_loss: 3.2480\n",
      "Iteration: 136 of 241\ttrain_loss: 2.9412\n",
      "Iteration: 138 of 241\ttrain_loss: 3.0457\n",
      "Iteration: 140 of 241\ttrain_loss: 3.4400\n",
      "Iteration: 142 of 241\ttrain_loss: 3.3048\n",
      "Iteration: 144 of 241\ttrain_loss: 3.2049\n",
      "Iteration: 146 of 241\ttrain_loss: 3.2020\n",
      "Iteration: 148 of 241\ttrain_loss: 3.2050\n",
      "Iteration: 150 of 241\ttrain_loss: 3.1726\n",
      "Iteration: 152 of 241\ttrain_loss: 3.3973\n",
      "Iteration: 154 of 241\ttrain_loss: 3.6143\n",
      "Iteration: 156 of 241\ttrain_loss: 3.4956\n",
      "Iteration: 158 of 241\ttrain_loss: 3.2685\n",
      "Iteration: 160 of 241\ttrain_loss: 3.2913\n",
      "Iteration: 162 of 241\ttrain_loss: 3.1123\n",
      "Iteration: 164 of 241\ttrain_loss: 3.2096\n",
      "Iteration: 166 of 241\ttrain_loss: 3.3541\n",
      "Iteration: 168 of 241\ttrain_loss: 3.1698\n",
      "Iteration: 170 of 241\ttrain_loss: 3.1181\n",
      "Iteration: 172 of 241\ttrain_loss: 3.4237\n",
      "Iteration: 174 of 241\ttrain_loss: 3.3221\n",
      "Iteration: 176 of 241\ttrain_loss: 3.1402\n",
      "Iteration: 178 of 241\ttrain_loss: 3.3263\n",
      "Iteration: 180 of 241\ttrain_loss: 3.1750\n",
      "Iteration: 182 of 241\ttrain_loss: 3.3906\n",
      "Iteration: 184 of 241\ttrain_loss: 3.4144\n",
      "Iteration: 186 of 241\ttrain_loss: 3.2644\n",
      "Iteration: 188 of 241\ttrain_loss: 3.6448\n",
      "Iteration: 190 of 241\ttrain_loss: 3.1522\n",
      "Iteration: 192 of 241\ttrain_loss: 3.0740\n",
      "Iteration: 194 of 241\ttrain_loss: 3.8326\n",
      "Iteration: 196 of 241\ttrain_loss: 3.3571\n",
      "Iteration: 198 of 241\ttrain_loss: 3.3486\n",
      "Iteration: 200 of 241\ttrain_loss: 3.1673\n",
      "Iteration: 202 of 241\ttrain_loss: 3.3818\n",
      "Iteration: 204 of 241\ttrain_loss: 3.2743\n",
      "Iteration: 206 of 241\ttrain_loss: 3.0387\n",
      "Iteration: 208 of 241\ttrain_loss: 3.3226\n",
      "Iteration: 210 of 241\ttrain_loss: 3.4621\n",
      "Iteration: 212 of 241\ttrain_loss: 3.1018\n",
      "Iteration: 214 of 241\ttrain_loss: 3.2842\n",
      "Iteration: 216 of 241\ttrain_loss: 3.0875\n",
      "Iteration: 218 of 241\ttrain_loss: 3.0249\n",
      "Iteration: 220 of 241\ttrain_loss: 3.5171\n",
      "Iteration: 222 of 241\ttrain_loss: 3.0249\n",
      "Iteration: 224 of 241\ttrain_loss: 3.3972\n",
      "Iteration: 226 of 241\ttrain_loss: 3.1236\n",
      "Iteration: 228 of 241\ttrain_loss: 3.5178\n",
      "Iteration: 230 of 241\ttrain_loss: 3.0513\n",
      "Iteration: 232 of 241\ttrain_loss: 3.0814\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9065\n",
      "Iteration: 236 of 241\ttrain_loss: 3.2572\n",
      "Iteration: 238 of 241\ttrain_loss: 3.3905\n",
      "Iteration: 240 of 241\ttrain_loss: 3.5471\n",
      "Iteration: 241 of 241\ttrain_loss: 3.4143\n",
      "Average Score for this Epoch: 3.1625568866729736\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 43 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.4063\n",
      "Iteration: 2 of 241\ttrain_loss: 2.4254\n",
      "Iteration: 4 of 241\ttrain_loss: 2.6551\n",
      "Iteration: 6 of 241\ttrain_loss: 2.9501\n",
      "Iteration: 8 of 241\ttrain_loss: 2.6743\n",
      "Iteration: 10 of 241\ttrain_loss: 3.1081\n",
      "Iteration: 12 of 241\ttrain_loss: 3.0865\n",
      "Iteration: 14 of 241\ttrain_loss: 2.7167\n",
      "Iteration: 16 of 241\ttrain_loss: 2.4261\n",
      "Iteration: 18 of 241\ttrain_loss: 2.8007\n",
      "Iteration: 20 of 241\ttrain_loss: 2.7302\n",
      "Iteration: 22 of 241\ttrain_loss: 2.7694\n",
      "Iteration: 24 of 241\ttrain_loss: 3.1694\n",
      "Iteration: 26 of 241\ttrain_loss: 3.1602\n",
      "Iteration: 28 of 241\ttrain_loss: 2.9822\n",
      "Iteration: 30 of 241\ttrain_loss: 2.7138\n",
      "Iteration: 32 of 241\ttrain_loss: 2.6107\n",
      "Iteration: 34 of 241\ttrain_loss: 3.0913\n",
      "Iteration: 36 of 241\ttrain_loss: 3.1622\n",
      "Iteration: 38 of 241\ttrain_loss: 3.2872\n",
      "Iteration: 40 of 241\ttrain_loss: 3.1105\n",
      "Iteration: 42 of 241\ttrain_loss: 2.9823\n",
      "Iteration: 44 of 241\ttrain_loss: 2.7731\n",
      "Iteration: 46 of 241\ttrain_loss: 2.8013\n",
      "Iteration: 48 of 241\ttrain_loss: 3.3397\n",
      "Iteration: 50 of 241\ttrain_loss: 3.1122\n",
      "Iteration: 52 of 241\ttrain_loss: 2.3429\n",
      "Iteration: 54 of 241\ttrain_loss: 2.9741\n",
      "Iteration: 56 of 241\ttrain_loss: 2.8364\n",
      "Iteration: 58 of 241\ttrain_loss: 3.2359\n",
      "Iteration: 60 of 241\ttrain_loss: 2.9648\n",
      "Iteration: 62 of 241\ttrain_loss: 2.9144\n",
      "Iteration: 64 of 241\ttrain_loss: 3.0976\n",
      "Iteration: 66 of 241\ttrain_loss: 3.0272\n",
      "Iteration: 68 of 241\ttrain_loss: 2.8438\n",
      "Iteration: 70 of 241\ttrain_loss: 2.8507\n",
      "Iteration: 72 of 241\ttrain_loss: 2.6472\n",
      "Iteration: 74 of 241\ttrain_loss: 3.2273\n",
      "Iteration: 76 of 241\ttrain_loss: 3.0368\n",
      "Iteration: 78 of 241\ttrain_loss: 3.0935\n",
      "Iteration: 80 of 241\ttrain_loss: 3.1649\n",
      "Iteration: 82 of 241\ttrain_loss: 3.0759\n",
      "Iteration: 84 of 241\ttrain_loss: 3.1105\n",
      "Iteration: 86 of 241\ttrain_loss: 3.0162\n",
      "Iteration: 88 of 241\ttrain_loss: 3.3075\n",
      "Iteration: 90 of 241\ttrain_loss: 3.1140\n",
      "Iteration: 92 of 241\ttrain_loss: 3.2483\n",
      "Iteration: 94 of 241\ttrain_loss: 3.4649\n",
      "Iteration: 96 of 241\ttrain_loss: 2.7604\n",
      "Iteration: 98 of 241\ttrain_loss: 3.0680\n",
      "Iteration: 100 of 241\ttrain_loss: 2.9352\n",
      "Iteration: 102 of 241\ttrain_loss: 3.2763\n",
      "Iteration: 104 of 241\ttrain_loss: 3.3026\n",
      "Iteration: 106 of 241\ttrain_loss: 2.9485\n",
      "Iteration: 108 of 241\ttrain_loss: 4.1683\n",
      "Iteration: 110 of 241\ttrain_loss: 3.2941\n",
      "Iteration: 112 of 241\ttrain_loss: 2.9328\n",
      "Iteration: 114 of 241\ttrain_loss: 3.0946\n",
      "Iteration: 116 of 241\ttrain_loss: 2.7837\n",
      "Iteration: 118 of 241\ttrain_loss: 3.0969\n",
      "Iteration: 120 of 241\ttrain_loss: 2.8698\n",
      "Iteration: 122 of 241\ttrain_loss: 3.1902\n",
      "Iteration: 124 of 241\ttrain_loss: 2.9205\n",
      "Iteration: 126 of 241\ttrain_loss: 3.3037\n",
      "Iteration: 128 of 241\ttrain_loss: 3.1157\n",
      "Iteration: 130 of 241\ttrain_loss: 3.0272\n",
      "Iteration: 132 of 241\ttrain_loss: 3.2883\n",
      "Iteration: 134 of 241\ttrain_loss: 3.0694\n",
      "Iteration: 136 of 241\ttrain_loss: 3.7457\n",
      "Iteration: 138 of 241\ttrain_loss: 3.8159\n",
      "Iteration: 140 of 241\ttrain_loss: 3.2957\n",
      "Iteration: 142 of 241\ttrain_loss: 3.3192\n",
      "Iteration: 144 of 241\ttrain_loss: 2.8418\n",
      "Iteration: 146 of 241\ttrain_loss: 3.0511\n",
      "Iteration: 148 of 241\ttrain_loss: 3.1570\n",
      "Iteration: 150 of 241\ttrain_loss: 3.0144\n",
      "Iteration: 152 of 241\ttrain_loss: 3.3001\n",
      "Iteration: 154 of 241\ttrain_loss: 3.3702\n",
      "Iteration: 156 of 241\ttrain_loss: 3.3904\n",
      "Iteration: 158 of 241\ttrain_loss: 3.3530\n",
      "Iteration: 160 of 241\ttrain_loss: 3.0263\n",
      "Iteration: 162 of 241\ttrain_loss: 3.7686\n",
      "Iteration: 164 of 241\ttrain_loss: 3.4268\n",
      "Iteration: 166 of 241\ttrain_loss: 2.9708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 168 of 241\ttrain_loss: 3.8520\n",
      "Iteration: 170 of 241\ttrain_loss: 3.0094\n",
      "Iteration: 172 of 241\ttrain_loss: 3.1027\n",
      "Iteration: 174 of 241\ttrain_loss: 2.9737\n",
      "Iteration: 176 of 241\ttrain_loss: 3.3416\n",
      "Iteration: 178 of 241\ttrain_loss: 3.1257\n",
      "Iteration: 180 of 241\ttrain_loss: 3.3120\n",
      "Iteration: 182 of 241\ttrain_loss: 3.0815\n",
      "Iteration: 184 of 241\ttrain_loss: 2.9380\n",
      "Iteration: 186 of 241\ttrain_loss: 3.2064\n",
      "Iteration: 188 of 241\ttrain_loss: 2.9979\n",
      "Iteration: 190 of 241\ttrain_loss: 3.2739\n",
      "Iteration: 192 of 241\ttrain_loss: 3.3064\n",
      "Iteration: 194 of 241\ttrain_loss: 3.4367\n",
      "Iteration: 196 of 241\ttrain_loss: 3.2044\n",
      "Iteration: 198 of 241\ttrain_loss: 2.9482\n",
      "Iteration: 200 of 241\ttrain_loss: 3.5572\n",
      "Iteration: 202 of 241\ttrain_loss: 3.3254\n",
      "Iteration: 204 of 241\ttrain_loss: 2.8854\n",
      "Iteration: 206 of 241\ttrain_loss: 3.1247\n",
      "Iteration: 208 of 241\ttrain_loss: 2.8418\n",
      "Iteration: 210 of 241\ttrain_loss: 3.0604\n",
      "Iteration: 212 of 241\ttrain_loss: 3.6185\n",
      "Iteration: 214 of 241\ttrain_loss: 3.2407\n",
      "Iteration: 216 of 241\ttrain_loss: 3.5958\n",
      "Iteration: 218 of 241\ttrain_loss: 3.2813\n",
      "Iteration: 220 of 241\ttrain_loss: 3.1005\n",
      "Iteration: 222 of 241\ttrain_loss: 3.0695\n",
      "Iteration: 224 of 241\ttrain_loss: 3.2078\n",
      "Iteration: 226 of 241\ttrain_loss: 3.3663\n",
      "Iteration: 228 of 241\ttrain_loss: 3.1026\n",
      "Iteration: 230 of 241\ttrain_loss: 3.0328\n",
      "Iteration: 232 of 241\ttrain_loss: 3.2955\n",
      "Iteration: 234 of 241\ttrain_loss: 3.4339\n",
      "Iteration: 236 of 241\ttrain_loss: 3.2522\n",
      "Iteration: 238 of 241\ttrain_loss: 3.2907\n",
      "Iteration: 240 of 241\ttrain_loss: 3.2074\n",
      "Iteration: 241 of 241\ttrain_loss: 2.9667\n",
      "Average Score for this Epoch: 3.1215269565582275\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 44 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.8061\n",
      "Iteration: 2 of 241\ttrain_loss: 2.7865\n",
      "Iteration: 4 of 241\ttrain_loss: 2.7426\n",
      "Iteration: 6 of 241\ttrain_loss: 2.7964\n",
      "Iteration: 8 of 241\ttrain_loss: 2.5966\n",
      "Iteration: 10 of 241\ttrain_loss: 3.0778\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7932\n",
      "Iteration: 14 of 241\ttrain_loss: 3.1656\n",
      "Iteration: 16 of 241\ttrain_loss: 3.0565\n",
      "Iteration: 18 of 241\ttrain_loss: 2.5438\n",
      "Iteration: 20 of 241\ttrain_loss: 2.8861\n",
      "Iteration: 22 of 241\ttrain_loss: 2.9255\n",
      "Iteration: 24 of 241\ttrain_loss: 3.0680\n",
      "Iteration: 26 of 241\ttrain_loss: 3.0577\n",
      "Iteration: 28 of 241\ttrain_loss: 3.0019\n",
      "Iteration: 30 of 241\ttrain_loss: 2.6864\n",
      "Iteration: 32 of 241\ttrain_loss: 2.9802\n",
      "Iteration: 34 of 241\ttrain_loss: 3.2247\n",
      "Iteration: 36 of 241\ttrain_loss: 2.9736\n",
      "Iteration: 38 of 241\ttrain_loss: 3.1411\n",
      "Iteration: 40 of 241\ttrain_loss: 2.6567\n",
      "Iteration: 42 of 241\ttrain_loss: 2.9082\n",
      "Iteration: 44 of 241\ttrain_loss: 3.2402\n",
      "Iteration: 46 of 241\ttrain_loss: 3.2577\n",
      "Iteration: 48 of 241\ttrain_loss: 2.7817\n",
      "Iteration: 50 of 241\ttrain_loss: 3.3347\n",
      "Iteration: 52 of 241\ttrain_loss: 3.2413\n",
      "Iteration: 54 of 241\ttrain_loss: 2.9077\n",
      "Iteration: 56 of 241\ttrain_loss: 2.9186\n",
      "Iteration: 58 of 241\ttrain_loss: 3.9554\n",
      "Iteration: 60 of 241\ttrain_loss: 2.8188\n",
      "Iteration: 62 of 241\ttrain_loss: 3.2103\n",
      "Iteration: 64 of 241\ttrain_loss: 2.7427\n",
      "Iteration: 66 of 241\ttrain_loss: 3.2498\n",
      "Iteration: 68 of 241\ttrain_loss: 3.4687\n",
      "Iteration: 70 of 241\ttrain_loss: 2.8333\n",
      "Iteration: 72 of 241\ttrain_loss: 3.2157\n",
      "Iteration: 74 of 241\ttrain_loss: 3.1212\n",
      "Iteration: 76 of 241\ttrain_loss: 3.3964\n",
      "Iteration: 78 of 241\ttrain_loss: 3.0836\n",
      "Iteration: 80 of 241\ttrain_loss: 3.0270\n",
      "Iteration: 82 of 241\ttrain_loss: 2.8884\n",
      "Iteration: 84 of 241\ttrain_loss: 3.2158\n",
      "Iteration: 86 of 241\ttrain_loss: 3.2887\n",
      "Iteration: 88 of 241\ttrain_loss: 2.9431\n",
      "Iteration: 90 of 241\ttrain_loss: 3.1647\n",
      "Iteration: 92 of 241\ttrain_loss: 2.9145\n",
      "Iteration: 94 of 241\ttrain_loss: 3.1435\n",
      "Iteration: 96 of 241\ttrain_loss: 3.0523\n",
      "Iteration: 98 of 241\ttrain_loss: 3.4312\n",
      "Iteration: 100 of 241\ttrain_loss: 2.9966\n",
      "Iteration: 102 of 241\ttrain_loss: 3.3186\n",
      "Iteration: 104 of 241\ttrain_loss: 3.3364\n",
      "Iteration: 106 of 241\ttrain_loss: 3.0182\n",
      "Iteration: 108 of 241\ttrain_loss: 3.0868\n",
      "Iteration: 110 of 241\ttrain_loss: 2.8637\n",
      "Iteration: 112 of 241\ttrain_loss: 3.2360\n",
      "Iteration: 114 of 241\ttrain_loss: 2.8505\n",
      "Iteration: 116 of 241\ttrain_loss: 3.0738\n",
      "Iteration: 118 of 241\ttrain_loss: 2.9214\n",
      "Iteration: 120 of 241\ttrain_loss: 2.8692\n",
      "Iteration: 122 of 241\ttrain_loss: 3.0181\n",
      "Iteration: 124 of 241\ttrain_loss: 3.0648\n",
      "Iteration: 126 of 241\ttrain_loss: 3.0521\n",
      "Iteration: 128 of 241\ttrain_loss: 3.0867\n",
      "Iteration: 130 of 241\ttrain_loss: 3.1312\n",
      "Iteration: 132 of 241\ttrain_loss: 3.0419\n",
      "Iteration: 134 of 241\ttrain_loss: 3.2938\n",
      "Iteration: 136 of 241\ttrain_loss: 3.1389\n",
      "Iteration: 138 of 241\ttrain_loss: 3.0860\n",
      "Iteration: 140 of 241\ttrain_loss: 2.7562\n",
      "Iteration: 142 of 241\ttrain_loss: 2.9081\n",
      "Iteration: 144 of 241\ttrain_loss: 2.9236\n",
      "Iteration: 146 of 241\ttrain_loss: 3.0177\n",
      "Iteration: 148 of 241\ttrain_loss: 2.9266\n",
      "Iteration: 150 of 241\ttrain_loss: 3.2429\n",
      "Iteration: 152 of 241\ttrain_loss: 2.7698\n",
      "Iteration: 154 of 241\ttrain_loss: 2.9808\n",
      "Iteration: 156 of 241\ttrain_loss: 2.6764\n",
      "Iteration: 158 of 241\ttrain_loss: 3.3348\n",
      "Iteration: 160 of 241\ttrain_loss: 2.9575\n",
      "Iteration: 162 of 241\ttrain_loss: 2.9110\n",
      "Iteration: 164 of 241\ttrain_loss: 3.0151\n",
      "Iteration: 166 of 241\ttrain_loss: 2.8310\n",
      "Iteration: 168 of 241\ttrain_loss: 2.9253\n",
      "Iteration: 170 of 241\ttrain_loss: 3.2045\n",
      "Iteration: 172 of 241\ttrain_loss: 2.9859\n",
      "Iteration: 174 of 241\ttrain_loss: 3.9469\n",
      "Iteration: 176 of 241\ttrain_loss: 3.3780\n",
      "Iteration: 178 of 241\ttrain_loss: 3.6055\n",
      "Iteration: 180 of 241\ttrain_loss: 2.7156\n",
      "Iteration: 182 of 241\ttrain_loss: 2.8716\n",
      "Iteration: 184 of 241\ttrain_loss: 3.3934\n",
      "Iteration: 186 of 241\ttrain_loss: 3.1245\n",
      "Iteration: 188 of 241\ttrain_loss: 3.0129\n",
      "Iteration: 190 of 241\ttrain_loss: 3.4142\n",
      "Iteration: 192 of 241\ttrain_loss: 3.3482\n",
      "Iteration: 194 of 241\ttrain_loss: 2.7841\n",
      "Iteration: 196 of 241\ttrain_loss: 2.8107\n",
      "Iteration: 198 of 241\ttrain_loss: 3.3128\n",
      "Iteration: 200 of 241\ttrain_loss: 2.7996\n",
      "Iteration: 202 of 241\ttrain_loss: 3.1968\n",
      "Iteration: 204 of 241\ttrain_loss: 3.1681\n",
      "Iteration: 206 of 241\ttrain_loss: 2.8377\n",
      "Iteration: 208 of 241\ttrain_loss: 3.1834\n",
      "Iteration: 210 of 241\ttrain_loss: 2.8036\n",
      "Iteration: 212 of 241\ttrain_loss: 3.1303\n",
      "Iteration: 214 of 241\ttrain_loss: 3.4073\n",
      "Iteration: 216 of 241\ttrain_loss: 2.8626\n",
      "Iteration: 218 of 241\ttrain_loss: 2.8603\n",
      "Iteration: 220 of 241\ttrain_loss: 3.2265\n",
      "Iteration: 222 of 241\ttrain_loss: 3.1105\n",
      "Iteration: 224 of 241\ttrain_loss: 2.9293\n",
      "Iteration: 226 of 241\ttrain_loss: 3.0396\n",
      "Iteration: 228 of 241\ttrain_loss: 2.7814\n",
      "Iteration: 230 of 241\ttrain_loss: 3.3117\n",
      "Iteration: 232 of 241\ttrain_loss: 3.3584\n",
      "Iteration: 234 of 241\ttrain_loss: 3.3530\n",
      "Iteration: 236 of 241\ttrain_loss: 3.1562\n",
      "Iteration: 238 of 241\ttrain_loss: 2.7155\n",
      "Iteration: 240 of 241\ttrain_loss: 2.9771\n",
      "Iteration: 241 of 241\ttrain_loss: 3.2036\n",
      "Average Score for this Epoch: 3.042151927947998\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 45 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.5680\n",
      "Iteration: 2 of 241\ttrain_loss: 2.6581\n",
      "Iteration: 4 of 241\ttrain_loss: 2.6092\n",
      "Iteration: 6 of 241\ttrain_loss: 2.7969\n",
      "Iteration: 8 of 241\ttrain_loss: 2.9804\n",
      "Iteration: 10 of 241\ttrain_loss: 2.9536\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7551\n",
      "Iteration: 14 of 241\ttrain_loss: 2.6090\n",
      "Iteration: 16 of 241\ttrain_loss: 2.9587\n",
      "Iteration: 18 of 241\ttrain_loss: 2.8531\n",
      "Iteration: 20 of 241\ttrain_loss: 2.6737\n",
      "Iteration: 22 of 241\ttrain_loss: 3.2058\n",
      "Iteration: 24 of 241\ttrain_loss: 2.7384\n",
      "Iteration: 26 of 241\ttrain_loss: 3.3026\n",
      "Iteration: 28 of 241\ttrain_loss: 3.1201\n",
      "Iteration: 30 of 241\ttrain_loss: 3.0322\n",
      "Iteration: 32 of 241\ttrain_loss: 3.2509\n",
      "Iteration: 34 of 241\ttrain_loss: 2.9753\n",
      "Iteration: 36 of 241\ttrain_loss: 3.0879\n",
      "Iteration: 38 of 241\ttrain_loss: 2.9780\n",
      "Iteration: 40 of 241\ttrain_loss: 3.0041\n",
      "Iteration: 42 of 241\ttrain_loss: 2.6067\n",
      "Iteration: 44 of 241\ttrain_loss: 2.9839\n",
      "Iteration: 46 of 241\ttrain_loss: 2.8777\n",
      "Iteration: 48 of 241\ttrain_loss: 2.6586\n",
      "Iteration: 50 of 241\ttrain_loss: 3.0401\n",
      "Iteration: 52 of 241\ttrain_loss: 3.0052\n",
      "Iteration: 54 of 241\ttrain_loss: 3.1942\n",
      "Iteration: 56 of 241\ttrain_loss: 3.3597\n",
      "Iteration: 58 of 241\ttrain_loss: 2.4922\n",
      "Iteration: 60 of 241\ttrain_loss: 3.1766\n",
      "Iteration: 62 of 241\ttrain_loss: 2.8884\n",
      "Iteration: 64 of 241\ttrain_loss: 2.9223\n",
      "Iteration: 66 of 241\ttrain_loss: 2.7292\n",
      "Iteration: 68 of 241\ttrain_loss: 2.6721\n",
      "Iteration: 70 of 241\ttrain_loss: 2.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 72 of 241\ttrain_loss: 2.8531\n",
      "Iteration: 74 of 241\ttrain_loss: 2.8144\n",
      "Iteration: 76 of 241\ttrain_loss: 3.1607\n",
      "Iteration: 78 of 241\ttrain_loss: 3.3976\n",
      "Iteration: 80 of 241\ttrain_loss: 2.9647\n",
      "Iteration: 82 of 241\ttrain_loss: 2.8597\n",
      "Iteration: 84 of 241\ttrain_loss: 2.8209\n",
      "Iteration: 86 of 241\ttrain_loss: 2.9758\n",
      "Iteration: 88 of 241\ttrain_loss: 3.0687\n",
      "Iteration: 90 of 241\ttrain_loss: 3.3710\n",
      "Iteration: 92 of 241\ttrain_loss: 2.7493\n",
      "Iteration: 94 of 241\ttrain_loss: 2.8300\n",
      "Iteration: 96 of 241\ttrain_loss: 2.9379\n",
      "Iteration: 98 of 241\ttrain_loss: 2.7570\n",
      "Iteration: 100 of 241\ttrain_loss: 2.6036\n",
      "Iteration: 102 of 241\ttrain_loss: 2.9698\n",
      "Iteration: 104 of 241\ttrain_loss: 3.0651\n",
      "Iteration: 106 of 241\ttrain_loss: 2.7650\n",
      "Iteration: 108 of 241\ttrain_loss: 2.5046\n",
      "Iteration: 110 of 241\ttrain_loss: 2.9992\n",
      "Iteration: 112 of 241\ttrain_loss: 2.9517\n",
      "Iteration: 114 of 241\ttrain_loss: 2.9288\n",
      "Iteration: 116 of 241\ttrain_loss: 2.6592\n",
      "Iteration: 118 of 241\ttrain_loss: 3.1376\n",
      "Iteration: 120 of 241\ttrain_loss: 3.1284\n",
      "Iteration: 122 of 241\ttrain_loss: 2.9420\n",
      "Iteration: 124 of 241\ttrain_loss: 2.8588\n",
      "Iteration: 126 of 241\ttrain_loss: 2.7466\n",
      "Iteration: 128 of 241\ttrain_loss: 2.9331\n",
      "Iteration: 130 of 241\ttrain_loss: 3.1305\n",
      "Iteration: 132 of 241\ttrain_loss: 2.4954\n",
      "Iteration: 134 of 241\ttrain_loss: 3.1618\n",
      "Iteration: 136 of 241\ttrain_loss: 2.5653\n",
      "Iteration: 138 of 241\ttrain_loss: 3.7531\n",
      "Iteration: 140 of 241\ttrain_loss: 2.8747\n",
      "Iteration: 142 of 241\ttrain_loss: 2.9510\n",
      "Iteration: 144 of 241\ttrain_loss: 2.8644\n",
      "Iteration: 146 of 241\ttrain_loss: 2.9841\n",
      "Iteration: 148 of 241\ttrain_loss: 2.6831\n",
      "Iteration: 150 of 241\ttrain_loss: 3.1495\n",
      "Iteration: 152 of 241\ttrain_loss: 2.5455\n",
      "Iteration: 154 of 241\ttrain_loss: 2.8699\n",
      "Iteration: 156 of 241\ttrain_loss: 2.8447\n",
      "Iteration: 158 of 241\ttrain_loss: 2.7916\n",
      "Iteration: 160 of 241\ttrain_loss: 2.9207\n",
      "Iteration: 162 of 241\ttrain_loss: 3.0387\n",
      "Iteration: 164 of 241\ttrain_loss: 2.9582\n",
      "Iteration: 166 of 241\ttrain_loss: 3.0881\n",
      "Iteration: 168 of 241\ttrain_loss: 2.6377\n",
      "Iteration: 170 of 241\ttrain_loss: 3.4288\n",
      "Iteration: 172 of 241\ttrain_loss: 2.6457\n",
      "Iteration: 174 of 241\ttrain_loss: 3.1978\n",
      "Iteration: 176 of 241\ttrain_loss: 2.9357\n",
      "Iteration: 178 of 241\ttrain_loss: 2.7072\n",
      "Iteration: 180 of 241\ttrain_loss: 3.2505\n",
      "Iteration: 182 of 241\ttrain_loss: 2.8313\n",
      "Iteration: 184 of 241\ttrain_loss: 3.0994\n",
      "Iteration: 186 of 241\ttrain_loss: 3.1683\n",
      "Iteration: 188 of 241\ttrain_loss: 2.8324\n",
      "Iteration: 190 of 241\ttrain_loss: 2.8472\n",
      "Iteration: 192 of 241\ttrain_loss: 2.6652\n",
      "Iteration: 194 of 241\ttrain_loss: 3.1765\n",
      "Iteration: 196 of 241\ttrain_loss: 2.9918\n",
      "Iteration: 198 of 241\ttrain_loss: 2.7680\n",
      "Iteration: 200 of 241\ttrain_loss: 2.8454\n",
      "Iteration: 202 of 241\ttrain_loss: 2.7806\n",
      "Iteration: 204 of 241\ttrain_loss: 3.3645\n",
      "Iteration: 206 of 241\ttrain_loss: 2.7959\n",
      "Iteration: 208 of 241\ttrain_loss: 3.1547\n",
      "Iteration: 210 of 241\ttrain_loss: 3.0880\n",
      "Iteration: 212 of 241\ttrain_loss: 3.0557\n",
      "Iteration: 214 of 241\ttrain_loss: 3.1459\n",
      "Iteration: 216 of 241\ttrain_loss: 3.0950\n",
      "Iteration: 218 of 241\ttrain_loss: 3.2429\n",
      "Iteration: 220 of 241\ttrain_loss: 2.9139\n",
      "Iteration: 222 of 241\ttrain_loss: 3.0053\n",
      "Iteration: 224 of 241\ttrain_loss: 3.2042\n",
      "Iteration: 226 of 241\ttrain_loss: 3.0725\n",
      "Iteration: 228 of 241\ttrain_loss: 2.7923\n",
      "Iteration: 230 of 241\ttrain_loss: 3.0769\n",
      "Iteration: 232 of 241\ttrain_loss: 3.1822\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9587\n",
      "Iteration: 236 of 241\ttrain_loss: 2.7945\n",
      "Iteration: 238 of 241\ttrain_loss: 3.0982\n",
      "Iteration: 240 of 241\ttrain_loss: 3.2543\n",
      "Iteration: 241 of 241\ttrain_loss: 2.8988\n",
      "Average Score for this Epoch: 2.9736921787261963\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 46 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.9445\n",
      "Iteration: 2 of 241\ttrain_loss: 2.6308\n",
      "Iteration: 4 of 241\ttrain_loss: 3.0305\n",
      "Iteration: 6 of 241\ttrain_loss: 2.9166\n",
      "Iteration: 8 of 241\ttrain_loss: 2.5350\n",
      "Iteration: 10 of 241\ttrain_loss: 3.0118\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7907\n",
      "Iteration: 14 of 241\ttrain_loss: 2.5099\n",
      "Iteration: 16 of 241\ttrain_loss: 2.5566\n",
      "Iteration: 18 of 241\ttrain_loss: 3.1440\n",
      "Iteration: 20 of 241\ttrain_loss: 2.6788\n",
      "Iteration: 22 of 241\ttrain_loss: 3.0484\n",
      "Iteration: 24 of 241\ttrain_loss: 3.0630\n",
      "Iteration: 26 of 241\ttrain_loss: 2.5718\n",
      "Iteration: 28 of 241\ttrain_loss: 2.4753\n",
      "Iteration: 30 of 241\ttrain_loss: 2.9249\n",
      "Iteration: 32 of 241\ttrain_loss: 2.8232\n",
      "Iteration: 34 of 241\ttrain_loss: 2.8466\n",
      "Iteration: 36 of 241\ttrain_loss: 3.5932\n",
      "Iteration: 38 of 241\ttrain_loss: 2.9418\n",
      "Iteration: 40 of 241\ttrain_loss: 3.3128\n",
      "Iteration: 42 of 241\ttrain_loss: 2.4463\n",
      "Iteration: 44 of 241\ttrain_loss: 2.8469\n",
      "Iteration: 46 of 241\ttrain_loss: 3.0398\n",
      "Iteration: 48 of 241\ttrain_loss: 2.8468\n",
      "Iteration: 50 of 241\ttrain_loss: 2.6277\n",
      "Iteration: 52 of 241\ttrain_loss: 2.4813\n",
      "Iteration: 54 of 241\ttrain_loss: 3.2997\n",
      "Iteration: 56 of 241\ttrain_loss: 2.8823\n",
      "Iteration: 58 of 241\ttrain_loss: 2.7867\n",
      "Iteration: 60 of 241\ttrain_loss: 2.8912\n",
      "Iteration: 62 of 241\ttrain_loss: 3.2770\n",
      "Iteration: 64 of 241\ttrain_loss: 2.6895\n",
      "Iteration: 66 of 241\ttrain_loss: 2.7319\n",
      "Iteration: 68 of 241\ttrain_loss: 2.6223\n",
      "Iteration: 70 of 241\ttrain_loss: 2.6729\n",
      "Iteration: 72 of 241\ttrain_loss: 3.0062\n",
      "Iteration: 74 of 241\ttrain_loss: 2.7104\n",
      "Iteration: 76 of 241\ttrain_loss: 2.3255\n",
      "Iteration: 78 of 241\ttrain_loss: 2.9524\n",
      "Iteration: 80 of 241\ttrain_loss: 2.6731\n",
      "Iteration: 82 of 241\ttrain_loss: 2.6747\n",
      "Iteration: 84 of 241\ttrain_loss: 2.6682\n",
      "Iteration: 86 of 241\ttrain_loss: 2.5898\n",
      "Iteration: 88 of 241\ttrain_loss: 2.9078\n",
      "Iteration: 90 of 241\ttrain_loss: 2.9157\n",
      "Iteration: 92 of 241\ttrain_loss: 3.1170\n",
      "Iteration: 94 of 241\ttrain_loss: 2.7804\n",
      "Iteration: 96 of 241\ttrain_loss: 2.7240\n",
      "Iteration: 98 of 241\ttrain_loss: 2.3917\n",
      "Iteration: 100 of 241\ttrain_loss: 3.1403\n",
      "Iteration: 102 of 241\ttrain_loss: 2.9247\n",
      "Iteration: 104 of 241\ttrain_loss: 2.8298\n",
      "Iteration: 106 of 241\ttrain_loss: 2.7625\n",
      "Iteration: 108 of 241\ttrain_loss: 3.2052\n",
      "Iteration: 110 of 241\ttrain_loss: 2.3985\n",
      "Iteration: 112 of 241\ttrain_loss: 2.2605\n",
      "Iteration: 114 of 241\ttrain_loss: 3.0162\n",
      "Iteration: 116 of 241\ttrain_loss: 2.7934\n",
      "Iteration: 118 of 241\ttrain_loss: 2.7963\n",
      "Iteration: 120 of 241\ttrain_loss: 2.7813\n",
      "Iteration: 122 of 241\ttrain_loss: 2.9335\n",
      "Iteration: 124 of 241\ttrain_loss: 3.0811\n",
      "Iteration: 126 of 241\ttrain_loss: 3.1229\n",
      "Iteration: 128 of 241\ttrain_loss: 2.9445\n",
      "Iteration: 130 of 241\ttrain_loss: 2.8708\n",
      "Iteration: 132 of 241\ttrain_loss: 3.1748\n",
      "Iteration: 134 of 241\ttrain_loss: 2.9863\n",
      "Iteration: 136 of 241\ttrain_loss: 2.5952\n",
      "Iteration: 138 of 241\ttrain_loss: 2.7849\n",
      "Iteration: 140 of 241\ttrain_loss: 2.8845\n",
      "Iteration: 142 of 241\ttrain_loss: 2.6426\n",
      "Iteration: 144 of 241\ttrain_loss: 3.2095\n",
      "Iteration: 146 of 241\ttrain_loss: 2.9332\n",
      "Iteration: 148 of 241\ttrain_loss: 2.8364\n",
      "Iteration: 150 of 241\ttrain_loss: 3.1536\n",
      "Iteration: 152 of 241\ttrain_loss: 2.7253\n",
      "Iteration: 154 of 241\ttrain_loss: 2.9086\n",
      "Iteration: 156 of 241\ttrain_loss: 3.1350\n",
      "Iteration: 158 of 241\ttrain_loss: 3.0119\n",
      "Iteration: 160 of 241\ttrain_loss: 2.6541\n",
      "Iteration: 162 of 241\ttrain_loss: 2.7792\n",
      "Iteration: 164 of 241\ttrain_loss: 2.9193\n",
      "Iteration: 166 of 241\ttrain_loss: 2.9107\n",
      "Iteration: 168 of 241\ttrain_loss: 2.7210\n",
      "Iteration: 170 of 241\ttrain_loss: 3.4070\n",
      "Iteration: 172 of 241\ttrain_loss: 3.2666\n",
      "Iteration: 174 of 241\ttrain_loss: 2.8357\n",
      "Iteration: 176 of 241\ttrain_loss: 2.9059\n",
      "Iteration: 178 of 241\ttrain_loss: 3.1086\n",
      "Iteration: 180 of 241\ttrain_loss: 2.5877\n",
      "Iteration: 182 of 241\ttrain_loss: 3.3595\n",
      "Iteration: 184 of 241\ttrain_loss: 3.0922\n",
      "Iteration: 186 of 241\ttrain_loss: 3.0180\n",
      "Iteration: 188 of 241\ttrain_loss: 3.0134\n",
      "Iteration: 190 of 241\ttrain_loss: 3.0367\n",
      "Iteration: 192 of 241\ttrain_loss: 3.4226\n",
      "Iteration: 194 of 241\ttrain_loss: 2.9258\n",
      "Iteration: 196 of 241\ttrain_loss: 2.9102\n",
      "Iteration: 198 of 241\ttrain_loss: 2.8005\n",
      "Iteration: 200 of 241\ttrain_loss: 3.3306\n",
      "Iteration: 202 of 241\ttrain_loss: 3.2959\n",
      "Iteration: 204 of 241\ttrain_loss: 2.9676\n",
      "Iteration: 206 of 241\ttrain_loss: 2.6891\n",
      "Iteration: 208 of 241\ttrain_loss: 2.9814\n",
      "Iteration: 210 of 241\ttrain_loss: 3.6944\n",
      "Iteration: 212 of 241\ttrain_loss: 2.8111\n",
      "Iteration: 214 of 241\ttrain_loss: 2.9800\n",
      "Iteration: 216 of 241\ttrain_loss: 2.9461\n",
      "Iteration: 218 of 241\ttrain_loss: 3.1404\n",
      "Iteration: 220 of 241\ttrain_loss: 3.0122\n",
      "Iteration: 222 of 241\ttrain_loss: 2.9422\n",
      "Iteration: 224 of 241\ttrain_loss: 3.3774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 226 of 241\ttrain_loss: 3.1751\n",
      "Iteration: 228 of 241\ttrain_loss: 2.8529\n",
      "Iteration: 230 of 241\ttrain_loss: 2.7315\n",
      "Iteration: 232 of 241\ttrain_loss: 2.8640\n",
      "Iteration: 234 of 241\ttrain_loss: 3.0841\n",
      "Iteration: 236 of 241\ttrain_loss: 2.7728\n",
      "Iteration: 238 of 241\ttrain_loss: 2.9828\n",
      "Iteration: 240 of 241\ttrain_loss: 2.9949\n",
      "Iteration: 241 of 241\ttrain_loss: 3.0892\n",
      "Average Score for this Epoch: 2.8892598152160645\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 47 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.7269\n",
      "Iteration: 2 of 241\ttrain_loss: 2.8176\n",
      "Iteration: 4 of 241\ttrain_loss: 2.9051\n",
      "Iteration: 6 of 241\ttrain_loss: 2.6339\n",
      "Iteration: 8 of 241\ttrain_loss: 2.3807\n",
      "Iteration: 10 of 241\ttrain_loss: 2.5057\n",
      "Iteration: 12 of 241\ttrain_loss: 2.9338\n",
      "Iteration: 14 of 241\ttrain_loss: 2.6459\n",
      "Iteration: 16 of 241\ttrain_loss: 2.6047\n",
      "Iteration: 18 of 241\ttrain_loss: 2.7797\n",
      "Iteration: 20 of 241\ttrain_loss: 2.9016\n",
      "Iteration: 22 of 241\ttrain_loss: 2.7290\n",
      "Iteration: 24 of 241\ttrain_loss: 2.4237\n",
      "Iteration: 26 of 241\ttrain_loss: 2.8664\n",
      "Iteration: 28 of 241\ttrain_loss: 3.2596\n",
      "Iteration: 30 of 241\ttrain_loss: 3.0433\n",
      "Iteration: 32 of 241\ttrain_loss: 2.8468\n",
      "Iteration: 34 of 241\ttrain_loss: 2.5844\n",
      "Iteration: 36 of 241\ttrain_loss: 2.6490\n",
      "Iteration: 38 of 241\ttrain_loss: 2.5359\n",
      "Iteration: 40 of 241\ttrain_loss: 2.5666\n",
      "Iteration: 42 of 241\ttrain_loss: 2.8388\n",
      "Iteration: 44 of 241\ttrain_loss: 2.3472\n",
      "Iteration: 46 of 241\ttrain_loss: 2.6862\n",
      "Iteration: 48 of 241\ttrain_loss: 2.6350\n",
      "Iteration: 50 of 241\ttrain_loss: 2.7438\n",
      "Iteration: 52 of 241\ttrain_loss: 2.5560\n",
      "Iteration: 54 of 241\ttrain_loss: 2.8764\n",
      "Iteration: 56 of 241\ttrain_loss: 2.6812\n",
      "Iteration: 58 of 241\ttrain_loss: 2.5909\n",
      "Iteration: 60 of 241\ttrain_loss: 2.6862\n",
      "Iteration: 62 of 241\ttrain_loss: 2.5794\n",
      "Iteration: 64 of 241\ttrain_loss: 3.0014\n",
      "Iteration: 66 of 241\ttrain_loss: 3.1423\n",
      "Iteration: 68 of 241\ttrain_loss: 2.4343\n",
      "Iteration: 70 of 241\ttrain_loss: 2.4927\n",
      "Iteration: 72 of 241\ttrain_loss: 2.7093\n",
      "Iteration: 74 of 241\ttrain_loss: 2.6725\n",
      "Iteration: 76 of 241\ttrain_loss: 2.9101\n",
      "Iteration: 78 of 241\ttrain_loss: 2.5358\n",
      "Iteration: 80 of 241\ttrain_loss: 2.9162\n",
      "Iteration: 82 of 241\ttrain_loss: 2.8974\n",
      "Iteration: 84 of 241\ttrain_loss: 2.6484\n",
      "Iteration: 86 of 241\ttrain_loss: 2.5865\n",
      "Iteration: 88 of 241\ttrain_loss: 2.9424\n",
      "Iteration: 90 of 241\ttrain_loss: 2.5328\n",
      "Iteration: 92 of 241\ttrain_loss: 2.9120\n",
      "Iteration: 94 of 241\ttrain_loss: 2.3320\n",
      "Iteration: 96 of 241\ttrain_loss: 2.6196\n",
      "Iteration: 98 of 241\ttrain_loss: 2.4987\n",
      "Iteration: 100 of 241\ttrain_loss: 3.2346\n",
      "Iteration: 102 of 241\ttrain_loss: 3.0680\n",
      "Iteration: 104 of 241\ttrain_loss: 2.8842\n",
      "Iteration: 106 of 241\ttrain_loss: 2.6383\n",
      "Iteration: 108 of 241\ttrain_loss: 3.4335\n",
      "Iteration: 110 of 241\ttrain_loss: 3.0515\n",
      "Iteration: 112 of 241\ttrain_loss: 2.9723\n",
      "Iteration: 114 of 241\ttrain_loss: 2.6161\n",
      "Iteration: 116 of 241\ttrain_loss: 2.5490\n",
      "Iteration: 118 of 241\ttrain_loss: 2.4894\n",
      "Iteration: 120 of 241\ttrain_loss: 2.8729\n",
      "Iteration: 122 of 241\ttrain_loss: 2.6236\n",
      "Iteration: 124 of 241\ttrain_loss: 2.7932\n",
      "Iteration: 126 of 241\ttrain_loss: 2.7370\n",
      "Iteration: 128 of 241\ttrain_loss: 3.1556\n",
      "Iteration: 130 of 241\ttrain_loss: 2.6929\n",
      "Iteration: 132 of 241\ttrain_loss: 2.5610\n",
      "Iteration: 134 of 241\ttrain_loss: 2.9405\n",
      "Iteration: 136 of 241\ttrain_loss: 2.7727\n",
      "Iteration: 138 of 241\ttrain_loss: 2.9102\n",
      "Iteration: 140 of 241\ttrain_loss: 3.0129\n",
      "Iteration: 142 of 241\ttrain_loss: 2.4033\n",
      "Iteration: 144 of 241\ttrain_loss: 3.0538\n",
      "Iteration: 146 of 241\ttrain_loss: 3.4076\n",
      "Iteration: 148 of 241\ttrain_loss: 2.5501\n",
      "Iteration: 150 of 241\ttrain_loss: 2.9390\n",
      "Iteration: 152 of 241\ttrain_loss: 2.7837\n",
      "Iteration: 154 of 241\ttrain_loss: 2.9846\n",
      "Iteration: 156 of 241\ttrain_loss: 3.0090\n",
      "Iteration: 158 of 241\ttrain_loss: 3.2118\n",
      "Iteration: 160 of 241\ttrain_loss: 3.0622\n",
      "Iteration: 162 of 241\ttrain_loss: 3.0157\n",
      "Iteration: 164 of 241\ttrain_loss: 2.7473\n",
      "Iteration: 166 of 241\ttrain_loss: 3.0506\n",
      "Iteration: 168 of 241\ttrain_loss: 2.8519\n",
      "Iteration: 170 of 241\ttrain_loss: 2.9989\n",
      "Iteration: 172 of 241\ttrain_loss: 2.6714\n",
      "Iteration: 174 of 241\ttrain_loss: 2.6999\n",
      "Iteration: 176 of 241\ttrain_loss: 2.9562\n",
      "Iteration: 178 of 241\ttrain_loss: 2.5242\n",
      "Iteration: 180 of 241\ttrain_loss: 2.9529\n",
      "Iteration: 182 of 241\ttrain_loss: 3.0311\n",
      "Iteration: 184 of 241\ttrain_loss: 2.8636\n",
      "Iteration: 186 of 241\ttrain_loss: 3.1584\n",
      "Iteration: 188 of 241\ttrain_loss: 2.8838\n",
      "Iteration: 190 of 241\ttrain_loss: 2.8820\n",
      "Iteration: 192 of 241\ttrain_loss: 2.8742\n",
      "Iteration: 194 of 241\ttrain_loss: 2.6443\n",
      "Iteration: 196 of 241\ttrain_loss: 3.1358\n",
      "Iteration: 198 of 241\ttrain_loss: 2.6439\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6894\n",
      "Iteration: 202 of 241\ttrain_loss: 2.9762\n",
      "Iteration: 204 of 241\ttrain_loss: 2.7790\n",
      "Iteration: 206 of 241\ttrain_loss: 3.1257\n",
      "Iteration: 208 of 241\ttrain_loss: 3.2428\n",
      "Iteration: 210 of 241\ttrain_loss: 3.0820\n",
      "Iteration: 212 of 241\ttrain_loss: 2.6094\n",
      "Iteration: 214 of 241\ttrain_loss: 2.8114\n",
      "Iteration: 216 of 241\ttrain_loss: 2.8421\n",
      "Iteration: 218 of 241\ttrain_loss: 3.0825\n",
      "Iteration: 220 of 241\ttrain_loss: 2.9079\n",
      "Iteration: 222 of 241\ttrain_loss: 2.8808\n",
      "Iteration: 224 of 241\ttrain_loss: 3.0005\n",
      "Iteration: 226 of 241\ttrain_loss: 3.1087\n",
      "Iteration: 228 of 241\ttrain_loss: 3.0385\n",
      "Iteration: 230 of 241\ttrain_loss: 2.5580\n",
      "Iteration: 232 of 241\ttrain_loss: 2.9633\n",
      "Iteration: 234 of 241\ttrain_loss: 3.2443\n",
      "Iteration: 236 of 241\ttrain_loss: 2.9364\n",
      "Iteration: 238 of 241\ttrain_loss: 3.1252\n",
      "Iteration: 240 of 241\ttrain_loss: 2.8343\n",
      "Iteration: 241 of 241\ttrain_loss: 2.9261\n",
      "Average Score for this Epoch: 2.836177349090576\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 48 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.4453\n",
      "Iteration: 2 of 241\ttrain_loss: 2.5011\n",
      "Iteration: 4 of 241\ttrain_loss: 2.3114\n",
      "Iteration: 6 of 241\ttrain_loss: 2.4902\n",
      "Iteration: 8 of 241\ttrain_loss: 2.9765\n",
      "Iteration: 10 of 241\ttrain_loss: 2.5748\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7638\n",
      "Iteration: 14 of 241\ttrain_loss: 2.6440\n",
      "Iteration: 16 of 241\ttrain_loss: 2.6120\n",
      "Iteration: 18 of 241\ttrain_loss: 2.6394\n",
      "Iteration: 20 of 241\ttrain_loss: 2.4154\n",
      "Iteration: 22 of 241\ttrain_loss: 3.2258\n",
      "Iteration: 24 of 241\ttrain_loss: 2.6866\n",
      "Iteration: 26 of 241\ttrain_loss: 2.8447\n",
      "Iteration: 28 of 241\ttrain_loss: 2.4514\n",
      "Iteration: 30 of 241\ttrain_loss: 2.4598\n",
      "Iteration: 32 of 241\ttrain_loss: 2.7844\n",
      "Iteration: 34 of 241\ttrain_loss: 3.0491\n",
      "Iteration: 36 of 241\ttrain_loss: 2.4362\n",
      "Iteration: 38 of 241\ttrain_loss: 2.6674\n",
      "Iteration: 40 of 241\ttrain_loss: 2.5305\n",
      "Iteration: 42 of 241\ttrain_loss: 2.6859\n",
      "Iteration: 44 of 241\ttrain_loss: 3.2095\n",
      "Iteration: 46 of 241\ttrain_loss: 2.8822\n",
      "Iteration: 48 of 241\ttrain_loss: 2.7319\n",
      "Iteration: 50 of 241\ttrain_loss: 2.7456\n",
      "Iteration: 52 of 241\ttrain_loss: 2.6583\n",
      "Iteration: 54 of 241\ttrain_loss: 2.4579\n",
      "Iteration: 56 of 241\ttrain_loss: 2.9206\n",
      "Iteration: 58 of 241\ttrain_loss: 2.9676\n",
      "Iteration: 60 of 241\ttrain_loss: 2.7650\n",
      "Iteration: 62 of 241\ttrain_loss: 2.6311\n",
      "Iteration: 64 of 241\ttrain_loss: 2.2933\n",
      "Iteration: 66 of 241\ttrain_loss: 2.6439\n",
      "Iteration: 68 of 241\ttrain_loss: 2.9355\n",
      "Iteration: 70 of 241\ttrain_loss: 2.9328\n",
      "Iteration: 72 of 241\ttrain_loss: 2.7892\n",
      "Iteration: 74 of 241\ttrain_loss: 2.5091\n",
      "Iteration: 76 of 241\ttrain_loss: 2.6379\n",
      "Iteration: 78 of 241\ttrain_loss: 2.7297\n",
      "Iteration: 80 of 241\ttrain_loss: 2.9426\n",
      "Iteration: 82 of 241\ttrain_loss: 2.6462\n",
      "Iteration: 84 of 241\ttrain_loss: 2.7354\n",
      "Iteration: 86 of 241\ttrain_loss: 2.8364\n",
      "Iteration: 88 of 241\ttrain_loss: 2.8778\n",
      "Iteration: 90 of 241\ttrain_loss: 3.0911\n",
      "Iteration: 92 of 241\ttrain_loss: 3.0252\n",
      "Iteration: 94 of 241\ttrain_loss: 2.5587\n",
      "Iteration: 96 of 241\ttrain_loss: 2.5618\n",
      "Iteration: 98 of 241\ttrain_loss: 2.7735\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4151\n",
      "Iteration: 102 of 241\ttrain_loss: 2.9462\n",
      "Iteration: 104 of 241\ttrain_loss: 2.6915\n",
      "Iteration: 106 of 241\ttrain_loss: 2.8084\n",
      "Iteration: 108 of 241\ttrain_loss: 2.7420\n",
      "Iteration: 110 of 241\ttrain_loss: 2.8009\n",
      "Iteration: 112 of 241\ttrain_loss: 2.6930\n",
      "Iteration: 114 of 241\ttrain_loss: 2.6180\n",
      "Iteration: 116 of 241\ttrain_loss: 2.8625\n",
      "Iteration: 118 of 241\ttrain_loss: 2.7557\n",
      "Iteration: 120 of 241\ttrain_loss: 2.7495\n",
      "Iteration: 122 of 241\ttrain_loss: 2.9026\n",
      "Iteration: 124 of 241\ttrain_loss: 2.7326\n",
      "Iteration: 126 of 241\ttrain_loss: 2.6024\n",
      "Iteration: 128 of 241\ttrain_loss: 2.8864\n",
      "Iteration: 130 of 241\ttrain_loss: 3.5299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 132 of 241\ttrain_loss: 2.6846\n",
      "Iteration: 134 of 241\ttrain_loss: 3.5973\n",
      "Iteration: 136 of 241\ttrain_loss: 2.9461\n",
      "Iteration: 138 of 241\ttrain_loss: 2.8466\n",
      "Iteration: 140 of 241\ttrain_loss: 2.4777\n",
      "Iteration: 142 of 241\ttrain_loss: 2.6205\n",
      "Iteration: 144 of 241\ttrain_loss: 2.9693\n",
      "Iteration: 146 of 241\ttrain_loss: 2.7774\n",
      "Iteration: 148 of 241\ttrain_loss: 2.5236\n",
      "Iteration: 150 of 241\ttrain_loss: 2.8225\n",
      "Iteration: 152 of 241\ttrain_loss: 2.4399\n",
      "Iteration: 154 of 241\ttrain_loss: 2.5573\n",
      "Iteration: 156 of 241\ttrain_loss: 2.8174\n",
      "Iteration: 158 of 241\ttrain_loss: 2.5977\n",
      "Iteration: 160 of 241\ttrain_loss: 2.3388\n",
      "Iteration: 162 of 241\ttrain_loss: 3.1071\n",
      "Iteration: 164 of 241\ttrain_loss: 3.0845\n",
      "Iteration: 166 of 241\ttrain_loss: 3.1317\n",
      "Iteration: 168 of 241\ttrain_loss: 3.0886\n",
      "Iteration: 170 of 241\ttrain_loss: 2.4851\n",
      "Iteration: 172 of 241\ttrain_loss: 3.0093\n",
      "Iteration: 174 of 241\ttrain_loss: 2.9245\n",
      "Iteration: 176 of 241\ttrain_loss: 2.9274\n",
      "Iteration: 178 of 241\ttrain_loss: 3.0179\n",
      "Iteration: 180 of 241\ttrain_loss: 2.7996\n",
      "Iteration: 182 of 241\ttrain_loss: 2.9053\n",
      "Iteration: 184 of 241\ttrain_loss: 2.7010\n",
      "Iteration: 186 of 241\ttrain_loss: 2.5677\n",
      "Iteration: 188 of 241\ttrain_loss: 2.6299\n",
      "Iteration: 190 of 241\ttrain_loss: 3.0118\n",
      "Iteration: 192 of 241\ttrain_loss: 2.8768\n",
      "Iteration: 194 of 241\ttrain_loss: 2.4373\n",
      "Iteration: 196 of 241\ttrain_loss: 3.1267\n",
      "Iteration: 198 of 241\ttrain_loss: 2.5005\n",
      "Iteration: 200 of 241\ttrain_loss: 2.5775\n",
      "Iteration: 202 of 241\ttrain_loss: 2.7277\n",
      "Iteration: 204 of 241\ttrain_loss: 2.6855\n",
      "Iteration: 206 of 241\ttrain_loss: 3.0967\n",
      "Iteration: 208 of 241\ttrain_loss: 3.0174\n",
      "Iteration: 210 of 241\ttrain_loss: 3.4517\n",
      "Iteration: 212 of 241\ttrain_loss: 2.4059\n",
      "Iteration: 214 of 241\ttrain_loss: 2.4148\n",
      "Iteration: 216 of 241\ttrain_loss: 2.8114\n",
      "Iteration: 218 of 241\ttrain_loss: 3.1866\n",
      "Iteration: 220 of 241\ttrain_loss: 2.7259\n",
      "Iteration: 222 of 241\ttrain_loss: 2.6039\n",
      "Iteration: 224 of 241\ttrain_loss: 3.1200\n",
      "Iteration: 226 of 241\ttrain_loss: 3.0871\n",
      "Iteration: 228 of 241\ttrain_loss: 3.0343\n",
      "Iteration: 230 of 241\ttrain_loss: 2.7490\n",
      "Iteration: 232 of 241\ttrain_loss: 2.7744\n",
      "Iteration: 234 of 241\ttrain_loss: 2.5925\n",
      "Iteration: 236 of 241\ttrain_loss: 2.8305\n",
      "Iteration: 238 of 241\ttrain_loss: 2.7413\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3480\n",
      "Iteration: 241 of 241\ttrain_loss: 3.4599\n",
      "Average Score for this Epoch: 2.7980475425720215\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 49 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.1748\n",
      "Iteration: 2 of 241\ttrain_loss: 3.1204\n",
      "Iteration: 4 of 241\ttrain_loss: 2.7656\n",
      "Iteration: 6 of 241\ttrain_loss: 2.4546\n",
      "Iteration: 8 of 241\ttrain_loss: 2.2952\n",
      "Iteration: 10 of 241\ttrain_loss: 2.8633\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7589\n",
      "Iteration: 14 of 241\ttrain_loss: 2.7177\n",
      "Iteration: 16 of 241\ttrain_loss: 2.4904\n",
      "Iteration: 18 of 241\ttrain_loss: 3.1253\n",
      "Iteration: 20 of 241\ttrain_loss: 2.3716\n",
      "Iteration: 22 of 241\ttrain_loss: 2.3983\n",
      "Iteration: 24 of 241\ttrain_loss: 2.7435\n",
      "Iteration: 26 of 241\ttrain_loss: 2.7646\n",
      "Iteration: 28 of 241\ttrain_loss: 3.0122\n",
      "Iteration: 30 of 241\ttrain_loss: 2.4736\n",
      "Iteration: 32 of 241\ttrain_loss: 2.3421\n",
      "Iteration: 34 of 241\ttrain_loss: 2.5002\n",
      "Iteration: 36 of 241\ttrain_loss: 2.8054\n",
      "Iteration: 38 of 241\ttrain_loss: 2.3618\n",
      "Iteration: 40 of 241\ttrain_loss: 2.4561\n",
      "Iteration: 42 of 241\ttrain_loss: 2.2992\n",
      "Iteration: 44 of 241\ttrain_loss: 2.8489\n",
      "Iteration: 46 of 241\ttrain_loss: 2.7198\n",
      "Iteration: 48 of 241\ttrain_loss: 2.8960\n",
      "Iteration: 50 of 241\ttrain_loss: 2.5911\n",
      "Iteration: 52 of 241\ttrain_loss: 2.6723\n",
      "Iteration: 54 of 241\ttrain_loss: 2.4953\n",
      "Iteration: 56 of 241\ttrain_loss: 2.4402\n",
      "Iteration: 58 of 241\ttrain_loss: 2.5150\n",
      "Iteration: 60 of 241\ttrain_loss: 2.3804\n",
      "Iteration: 62 of 241\ttrain_loss: 2.4646\n",
      "Iteration: 64 of 241\ttrain_loss: 2.6660\n",
      "Iteration: 66 of 241\ttrain_loss: 3.0311\n",
      "Iteration: 68 of 241\ttrain_loss: 2.7165\n",
      "Iteration: 70 of 241\ttrain_loss: 2.6986\n",
      "Iteration: 72 of 241\ttrain_loss: 2.5817\n",
      "Iteration: 74 of 241\ttrain_loss: 2.6053\n",
      "Iteration: 76 of 241\ttrain_loss: 2.6163\n",
      "Iteration: 78 of 241\ttrain_loss: 2.8152\n",
      "Iteration: 80 of 241\ttrain_loss: 2.4583\n",
      "Iteration: 82 of 241\ttrain_loss: 3.1561\n",
      "Iteration: 84 of 241\ttrain_loss: 2.8374\n",
      "Iteration: 86 of 241\ttrain_loss: 2.9901\n",
      "Iteration: 88 of 241\ttrain_loss: 2.9125\n",
      "Iteration: 90 of 241\ttrain_loss: 2.9888\n",
      "Iteration: 92 of 241\ttrain_loss: 2.5728\n",
      "Iteration: 94 of 241\ttrain_loss: 2.8355\n",
      "Iteration: 96 of 241\ttrain_loss: 2.5909\n",
      "Iteration: 98 of 241\ttrain_loss: 2.7138\n",
      "Iteration: 100 of 241\ttrain_loss: 2.6856\n",
      "Iteration: 102 of 241\ttrain_loss: 2.5567\n",
      "Iteration: 104 of 241\ttrain_loss: 2.7412\n",
      "Iteration: 106 of 241\ttrain_loss: 2.8555\n",
      "Iteration: 108 of 241\ttrain_loss: 3.0405\n",
      "Iteration: 110 of 241\ttrain_loss: 3.1208\n",
      "Iteration: 112 of 241\ttrain_loss: 2.7983\n",
      "Iteration: 114 of 241\ttrain_loss: 2.7204\n",
      "Iteration: 116 of 241\ttrain_loss: 2.5688\n",
      "Iteration: 118 of 241\ttrain_loss: 2.3585\n",
      "Iteration: 120 of 241\ttrain_loss: 2.7405\n",
      "Iteration: 122 of 241\ttrain_loss: 2.9555\n",
      "Iteration: 124 of 241\ttrain_loss: 2.8316\n",
      "Iteration: 126 of 241\ttrain_loss: 2.7591\n",
      "Iteration: 128 of 241\ttrain_loss: 2.7558\n",
      "Iteration: 130 of 241\ttrain_loss: 2.9478\n",
      "Iteration: 132 of 241\ttrain_loss: 2.6192\n",
      "Iteration: 134 of 241\ttrain_loss: 2.4140\n",
      "Iteration: 136 of 241\ttrain_loss: 2.8565\n",
      "Iteration: 138 of 241\ttrain_loss: 2.7036\n",
      "Iteration: 140 of 241\ttrain_loss: 2.6533\n",
      "Iteration: 142 of 241\ttrain_loss: 2.8248\n",
      "Iteration: 144 of 241\ttrain_loss: 3.0173\n",
      "Iteration: 146 of 241\ttrain_loss: 2.8999\n",
      "Iteration: 148 of 241\ttrain_loss: 3.1013\n",
      "Iteration: 150 of 241\ttrain_loss: 2.6437\n",
      "Iteration: 152 of 241\ttrain_loss: 3.1448\n",
      "Iteration: 154 of 241\ttrain_loss: 2.6986\n",
      "Iteration: 156 of 241\ttrain_loss: 2.9403\n",
      "Iteration: 158 of 241\ttrain_loss: 2.5600\n",
      "Iteration: 160 of 241\ttrain_loss: 2.4811\n",
      "Iteration: 162 of 241\ttrain_loss: 2.7235\n",
      "Iteration: 164 of 241\ttrain_loss: 2.7681\n",
      "Iteration: 166 of 241\ttrain_loss: 2.9673\n",
      "Iteration: 168 of 241\ttrain_loss: 2.6138\n",
      "Iteration: 170 of 241\ttrain_loss: 2.6021\n",
      "Iteration: 172 of 241\ttrain_loss: 3.1740\n",
      "Iteration: 174 of 241\ttrain_loss: 2.3898\n",
      "Iteration: 176 of 241\ttrain_loss: 2.8494\n",
      "Iteration: 178 of 241\ttrain_loss: 2.5211\n",
      "Iteration: 180 of 241\ttrain_loss: 2.7532\n",
      "Iteration: 182 of 241\ttrain_loss: 2.8896\n",
      "Iteration: 184 of 241\ttrain_loss: 2.6403\n",
      "Iteration: 186 of 241\ttrain_loss: 2.6788\n",
      "Iteration: 188 of 241\ttrain_loss: 2.8239\n",
      "Iteration: 190 of 241\ttrain_loss: 2.6697\n",
      "Iteration: 192 of 241\ttrain_loss: 2.9103\n",
      "Iteration: 194 of 241\ttrain_loss: 2.6622\n",
      "Iteration: 196 of 241\ttrain_loss: 3.3521\n",
      "Iteration: 198 of 241\ttrain_loss: 2.5275\n",
      "Iteration: 200 of 241\ttrain_loss: 2.4257\n",
      "Iteration: 202 of 241\ttrain_loss: 3.0385\n",
      "Iteration: 204 of 241\ttrain_loss: 2.9062\n",
      "Iteration: 206 of 241\ttrain_loss: 2.9635\n",
      "Iteration: 208 of 241\ttrain_loss: 2.8594\n",
      "Iteration: 210 of 241\ttrain_loss: 2.7956\n",
      "Iteration: 212 of 241\ttrain_loss: 2.7218\n",
      "Iteration: 214 of 241\ttrain_loss: 2.8234\n",
      "Iteration: 216 of 241\ttrain_loss: 2.9065\n",
      "Iteration: 218 of 241\ttrain_loss: 3.0771\n",
      "Iteration: 220 of 241\ttrain_loss: 2.8780\n",
      "Iteration: 222 of 241\ttrain_loss: 2.3662\n",
      "Iteration: 224 of 241\ttrain_loss: 2.6595\n",
      "Iteration: 226 of 241\ttrain_loss: 2.8426\n",
      "Iteration: 228 of 241\ttrain_loss: 2.9839\n",
      "Iteration: 230 of 241\ttrain_loss: 2.2368\n",
      "Iteration: 232 of 241\ttrain_loss: 2.8968\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2813\n",
      "Iteration: 236 of 241\ttrain_loss: 2.5277\n",
      "Iteration: 238 of 241\ttrain_loss: 2.8155\n",
      "Iteration: 240 of 241\ttrain_loss: 2.4199\n",
      "Iteration: 241 of 241\ttrain_loss: 2.8859\n",
      "Average Score for this Epoch: 2.7397139072418213\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 50 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.3039\n",
      "Iteration: 2 of 241\ttrain_loss: 2.5937\n",
      "Iteration: 4 of 241\ttrain_loss: 2.4958\n",
      "Iteration: 6 of 241\ttrain_loss: 2.5148\n",
      "Iteration: 8 of 241\ttrain_loss: 2.4129\n",
      "Iteration: 10 of 241\ttrain_loss: 2.3658\n",
      "Iteration: 12 of 241\ttrain_loss: 2.7283\n",
      "Iteration: 14 of 241\ttrain_loss: 2.7979\n",
      "Iteration: 16 of 241\ttrain_loss: 2.7012\n",
      "Iteration: 18 of 241\ttrain_loss: 2.6453\n",
      "Iteration: 20 of 241\ttrain_loss: 2.6231\n",
      "Iteration: 22 of 241\ttrain_loss: 2.8617\n",
      "Iteration: 24 of 241\ttrain_loss: 2.4132\n",
      "Iteration: 26 of 241\ttrain_loss: 2.3436\n",
      "Iteration: 28 of 241\ttrain_loss: 2.6547\n",
      "Iteration: 30 of 241\ttrain_loss: 2.4337\n",
      "Iteration: 32 of 241\ttrain_loss: 2.6357\n",
      "Iteration: 34 of 241\ttrain_loss: 2.8217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 36 of 241\ttrain_loss: 2.5512\n",
      "Iteration: 38 of 241\ttrain_loss: 2.7170\n",
      "Iteration: 40 of 241\ttrain_loss: 2.4074\n",
      "Iteration: 42 of 241\ttrain_loss: 2.5047\n",
      "Iteration: 44 of 241\ttrain_loss: 2.6712\n",
      "Iteration: 46 of 241\ttrain_loss: 2.5972\n",
      "Iteration: 48 of 241\ttrain_loss: 2.4408\n",
      "Iteration: 50 of 241\ttrain_loss: 2.6003\n",
      "Iteration: 52 of 241\ttrain_loss: 2.6386\n",
      "Iteration: 54 of 241\ttrain_loss: 2.4374\n",
      "Iteration: 56 of 241\ttrain_loss: 2.8412\n",
      "Iteration: 58 of 241\ttrain_loss: 2.3465\n",
      "Iteration: 60 of 241\ttrain_loss: 2.4378\n",
      "Iteration: 62 of 241\ttrain_loss: 2.9655\n",
      "Iteration: 64 of 241\ttrain_loss: 2.3952\n",
      "Iteration: 66 of 241\ttrain_loss: 2.8280\n",
      "Iteration: 68 of 241\ttrain_loss: 2.3935\n",
      "Iteration: 70 of 241\ttrain_loss: 2.3612\n",
      "Iteration: 72 of 241\ttrain_loss: 2.3483\n",
      "Iteration: 74 of 241\ttrain_loss: 2.9855\n",
      "Iteration: 76 of 241\ttrain_loss: 2.7646\n",
      "Iteration: 78 of 241\ttrain_loss: 2.9787\n",
      "Iteration: 80 of 241\ttrain_loss: 2.5490\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4291\n",
      "Iteration: 84 of 241\ttrain_loss: 2.8094\n",
      "Iteration: 86 of 241\ttrain_loss: 2.5353\n",
      "Iteration: 88 of 241\ttrain_loss: 2.7301\n",
      "Iteration: 90 of 241\ttrain_loss: 2.4451\n",
      "Iteration: 92 of 241\ttrain_loss: 2.7938\n",
      "Iteration: 94 of 241\ttrain_loss: 2.7784\n",
      "Iteration: 96 of 241\ttrain_loss: 2.8134\n",
      "Iteration: 98 of 241\ttrain_loss: 2.7857\n",
      "Iteration: 100 of 241\ttrain_loss: 2.6158\n",
      "Iteration: 102 of 241\ttrain_loss: 2.6177\n",
      "Iteration: 104 of 241\ttrain_loss: 2.6639\n",
      "Iteration: 106 of 241\ttrain_loss: 2.4517\n",
      "Iteration: 108 of 241\ttrain_loss: 2.5838\n",
      "Iteration: 110 of 241\ttrain_loss: 2.7852\n",
      "Iteration: 112 of 241\ttrain_loss: 2.2525\n",
      "Iteration: 114 of 241\ttrain_loss: 2.7554\n",
      "Iteration: 116 of 241\ttrain_loss: 2.8064\n",
      "Iteration: 118 of 241\ttrain_loss: 2.7803\n",
      "Iteration: 120 of 241\ttrain_loss: 2.4853\n",
      "Iteration: 122 of 241\ttrain_loss: 2.5416\n",
      "Iteration: 124 of 241\ttrain_loss: 2.8599\n",
      "Iteration: 126 of 241\ttrain_loss: 2.4228\n",
      "Iteration: 128 of 241\ttrain_loss: 2.3746\n",
      "Iteration: 130 of 241\ttrain_loss: 2.5235\n",
      "Iteration: 132 of 241\ttrain_loss: 2.4840\n",
      "Iteration: 134 of 241\ttrain_loss: 2.3524\n",
      "Iteration: 136 of 241\ttrain_loss: 2.6432\n",
      "Iteration: 138 of 241\ttrain_loss: 2.5180\n",
      "Iteration: 140 of 241\ttrain_loss: 3.0075\n",
      "Iteration: 142 of 241\ttrain_loss: 2.7402\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5366\n",
      "Iteration: 146 of 241\ttrain_loss: 2.8053\n",
      "Iteration: 148 of 241\ttrain_loss: 3.1483\n",
      "Iteration: 150 of 241\ttrain_loss: 2.3822\n",
      "Iteration: 152 of 241\ttrain_loss: 2.2604\n",
      "Iteration: 154 of 241\ttrain_loss: 2.4280\n",
      "Iteration: 156 of 241\ttrain_loss: 2.6509\n",
      "Iteration: 158 of 241\ttrain_loss: 2.2807\n",
      "Iteration: 160 of 241\ttrain_loss: 2.5901\n",
      "Iteration: 162 of 241\ttrain_loss: 2.9489\n",
      "Iteration: 164 of 241\ttrain_loss: 2.7979\n",
      "Iteration: 166 of 241\ttrain_loss: 2.9872\n",
      "Iteration: 168 of 241\ttrain_loss: 2.7830\n",
      "Iteration: 170 of 241\ttrain_loss: 2.6084\n",
      "Iteration: 172 of 241\ttrain_loss: 2.5955\n",
      "Iteration: 174 of 241\ttrain_loss: 2.7911\n",
      "Iteration: 176 of 241\ttrain_loss: 2.8070\n",
      "Iteration: 178 of 241\ttrain_loss: 2.3855\n",
      "Iteration: 180 of 241\ttrain_loss: 2.8378\n",
      "Iteration: 182 of 241\ttrain_loss: 3.3260\n",
      "Iteration: 184 of 241\ttrain_loss: 2.6400\n",
      "Iteration: 186 of 241\ttrain_loss: 2.6026\n",
      "Iteration: 188 of 241\ttrain_loss: 2.8893\n",
      "Iteration: 190 of 241\ttrain_loss: 2.3911\n",
      "Iteration: 192 of 241\ttrain_loss: 2.9828\n",
      "Iteration: 194 of 241\ttrain_loss: 2.7115\n",
      "Iteration: 196 of 241\ttrain_loss: 2.4684\n",
      "Iteration: 198 of 241\ttrain_loss: 2.7917\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6474\n",
      "Iteration: 202 of 241\ttrain_loss: 2.7442\n",
      "Iteration: 204 of 241\ttrain_loss: 2.9101\n",
      "Iteration: 206 of 241\ttrain_loss: 2.4708\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4637\n",
      "Iteration: 210 of 241\ttrain_loss: 2.7317\n",
      "Iteration: 212 of 241\ttrain_loss: 3.2573\n",
      "Iteration: 214 of 241\ttrain_loss: 2.5939\n",
      "Iteration: 216 of 241\ttrain_loss: 3.0095\n",
      "Iteration: 218 of 241\ttrain_loss: 2.6476\n",
      "Iteration: 220 of 241\ttrain_loss: 2.7124\n",
      "Iteration: 222 of 241\ttrain_loss: 2.4163\n",
      "Iteration: 224 of 241\ttrain_loss: 3.1744\n",
      "Iteration: 226 of 241\ttrain_loss: 3.2728\n",
      "Iteration: 228 of 241\ttrain_loss: 2.3826\n",
      "Iteration: 230 of 241\ttrain_loss: 2.6971\n",
      "Iteration: 232 of 241\ttrain_loss: 3.0176\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9366\n",
      "Iteration: 236 of 241\ttrain_loss: 3.0409\n",
      "Iteration: 238 of 241\ttrain_loss: 2.9822\n",
      "Iteration: 240 of 241\ttrain_loss: 2.7618\n",
      "Iteration: 241 of 241\ttrain_loss: 2.7023\n",
      "Average Score for this Epoch: 2.689868450164795\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 51 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.3447\n",
      "Iteration: 2 of 241\ttrain_loss: 2.3267\n",
      "Iteration: 4 of 241\ttrain_loss: 2.1478\n",
      "Iteration: 6 of 241\ttrain_loss: 2.6503\n",
      "Iteration: 8 of 241\ttrain_loss: 2.7261\n",
      "Iteration: 10 of 241\ttrain_loss: 2.2326\n",
      "Iteration: 12 of 241\ttrain_loss: 2.6152\n",
      "Iteration: 14 of 241\ttrain_loss: 2.5264\n",
      "Iteration: 16 of 241\ttrain_loss: 2.4110\n",
      "Iteration: 18 of 241\ttrain_loss: 2.8216\n",
      "Iteration: 20 of 241\ttrain_loss: 2.2574\n",
      "Iteration: 22 of 241\ttrain_loss: 2.9171\n",
      "Iteration: 24 of 241\ttrain_loss: 2.7933\n",
      "Iteration: 26 of 241\ttrain_loss: 2.3939\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1684\n",
      "Iteration: 30 of 241\ttrain_loss: 2.3958\n",
      "Iteration: 32 of 241\ttrain_loss: 2.5477\n",
      "Iteration: 34 of 241\ttrain_loss: 2.8814\n",
      "Iteration: 36 of 241\ttrain_loss: 2.2116\n",
      "Iteration: 38 of 241\ttrain_loss: 2.6049\n",
      "Iteration: 40 of 241\ttrain_loss: 2.5438\n",
      "Iteration: 42 of 241\ttrain_loss: 2.5378\n",
      "Iteration: 44 of 241\ttrain_loss: 2.7420\n",
      "Iteration: 46 of 241\ttrain_loss: 2.7823\n",
      "Iteration: 48 of 241\ttrain_loss: 2.5759\n",
      "Iteration: 50 of 241\ttrain_loss: 2.6104\n",
      "Iteration: 52 of 241\ttrain_loss: 2.9863\n",
      "Iteration: 54 of 241\ttrain_loss: 3.3400\n",
      "Iteration: 56 of 241\ttrain_loss: 2.6576\n",
      "Iteration: 58 of 241\ttrain_loss: 2.5045\n",
      "Iteration: 60 of 241\ttrain_loss: 2.6438\n",
      "Iteration: 62 of 241\ttrain_loss: 2.2798\n",
      "Iteration: 64 of 241\ttrain_loss: 2.3941\n",
      "Iteration: 66 of 241\ttrain_loss: 2.5636\n",
      "Iteration: 68 of 241\ttrain_loss: 2.5478\n",
      "Iteration: 70 of 241\ttrain_loss: 2.4076\n",
      "Iteration: 72 of 241\ttrain_loss: 2.5725\n",
      "Iteration: 74 of 241\ttrain_loss: 2.2930\n",
      "Iteration: 76 of 241\ttrain_loss: 2.4094\n",
      "Iteration: 78 of 241\ttrain_loss: 2.3333\n",
      "Iteration: 80 of 241\ttrain_loss: 2.5120\n",
      "Iteration: 82 of 241\ttrain_loss: 2.3016\n",
      "Iteration: 84 of 241\ttrain_loss: 2.3132\n",
      "Iteration: 86 of 241\ttrain_loss: 2.6322\n",
      "Iteration: 88 of 241\ttrain_loss: 2.6935\n",
      "Iteration: 90 of 241\ttrain_loss: 2.8648\n",
      "Iteration: 92 of 241\ttrain_loss: 2.8702\n",
      "Iteration: 94 of 241\ttrain_loss: 2.9060\n",
      "Iteration: 96 of 241\ttrain_loss: 2.2568\n",
      "Iteration: 98 of 241\ttrain_loss: 2.5196\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4255\n",
      "Iteration: 102 of 241\ttrain_loss: 2.2078\n",
      "Iteration: 104 of 241\ttrain_loss: 2.6876\n",
      "Iteration: 106 of 241\ttrain_loss: 2.7921\n",
      "Iteration: 108 of 241\ttrain_loss: 2.4843\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1247\n",
      "Iteration: 112 of 241\ttrain_loss: 2.7964\n",
      "Iteration: 114 of 241\ttrain_loss: 2.2726\n",
      "Iteration: 116 of 241\ttrain_loss: 2.4114\n",
      "Iteration: 118 of 241\ttrain_loss: 2.7305\n",
      "Iteration: 120 of 241\ttrain_loss: 2.4814\n",
      "Iteration: 122 of 241\ttrain_loss: 2.4438\n",
      "Iteration: 124 of 241\ttrain_loss: 2.6363\n",
      "Iteration: 126 of 241\ttrain_loss: 2.4664\n",
      "Iteration: 128 of 241\ttrain_loss: 2.5245\n",
      "Iteration: 130 of 241\ttrain_loss: 2.7230\n",
      "Iteration: 132 of 241\ttrain_loss: 2.7924\n",
      "Iteration: 134 of 241\ttrain_loss: 2.6326\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3816\n",
      "Iteration: 138 of 241\ttrain_loss: 2.7471\n",
      "Iteration: 140 of 241\ttrain_loss: 2.8279\n",
      "Iteration: 142 of 241\ttrain_loss: 3.0424\n",
      "Iteration: 144 of 241\ttrain_loss: 2.6173\n",
      "Iteration: 146 of 241\ttrain_loss: 2.6113\n",
      "Iteration: 148 of 241\ttrain_loss: 2.5441\n",
      "Iteration: 150 of 241\ttrain_loss: 2.5980\n",
      "Iteration: 152 of 241\ttrain_loss: 2.4385\n",
      "Iteration: 154 of 241\ttrain_loss: 2.8694\n",
      "Iteration: 156 of 241\ttrain_loss: 2.5828\n",
      "Iteration: 158 of 241\ttrain_loss: 2.3116\n",
      "Iteration: 160 of 241\ttrain_loss: 2.8071\n",
      "Iteration: 162 of 241\ttrain_loss: 2.7334\n",
      "Iteration: 164 of 241\ttrain_loss: 2.8719\n",
      "Iteration: 166 of 241\ttrain_loss: 2.7904\n",
      "Iteration: 168 of 241\ttrain_loss: 2.8262\n",
      "Iteration: 170 of 241\ttrain_loss: 2.6950\n",
      "Iteration: 172 of 241\ttrain_loss: 2.6496\n",
      "Iteration: 174 of 241\ttrain_loss: 2.7029\n",
      "Iteration: 176 of 241\ttrain_loss: 2.8066\n",
      "Iteration: 178 of 241\ttrain_loss: 2.3439\n",
      "Iteration: 180 of 241\ttrain_loss: 3.3607\n",
      "Iteration: 182 of 241\ttrain_loss: 2.6125\n",
      "Iteration: 184 of 241\ttrain_loss: 2.6109\n",
      "Iteration: 186 of 241\ttrain_loss: 2.9451\n",
      "Iteration: 188 of 241\ttrain_loss: 2.3922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 190 of 241\ttrain_loss: 2.9475\n",
      "Iteration: 192 of 241\ttrain_loss: 2.6034\n",
      "Iteration: 194 of 241\ttrain_loss: 2.4046\n",
      "Iteration: 196 of 241\ttrain_loss: 2.9092\n",
      "Iteration: 198 of 241\ttrain_loss: 2.9043\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6990\n",
      "Iteration: 202 of 241\ttrain_loss: 2.7348\n",
      "Iteration: 204 of 241\ttrain_loss: 3.0750\n",
      "Iteration: 206 of 241\ttrain_loss: 2.5561\n",
      "Iteration: 208 of 241\ttrain_loss: 2.3233\n",
      "Iteration: 210 of 241\ttrain_loss: 3.1298\n",
      "Iteration: 212 of 241\ttrain_loss: 2.0838\n",
      "Iteration: 214 of 241\ttrain_loss: 2.9246\n",
      "Iteration: 216 of 241\ttrain_loss: 2.9127\n",
      "Iteration: 218 of 241\ttrain_loss: 2.3813\n",
      "Iteration: 220 of 241\ttrain_loss: 2.7564\n",
      "Iteration: 222 of 241\ttrain_loss: 2.7151\n",
      "Iteration: 224 of 241\ttrain_loss: 2.4983\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5453\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1361\n",
      "Iteration: 230 of 241\ttrain_loss: 2.7446\n",
      "Iteration: 232 of 241\ttrain_loss: 2.5387\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9638\n",
      "Iteration: 236 of 241\ttrain_loss: 2.9693\n",
      "Iteration: 238 of 241\ttrain_loss: 2.8835\n",
      "Iteration: 240 of 241\ttrain_loss: 2.6629\n",
      "Iteration: 241 of 241\ttrain_loss: 3.0412\n",
      "Average Score for this Epoch: 2.6092336177825928\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 52 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.6663\n",
      "Iteration: 2 of 241\ttrain_loss: 2.4507\n",
      "Iteration: 4 of 241\ttrain_loss: 2.1987\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9446\n",
      "Iteration: 8 of 241\ttrain_loss: 2.4403\n",
      "Iteration: 10 of 241\ttrain_loss: 2.3656\n",
      "Iteration: 12 of 241\ttrain_loss: 2.4702\n",
      "Iteration: 14 of 241\ttrain_loss: 2.5283\n",
      "Iteration: 16 of 241\ttrain_loss: 2.6923\n",
      "Iteration: 18 of 241\ttrain_loss: 2.4590\n",
      "Iteration: 20 of 241\ttrain_loss: 2.2846\n",
      "Iteration: 22 of 241\ttrain_loss: 2.4033\n",
      "Iteration: 24 of 241\ttrain_loss: 2.1967\n",
      "Iteration: 26 of 241\ttrain_loss: 2.2669\n",
      "Iteration: 28 of 241\ttrain_loss: 2.9119\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9572\n",
      "Iteration: 32 of 241\ttrain_loss: 2.4294\n",
      "Iteration: 34 of 241\ttrain_loss: 2.4843\n",
      "Iteration: 36 of 241\ttrain_loss: 2.3444\n",
      "Iteration: 38 of 241\ttrain_loss: 2.4045\n",
      "Iteration: 40 of 241\ttrain_loss: 2.6157\n",
      "Iteration: 42 of 241\ttrain_loss: 2.1968\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1038\n",
      "Iteration: 46 of 241\ttrain_loss: 2.2388\n",
      "Iteration: 48 of 241\ttrain_loss: 2.3482\n",
      "Iteration: 50 of 241\ttrain_loss: 2.3151\n",
      "Iteration: 52 of 241\ttrain_loss: 2.3370\n",
      "Iteration: 54 of 241\ttrain_loss: 2.1808\n",
      "Iteration: 56 of 241\ttrain_loss: 2.4091\n",
      "Iteration: 58 of 241\ttrain_loss: 2.3790\n",
      "Iteration: 60 of 241\ttrain_loss: 2.9850\n",
      "Iteration: 62 of 241\ttrain_loss: 2.1286\n",
      "Iteration: 64 of 241\ttrain_loss: 2.5730\n",
      "Iteration: 66 of 241\ttrain_loss: 2.2996\n",
      "Iteration: 68 of 241\ttrain_loss: 2.8856\n",
      "Iteration: 70 of 241\ttrain_loss: 2.3805\n",
      "Iteration: 72 of 241\ttrain_loss: 2.4393\n",
      "Iteration: 74 of 241\ttrain_loss: 2.8794\n",
      "Iteration: 76 of 241\ttrain_loss: 2.3940\n",
      "Iteration: 78 of 241\ttrain_loss: 2.3911\n",
      "Iteration: 80 of 241\ttrain_loss: 2.2424\n",
      "Iteration: 82 of 241\ttrain_loss: 2.3149\n",
      "Iteration: 84 of 241\ttrain_loss: 2.2519\n",
      "Iteration: 86 of 241\ttrain_loss: 2.7273\n",
      "Iteration: 88 of 241\ttrain_loss: 2.5936\n",
      "Iteration: 90 of 241\ttrain_loss: 2.3940\n",
      "Iteration: 92 of 241\ttrain_loss: 2.3906\n",
      "Iteration: 94 of 241\ttrain_loss: 2.7857\n",
      "Iteration: 96 of 241\ttrain_loss: 2.5526\n",
      "Iteration: 98 of 241\ttrain_loss: 2.5366\n",
      "Iteration: 100 of 241\ttrain_loss: 2.5624\n",
      "Iteration: 102 of 241\ttrain_loss: 2.7605\n",
      "Iteration: 104 of 241\ttrain_loss: 2.6890\n",
      "Iteration: 106 of 241\ttrain_loss: 2.5883\n",
      "Iteration: 108 of 241\ttrain_loss: 2.3588\n",
      "Iteration: 110 of 241\ttrain_loss: 2.6869\n",
      "Iteration: 112 of 241\ttrain_loss: 2.5607\n",
      "Iteration: 114 of 241\ttrain_loss: 2.7368\n",
      "Iteration: 116 of 241\ttrain_loss: 2.2345\n",
      "Iteration: 118 of 241\ttrain_loss: 2.5322\n",
      "Iteration: 120 of 241\ttrain_loss: 2.8697\n",
      "Iteration: 122 of 241\ttrain_loss: 2.7094\n",
      "Iteration: 124 of 241\ttrain_loss: 3.1352\n",
      "Iteration: 126 of 241\ttrain_loss: 2.3358\n",
      "Iteration: 128 of 241\ttrain_loss: 2.7509\n",
      "Iteration: 130 of 241\ttrain_loss: 2.6680\n",
      "Iteration: 132 of 241\ttrain_loss: 2.3626\n",
      "Iteration: 134 of 241\ttrain_loss: 2.8504\n",
      "Iteration: 136 of 241\ttrain_loss: 2.4635\n",
      "Iteration: 138 of 241\ttrain_loss: 2.3126\n",
      "Iteration: 140 of 241\ttrain_loss: 2.6142\n",
      "Iteration: 142 of 241\ttrain_loss: 2.6409\n",
      "Iteration: 144 of 241\ttrain_loss: 2.2514\n",
      "Iteration: 146 of 241\ttrain_loss: 2.4235\n",
      "Iteration: 148 of 241\ttrain_loss: 2.4849\n",
      "Iteration: 150 of 241\ttrain_loss: 2.7454\n",
      "Iteration: 152 of 241\ttrain_loss: 2.7994\n",
      "Iteration: 154 of 241\ttrain_loss: 2.5850\n",
      "Iteration: 156 of 241\ttrain_loss: 2.3739\n",
      "Iteration: 158 of 241\ttrain_loss: 2.8191\n",
      "Iteration: 160 of 241\ttrain_loss: 2.5694\n",
      "Iteration: 162 of 241\ttrain_loss: 2.4170\n",
      "Iteration: 164 of 241\ttrain_loss: 2.6423\n",
      "Iteration: 166 of 241\ttrain_loss: 2.8896\n",
      "Iteration: 168 of 241\ttrain_loss: 2.6504\n",
      "Iteration: 170 of 241\ttrain_loss: 2.7147\n",
      "Iteration: 172 of 241\ttrain_loss: 2.4315\n",
      "Iteration: 174 of 241\ttrain_loss: 2.7467\n",
      "Iteration: 176 of 241\ttrain_loss: 2.5555\n",
      "Iteration: 178 of 241\ttrain_loss: 2.2819\n",
      "Iteration: 180 of 241\ttrain_loss: 2.3175\n",
      "Iteration: 182 of 241\ttrain_loss: 2.7157\n",
      "Iteration: 184 of 241\ttrain_loss: 2.7048\n",
      "Iteration: 186 of 241\ttrain_loss: 3.2159\n",
      "Iteration: 188 of 241\ttrain_loss: 2.4061\n",
      "Iteration: 190 of 241\ttrain_loss: 2.4692\n",
      "Iteration: 192 of 241\ttrain_loss: 2.7007\n",
      "Iteration: 194 of 241\ttrain_loss: 2.8474\n",
      "Iteration: 196 of 241\ttrain_loss: 3.0132\n",
      "Iteration: 198 of 241\ttrain_loss: 2.6645\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6687\n",
      "Iteration: 202 of 241\ttrain_loss: 2.3006\n",
      "Iteration: 204 of 241\ttrain_loss: 2.8018\n",
      "Iteration: 206 of 241\ttrain_loss: 2.8906\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4167\n",
      "Iteration: 210 of 241\ttrain_loss: 2.4272\n",
      "Iteration: 212 of 241\ttrain_loss: 2.8142\n",
      "Iteration: 214 of 241\ttrain_loss: 3.0063\n",
      "Iteration: 216 of 241\ttrain_loss: 2.5028\n",
      "Iteration: 218 of 241\ttrain_loss: 2.3441\n",
      "Iteration: 220 of 241\ttrain_loss: 2.5883\n",
      "Iteration: 222 of 241\ttrain_loss: 2.7666\n",
      "Iteration: 224 of 241\ttrain_loss: 2.7573\n",
      "Iteration: 226 of 241\ttrain_loss: 2.6936\n",
      "Iteration: 228 of 241\ttrain_loss: 2.6168\n",
      "Iteration: 230 of 241\ttrain_loss: 2.4517\n",
      "Iteration: 232 of 241\ttrain_loss: 3.0393\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9317\n",
      "Iteration: 236 of 241\ttrain_loss: 2.3930\n",
      "Iteration: 238 of 241\ttrain_loss: 2.7203\n",
      "Iteration: 240 of 241\ttrain_loss: 2.4637\n",
      "Iteration: 241 of 241\ttrain_loss: 2.5379\n",
      "Average Score for this Epoch: 2.560699224472046\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 53 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.1430\n",
      "Iteration: 2 of 241\ttrain_loss: 2.5794\n",
      "Iteration: 4 of 241\ttrain_loss: 2.8204\n",
      "Iteration: 6 of 241\ttrain_loss: 2.3570\n",
      "Iteration: 8 of 241\ttrain_loss: 2.3813\n",
      "Iteration: 10 of 241\ttrain_loss: 2.2772\n",
      "Iteration: 12 of 241\ttrain_loss: 2.5507\n",
      "Iteration: 14 of 241\ttrain_loss: 2.1850\n",
      "Iteration: 16 of 241\ttrain_loss: 2.4724\n",
      "Iteration: 18 of 241\ttrain_loss: 2.5347\n",
      "Iteration: 20 of 241\ttrain_loss: 2.4173\n",
      "Iteration: 22 of 241\ttrain_loss: 2.4384\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0525\n",
      "Iteration: 26 of 241\ttrain_loss: 2.3090\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1537\n",
      "Iteration: 30 of 241\ttrain_loss: 2.9306\n",
      "Iteration: 32 of 241\ttrain_loss: 2.2059\n",
      "Iteration: 34 of 241\ttrain_loss: 2.0933\n",
      "Iteration: 36 of 241\ttrain_loss: 2.3524\n",
      "Iteration: 38 of 241\ttrain_loss: 2.6199\n",
      "Iteration: 40 of 241\ttrain_loss: 2.5288\n",
      "Iteration: 42 of 241\ttrain_loss: 2.6528\n",
      "Iteration: 44 of 241\ttrain_loss: 2.7680\n",
      "Iteration: 46 of 241\ttrain_loss: 2.5010\n",
      "Iteration: 48 of 241\ttrain_loss: 2.5699\n",
      "Iteration: 50 of 241\ttrain_loss: 2.5195\n",
      "Iteration: 52 of 241\ttrain_loss: 2.7183\n",
      "Iteration: 54 of 241\ttrain_loss: 2.4530\n",
      "Iteration: 56 of 241\ttrain_loss: 2.7678\n",
      "Iteration: 58 of 241\ttrain_loss: 2.6757\n",
      "Iteration: 60 of 241\ttrain_loss: 2.3901\n",
      "Iteration: 62 of 241\ttrain_loss: 2.4662\n",
      "Iteration: 64 of 241\ttrain_loss: 2.6358\n",
      "Iteration: 66 of 241\ttrain_loss: 2.8659\n",
      "Iteration: 68 of 241\ttrain_loss: 2.2269\n",
      "Iteration: 70 of 241\ttrain_loss: 2.8296\n",
      "Iteration: 72 of 241\ttrain_loss: 2.6917\n",
      "Iteration: 74 of 241\ttrain_loss: 2.4906\n",
      "Iteration: 76 of 241\ttrain_loss: 2.5756\n",
      "Iteration: 78 of 241\ttrain_loss: 2.2278\n",
      "Iteration: 80 of 241\ttrain_loss: 2.6471\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4798\n",
      "Iteration: 84 of 241\ttrain_loss: 2.4130\n",
      "Iteration: 86 of 241\ttrain_loss: 2.6320\n",
      "Iteration: 88 of 241\ttrain_loss: 2.7635\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0542\n",
      "Iteration: 92 of 241\ttrain_loss: 2.5441\n",
      "Iteration: 94 of 241\ttrain_loss: 2.5581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 96 of 241\ttrain_loss: 2.6920\n",
      "Iteration: 98 of 241\ttrain_loss: 3.0138\n",
      "Iteration: 100 of 241\ttrain_loss: 2.5992\n",
      "Iteration: 102 of 241\ttrain_loss: 2.4361\n",
      "Iteration: 104 of 241\ttrain_loss: 2.8557\n",
      "Iteration: 106 of 241\ttrain_loss: 3.0283\n",
      "Iteration: 108 of 241\ttrain_loss: 2.6014\n",
      "Iteration: 110 of 241\ttrain_loss: 3.2833\n",
      "Iteration: 112 of 241\ttrain_loss: 2.3974\n",
      "Iteration: 114 of 241\ttrain_loss: 2.6117\n",
      "Iteration: 116 of 241\ttrain_loss: 2.5981\n",
      "Iteration: 118 of 241\ttrain_loss: 2.6193\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3247\n",
      "Iteration: 122 of 241\ttrain_loss: 2.8079\n",
      "Iteration: 124 of 241\ttrain_loss: 2.6682\n",
      "Iteration: 126 of 241\ttrain_loss: 2.7614\n",
      "Iteration: 128 of 241\ttrain_loss: 2.5549\n",
      "Iteration: 130 of 241\ttrain_loss: 2.2704\n",
      "Iteration: 132 of 241\ttrain_loss: 2.8025\n",
      "Iteration: 134 of 241\ttrain_loss: 3.1271\n",
      "Iteration: 136 of 241\ttrain_loss: 2.6331\n",
      "Iteration: 138 of 241\ttrain_loss: 3.2370\n",
      "Iteration: 140 of 241\ttrain_loss: 2.3891\n",
      "Iteration: 142 of 241\ttrain_loss: 2.6865\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5215\n",
      "Iteration: 146 of 241\ttrain_loss: 2.3739\n",
      "Iteration: 148 of 241\ttrain_loss: 2.4909\n",
      "Iteration: 150 of 241\ttrain_loss: 2.2923\n",
      "Iteration: 152 of 241\ttrain_loss: 2.3462\n",
      "Iteration: 154 of 241\ttrain_loss: 2.7386\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4718\n",
      "Iteration: 158 of 241\ttrain_loss: 2.4182\n",
      "Iteration: 160 of 241\ttrain_loss: 2.4314\n",
      "Iteration: 162 of 241\ttrain_loss: 2.7722\n",
      "Iteration: 164 of 241\ttrain_loss: 2.3854\n",
      "Iteration: 166 of 241\ttrain_loss: 2.3747\n",
      "Iteration: 168 of 241\ttrain_loss: 2.8597\n",
      "Iteration: 170 of 241\ttrain_loss: 2.6838\n",
      "Iteration: 172 of 241\ttrain_loss: 2.5576\n",
      "Iteration: 174 of 241\ttrain_loss: 2.3238\n",
      "Iteration: 176 of 241\ttrain_loss: 2.2376\n",
      "Iteration: 178 of 241\ttrain_loss: 2.0939\n",
      "Iteration: 180 of 241\ttrain_loss: 2.9204\n",
      "Iteration: 182 of 241\ttrain_loss: 2.4168\n",
      "Iteration: 184 of 241\ttrain_loss: 2.8632\n",
      "Iteration: 186 of 241\ttrain_loss: 2.4090\n",
      "Iteration: 188 of 241\ttrain_loss: 2.1167\n",
      "Iteration: 190 of 241\ttrain_loss: 2.5402\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0234\n",
      "Iteration: 194 of 241\ttrain_loss: 2.4953\n",
      "Iteration: 196 of 241\ttrain_loss: 2.9419\n",
      "Iteration: 198 of 241\ttrain_loss: 2.4896\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6968\n",
      "Iteration: 202 of 241\ttrain_loss: 2.7120\n",
      "Iteration: 204 of 241\ttrain_loss: 2.6606\n",
      "Iteration: 206 of 241\ttrain_loss: 2.7378\n",
      "Iteration: 208 of 241\ttrain_loss: 2.5200\n",
      "Iteration: 210 of 241\ttrain_loss: 2.5585\n",
      "Iteration: 212 of 241\ttrain_loss: 2.9063\n",
      "Iteration: 214 of 241\ttrain_loss: 2.6456\n",
      "Iteration: 216 of 241\ttrain_loss: 2.5785\n",
      "Iteration: 218 of 241\ttrain_loss: 2.5926\n",
      "Iteration: 220 of 241\ttrain_loss: 2.5260\n",
      "Iteration: 222 of 241\ttrain_loss: 2.1370\n",
      "Iteration: 224 of 241\ttrain_loss: 2.0910\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5450\n",
      "Iteration: 228 of 241\ttrain_loss: 2.9019\n",
      "Iteration: 230 of 241\ttrain_loss: 2.4895\n",
      "Iteration: 232 of 241\ttrain_loss: 2.8508\n",
      "Iteration: 234 of 241\ttrain_loss: 2.9778\n",
      "Iteration: 236 of 241\ttrain_loss: 2.3949\n",
      "Iteration: 238 of 241\ttrain_loss: 2.8659\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3655\n",
      "Iteration: 241 of 241\ttrain_loss: 2.7742\n",
      "Average Score for this Epoch: 2.5495052337646484\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 54 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.5978\n",
      "Iteration: 2 of 241\ttrain_loss: 2.2813\n",
      "Iteration: 4 of 241\ttrain_loss: 2.5905\n",
      "Iteration: 6 of 241\ttrain_loss: 2.2386\n",
      "Iteration: 8 of 241\ttrain_loss: 3.0038\n",
      "Iteration: 10 of 241\ttrain_loss: 2.5042\n",
      "Iteration: 12 of 241\ttrain_loss: 2.6375\n",
      "Iteration: 14 of 241\ttrain_loss: 2.7075\n",
      "Iteration: 16 of 241\ttrain_loss: 2.3509\n",
      "Iteration: 18 of 241\ttrain_loss: 2.1022\n",
      "Iteration: 20 of 241\ttrain_loss: 2.5614\n",
      "Iteration: 22 of 241\ttrain_loss: 2.6417\n",
      "Iteration: 24 of 241\ttrain_loss: 2.7099\n",
      "Iteration: 26 of 241\ttrain_loss: 2.2151\n",
      "Iteration: 28 of 241\ttrain_loss: 2.6278\n",
      "Iteration: 30 of 241\ttrain_loss: 2.5715\n",
      "Iteration: 32 of 241\ttrain_loss: 2.2596\n",
      "Iteration: 34 of 241\ttrain_loss: 2.3453\n",
      "Iteration: 36 of 241\ttrain_loss: 2.2296\n",
      "Iteration: 38 of 241\ttrain_loss: 2.7514\n",
      "Iteration: 40 of 241\ttrain_loss: 2.5707\n",
      "Iteration: 42 of 241\ttrain_loss: 2.6241\n",
      "Iteration: 44 of 241\ttrain_loss: 2.5779\n",
      "Iteration: 46 of 241\ttrain_loss: 2.5744\n",
      "Iteration: 48 of 241\ttrain_loss: 2.6862\n",
      "Iteration: 50 of 241\ttrain_loss: 2.4691\n",
      "Iteration: 52 of 241\ttrain_loss: 2.2891\n",
      "Iteration: 54 of 241\ttrain_loss: 2.6292\n",
      "Iteration: 56 of 241\ttrain_loss: 2.6641\n",
      "Iteration: 58 of 241\ttrain_loss: 2.3358\n",
      "Iteration: 60 of 241\ttrain_loss: 3.4275\n",
      "Iteration: 62 of 241\ttrain_loss: 2.4730\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4359\n",
      "Iteration: 66 of 241\ttrain_loss: 2.4531\n",
      "Iteration: 68 of 241\ttrain_loss: 2.2872\n",
      "Iteration: 70 of 241\ttrain_loss: 2.3122\n",
      "Iteration: 72 of 241\ttrain_loss: 2.7559\n",
      "Iteration: 74 of 241\ttrain_loss: 2.3428\n",
      "Iteration: 76 of 241\ttrain_loss: 2.7279\n",
      "Iteration: 78 of 241\ttrain_loss: 2.6635\n",
      "Iteration: 80 of 241\ttrain_loss: 2.8360\n",
      "Iteration: 82 of 241\ttrain_loss: 2.1659\n",
      "Iteration: 84 of 241\ttrain_loss: 2.4756\n",
      "Iteration: 86 of 241\ttrain_loss: 2.5993\n",
      "Iteration: 88 of 241\ttrain_loss: 2.7927\n",
      "Iteration: 90 of 241\ttrain_loss: 2.1946\n",
      "Iteration: 92 of 241\ttrain_loss: 2.8885\n",
      "Iteration: 94 of 241\ttrain_loss: 2.6304\n",
      "Iteration: 96 of 241\ttrain_loss: 2.4548\n",
      "Iteration: 98 of 241\ttrain_loss: 2.8715\n",
      "Iteration: 100 of 241\ttrain_loss: 2.3919\n",
      "Iteration: 102 of 241\ttrain_loss: 2.5339\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4183\n",
      "Iteration: 106 of 241\ttrain_loss: 2.2984\n",
      "Iteration: 108 of 241\ttrain_loss: 2.3479\n",
      "Iteration: 110 of 241\ttrain_loss: 2.7407\n",
      "Iteration: 112 of 241\ttrain_loss: 2.2846\n",
      "Iteration: 114 of 241\ttrain_loss: 2.2805\n",
      "Iteration: 116 of 241\ttrain_loss: 2.6470\n",
      "Iteration: 118 of 241\ttrain_loss: 2.4422\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3334\n",
      "Iteration: 122 of 241\ttrain_loss: 2.7184\n",
      "Iteration: 124 of 241\ttrain_loss: 3.0487\n",
      "Iteration: 126 of 241\ttrain_loss: 2.2986\n",
      "Iteration: 128 of 241\ttrain_loss: 2.5824\n",
      "Iteration: 130 of 241\ttrain_loss: 2.5714\n",
      "Iteration: 132 of 241\ttrain_loss: 2.7482\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0139\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3748\n",
      "Iteration: 138 of 241\ttrain_loss: 2.3101\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2740\n",
      "Iteration: 142 of 241\ttrain_loss: 2.7909\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5190\n",
      "Iteration: 146 of 241\ttrain_loss: 2.5278\n",
      "Iteration: 148 of 241\ttrain_loss: 2.8715\n",
      "Iteration: 150 of 241\ttrain_loss: 2.1805\n",
      "Iteration: 152 of 241\ttrain_loss: 2.4757\n",
      "Iteration: 154 of 241\ttrain_loss: 2.3336\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9897\n",
      "Iteration: 158 of 241\ttrain_loss: 1.8200\n",
      "Iteration: 160 of 241\ttrain_loss: 2.1441\n",
      "Iteration: 162 of 241\ttrain_loss: 2.1272\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9837\n",
      "Iteration: 166 of 241\ttrain_loss: 2.4378\n",
      "Iteration: 168 of 241\ttrain_loss: 2.4788\n",
      "Iteration: 170 of 241\ttrain_loss: 2.7504\n",
      "Iteration: 172 of 241\ttrain_loss: 2.6581\n",
      "Iteration: 174 of 241\ttrain_loss: 2.8159\n",
      "Iteration: 176 of 241\ttrain_loss: 2.5776\n",
      "Iteration: 178 of 241\ttrain_loss: 2.3447\n",
      "Iteration: 180 of 241\ttrain_loss: 2.3922\n",
      "Iteration: 182 of 241\ttrain_loss: 2.5598\n",
      "Iteration: 184 of 241\ttrain_loss: 2.2363\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2553\n",
      "Iteration: 188 of 241\ttrain_loss: 2.5535\n",
      "Iteration: 190 of 241\ttrain_loss: 2.3350\n",
      "Iteration: 192 of 241\ttrain_loss: 2.5190\n",
      "Iteration: 194 of 241\ttrain_loss: 2.8167\n",
      "Iteration: 196 of 241\ttrain_loss: 2.5039\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1263\n",
      "Iteration: 200 of 241\ttrain_loss: 2.7923\n",
      "Iteration: 202 of 241\ttrain_loss: 2.2836\n",
      "Iteration: 204 of 241\ttrain_loss: 2.9610\n",
      "Iteration: 206 of 241\ttrain_loss: 2.6467\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4060\n",
      "Iteration: 210 of 241\ttrain_loss: 2.1604\n",
      "Iteration: 212 of 241\ttrain_loss: 2.7222\n",
      "Iteration: 214 of 241\ttrain_loss: 2.4728\n",
      "Iteration: 216 of 241\ttrain_loss: 2.3488\n",
      "Iteration: 218 of 241\ttrain_loss: 2.4746\n",
      "Iteration: 220 of 241\ttrain_loss: 2.6522\n",
      "Iteration: 222 of 241\ttrain_loss: 2.4774\n",
      "Iteration: 224 of 241\ttrain_loss: 2.5076\n",
      "Iteration: 226 of 241\ttrain_loss: 2.6099\n",
      "Iteration: 228 of 241\ttrain_loss: 2.5429\n",
      "Iteration: 230 of 241\ttrain_loss: 2.4790\n",
      "Iteration: 232 of 241\ttrain_loss: 2.5545\n",
      "Iteration: 234 of 241\ttrain_loss: 2.3742\n",
      "Iteration: 236 of 241\ttrain_loss: 2.4150\n",
      "Iteration: 238 of 241\ttrain_loss: 2.5672\n",
      "Iteration: 240 of 241\ttrain_loss: 3.2256\n",
      "Iteration: 241 of 241\ttrain_loss: 2.9258\n",
      "Average Score for this Epoch: 2.50943660736084\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 55 of 100 --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 of 241\ttrain_loss: 2.6656\n",
      "Iteration: 2 of 241\ttrain_loss: 2.5747\n",
      "Iteration: 4 of 241\ttrain_loss: 2.1456\n",
      "Iteration: 6 of 241\ttrain_loss: 2.1108\n",
      "Iteration: 8 of 241\ttrain_loss: 2.4518\n",
      "Iteration: 10 of 241\ttrain_loss: 2.5856\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0786\n",
      "Iteration: 14 of 241\ttrain_loss: 2.3463\n",
      "Iteration: 16 of 241\ttrain_loss: 2.2522\n",
      "Iteration: 18 of 241\ttrain_loss: 2.4648\n",
      "Iteration: 20 of 241\ttrain_loss: 2.7255\n",
      "Iteration: 22 of 241\ttrain_loss: 2.4769\n",
      "Iteration: 24 of 241\ttrain_loss: 2.4056\n",
      "Iteration: 26 of 241\ttrain_loss: 2.4978\n",
      "Iteration: 28 of 241\ttrain_loss: 2.6712\n",
      "Iteration: 30 of 241\ttrain_loss: 2.3590\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0601\n",
      "Iteration: 34 of 241\ttrain_loss: 2.3986\n",
      "Iteration: 36 of 241\ttrain_loss: 2.5860\n",
      "Iteration: 38 of 241\ttrain_loss: 2.4041\n",
      "Iteration: 40 of 241\ttrain_loss: 2.4887\n",
      "Iteration: 42 of 241\ttrain_loss: 2.3723\n",
      "Iteration: 44 of 241\ttrain_loss: 2.5492\n",
      "Iteration: 46 of 241\ttrain_loss: 2.5646\n",
      "Iteration: 48 of 241\ttrain_loss: 2.8564\n",
      "Iteration: 50 of 241\ttrain_loss: 2.3438\n",
      "Iteration: 52 of 241\ttrain_loss: 2.1706\n",
      "Iteration: 54 of 241\ttrain_loss: 2.2781\n",
      "Iteration: 56 of 241\ttrain_loss: 2.3185\n",
      "Iteration: 58 of 241\ttrain_loss: 2.4080\n",
      "Iteration: 60 of 241\ttrain_loss: 2.6172\n",
      "Iteration: 62 of 241\ttrain_loss: 2.2499\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4947\n",
      "Iteration: 66 of 241\ttrain_loss: 2.3593\n",
      "Iteration: 68 of 241\ttrain_loss: 2.5645\n",
      "Iteration: 70 of 241\ttrain_loss: 2.3740\n",
      "Iteration: 72 of 241\ttrain_loss: 2.8412\n",
      "Iteration: 74 of 241\ttrain_loss: 2.0710\n",
      "Iteration: 76 of 241\ttrain_loss: 2.3408\n",
      "Iteration: 78 of 241\ttrain_loss: 2.3122\n",
      "Iteration: 80 of 241\ttrain_loss: 2.1067\n",
      "Iteration: 82 of 241\ttrain_loss: 2.6752\n",
      "Iteration: 84 of 241\ttrain_loss: 2.7893\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2513\n",
      "Iteration: 88 of 241\ttrain_loss: 2.3124\n",
      "Iteration: 90 of 241\ttrain_loss: 2.3448\n",
      "Iteration: 92 of 241\ttrain_loss: 2.2455\n",
      "Iteration: 94 of 241\ttrain_loss: 2.1669\n",
      "Iteration: 96 of 241\ttrain_loss: 2.2821\n",
      "Iteration: 98 of 241\ttrain_loss: 2.4173\n",
      "Iteration: 100 of 241\ttrain_loss: 2.5444\n",
      "Iteration: 102 of 241\ttrain_loss: 2.8882\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4097\n",
      "Iteration: 106 of 241\ttrain_loss: 2.8346\n",
      "Iteration: 108 of 241\ttrain_loss: 2.2899\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1526\n",
      "Iteration: 112 of 241\ttrain_loss: 2.0811\n",
      "Iteration: 114 of 241\ttrain_loss: 2.1302\n",
      "Iteration: 116 of 241\ttrain_loss: 2.1919\n",
      "Iteration: 118 of 241\ttrain_loss: 2.2891\n",
      "Iteration: 120 of 241\ttrain_loss: 2.4551\n",
      "Iteration: 122 of 241\ttrain_loss: 2.0803\n",
      "Iteration: 124 of 241\ttrain_loss: 2.3191\n",
      "Iteration: 126 of 241\ttrain_loss: 2.3963\n",
      "Iteration: 128 of 241\ttrain_loss: 2.7028\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1711\n",
      "Iteration: 132 of 241\ttrain_loss: 2.4275\n",
      "Iteration: 134 of 241\ttrain_loss: 2.2502\n",
      "Iteration: 136 of 241\ttrain_loss: 2.7158\n",
      "Iteration: 138 of 241\ttrain_loss: 2.3503\n",
      "Iteration: 140 of 241\ttrain_loss: 2.4801\n",
      "Iteration: 142 of 241\ttrain_loss: 2.3995\n",
      "Iteration: 144 of 241\ttrain_loss: 2.0994\n",
      "Iteration: 146 of 241\ttrain_loss: 2.2786\n",
      "Iteration: 148 of 241\ttrain_loss: 2.1616\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0730\n",
      "Iteration: 152 of 241\ttrain_loss: 2.0438\n",
      "Iteration: 154 of 241\ttrain_loss: 2.5589\n",
      "Iteration: 156 of 241\ttrain_loss: 3.0293\n",
      "Iteration: 158 of 241\ttrain_loss: 2.1753\n",
      "Iteration: 160 of 241\ttrain_loss: 2.4591\n",
      "Iteration: 162 of 241\ttrain_loss: 2.5413\n",
      "Iteration: 164 of 241\ttrain_loss: 2.4514\n",
      "Iteration: 166 of 241\ttrain_loss: 2.5831\n",
      "Iteration: 168 of 241\ttrain_loss: 2.3679\n",
      "Iteration: 170 of 241\ttrain_loss: 3.0760\n",
      "Iteration: 172 of 241\ttrain_loss: 2.2284\n",
      "Iteration: 174 of 241\ttrain_loss: 2.4263\n",
      "Iteration: 176 of 241\ttrain_loss: 2.8696\n",
      "Iteration: 178 of 241\ttrain_loss: 2.4246\n",
      "Iteration: 180 of 241\ttrain_loss: 3.1032\n",
      "Iteration: 182 of 241\ttrain_loss: 2.3655\n",
      "Iteration: 184 of 241\ttrain_loss: 2.3857\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2243\n",
      "Iteration: 188 of 241\ttrain_loss: 2.3289\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1121\n",
      "Iteration: 192 of 241\ttrain_loss: 2.3294\n",
      "Iteration: 194 of 241\ttrain_loss: 2.5699\n",
      "Iteration: 196 of 241\ttrain_loss: 2.2608\n",
      "Iteration: 198 of 241\ttrain_loss: 2.2435\n",
      "Iteration: 200 of 241\ttrain_loss: 2.4810\n",
      "Iteration: 202 of 241\ttrain_loss: 2.4360\n",
      "Iteration: 204 of 241\ttrain_loss: 2.4778\n",
      "Iteration: 206 of 241\ttrain_loss: 2.4500\n",
      "Iteration: 208 of 241\ttrain_loss: 2.3101\n",
      "Iteration: 210 of 241\ttrain_loss: 2.8223\n",
      "Iteration: 212 of 241\ttrain_loss: 2.1938\n",
      "Iteration: 214 of 241\ttrain_loss: 2.6467\n",
      "Iteration: 216 of 241\ttrain_loss: 2.3335\n",
      "Iteration: 218 of 241\ttrain_loss: 2.8875\n",
      "Iteration: 220 of 241\ttrain_loss: 2.5524\n",
      "Iteration: 222 of 241\ttrain_loss: 2.2414\n",
      "Iteration: 224 of 241\ttrain_loss: 2.6087\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5551\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1986\n",
      "Iteration: 230 of 241\ttrain_loss: 2.6679\n",
      "Iteration: 232 of 241\ttrain_loss: 2.7854\n",
      "Iteration: 234 of 241\ttrain_loss: 2.8250\n",
      "Iteration: 236 of 241\ttrain_loss: 2.5294\n",
      "Iteration: 238 of 241\ttrain_loss: 2.7289\n",
      "Iteration: 240 of 241\ttrain_loss: 2.8146\n",
      "Iteration: 241 of 241\ttrain_loss: 2.6203\n",
      "Average Score for this Epoch: 2.448777198791504\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 56 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.4760\n",
      "Iteration: 2 of 241\ttrain_loss: 2.1739\n",
      "Iteration: 4 of 241\ttrain_loss: 2.7361\n",
      "Iteration: 6 of 241\ttrain_loss: 2.1173\n",
      "Iteration: 8 of 241\ttrain_loss: 2.0976\n",
      "Iteration: 10 of 241\ttrain_loss: 1.9929\n",
      "Iteration: 12 of 241\ttrain_loss: 2.5591\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9700\n",
      "Iteration: 16 of 241\ttrain_loss: 2.2389\n",
      "Iteration: 18 of 241\ttrain_loss: 2.1758\n",
      "Iteration: 20 of 241\ttrain_loss: 2.1616\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1659\n",
      "Iteration: 24 of 241\ttrain_loss: 2.1137\n",
      "Iteration: 26 of 241\ttrain_loss: 2.4425\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1658\n",
      "Iteration: 30 of 241\ttrain_loss: 2.2015\n",
      "Iteration: 32 of 241\ttrain_loss: 2.4855\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1732\n",
      "Iteration: 36 of 241\ttrain_loss: 2.4638\n",
      "Iteration: 38 of 241\ttrain_loss: 2.2241\n",
      "Iteration: 40 of 241\ttrain_loss: 2.1212\n",
      "Iteration: 42 of 241\ttrain_loss: 2.2108\n",
      "Iteration: 44 of 241\ttrain_loss: 1.7080\n",
      "Iteration: 46 of 241\ttrain_loss: 2.2342\n",
      "Iteration: 48 of 241\ttrain_loss: 2.1114\n",
      "Iteration: 50 of 241\ttrain_loss: 2.4608\n",
      "Iteration: 52 of 241\ttrain_loss: 2.1239\n",
      "Iteration: 54 of 241\ttrain_loss: 2.8200\n",
      "Iteration: 56 of 241\ttrain_loss: 2.3216\n",
      "Iteration: 58 of 241\ttrain_loss: 2.4918\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8356\n",
      "Iteration: 62 of 241\ttrain_loss: 2.5600\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4769\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1286\n",
      "Iteration: 68 of 241\ttrain_loss: 2.7847\n",
      "Iteration: 70 of 241\ttrain_loss: 1.8089\n",
      "Iteration: 72 of 241\ttrain_loss: 2.4060\n",
      "Iteration: 74 of 241\ttrain_loss: 2.5876\n",
      "Iteration: 76 of 241\ttrain_loss: 2.5329\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1039\n",
      "Iteration: 80 of 241\ttrain_loss: 2.4328\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4162\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0857\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2937\n",
      "Iteration: 88 of 241\ttrain_loss: 2.3937\n",
      "Iteration: 90 of 241\ttrain_loss: 2.4379\n",
      "Iteration: 92 of 241\ttrain_loss: 2.4392\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9760\n",
      "Iteration: 96 of 241\ttrain_loss: 2.7725\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9881\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4993\n",
      "Iteration: 102 of 241\ttrain_loss: 2.6847\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4339\n",
      "Iteration: 106 of 241\ttrain_loss: 2.4624\n",
      "Iteration: 108 of 241\ttrain_loss: 2.8226\n",
      "Iteration: 110 of 241\ttrain_loss: 2.8076\n",
      "Iteration: 112 of 241\ttrain_loss: 2.5385\n",
      "Iteration: 114 of 241\ttrain_loss: 2.3243\n",
      "Iteration: 116 of 241\ttrain_loss: 2.8718\n",
      "Iteration: 118 of 241\ttrain_loss: 2.0251\n",
      "Iteration: 120 of 241\ttrain_loss: 2.1674\n",
      "Iteration: 122 of 241\ttrain_loss: 2.3813\n",
      "Iteration: 124 of 241\ttrain_loss: 2.3295\n",
      "Iteration: 126 of 241\ttrain_loss: 2.4925\n",
      "Iteration: 128 of 241\ttrain_loss: 2.4641\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1544\n",
      "Iteration: 132 of 241\ttrain_loss: 2.1822\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0244\n",
      "Iteration: 136 of 241\ttrain_loss: 2.1552\n",
      "Iteration: 138 of 241\ttrain_loss: 2.6738\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0622\n",
      "Iteration: 142 of 241\ttrain_loss: 2.3066\n",
      "Iteration: 144 of 241\ttrain_loss: 2.7522\n",
      "Iteration: 146 of 241\ttrain_loss: 2.7363\n",
      "Iteration: 148 of 241\ttrain_loss: 2.5350\n",
      "Iteration: 150 of 241\ttrain_loss: 2.3110\n",
      "Iteration: 152 of 241\ttrain_loss: 2.3911\n",
      "Iteration: 154 of 241\ttrain_loss: 2.8605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 156 of 241\ttrain_loss: 2.3494\n",
      "Iteration: 158 of 241\ttrain_loss: 2.8595\n",
      "Iteration: 160 of 241\ttrain_loss: 2.2939\n",
      "Iteration: 162 of 241\ttrain_loss: 2.2731\n",
      "Iteration: 164 of 241\ttrain_loss: 2.4244\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1197\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2903\n",
      "Iteration: 170 of 241\ttrain_loss: 2.7224\n",
      "Iteration: 172 of 241\ttrain_loss: 2.9260\n",
      "Iteration: 174 of 241\ttrain_loss: 2.4993\n",
      "Iteration: 176 of 241\ttrain_loss: 3.0988\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1520\n",
      "Iteration: 180 of 241\ttrain_loss: 2.4885\n",
      "Iteration: 182 of 241\ttrain_loss: 2.6122\n",
      "Iteration: 184 of 241\ttrain_loss: 2.2494\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2184\n",
      "Iteration: 188 of 241\ttrain_loss: 2.6077\n",
      "Iteration: 190 of 241\ttrain_loss: 2.5341\n",
      "Iteration: 192 of 241\ttrain_loss: 2.5490\n",
      "Iteration: 194 of 241\ttrain_loss: 2.5516\n",
      "Iteration: 196 of 241\ttrain_loss: 2.3346\n",
      "Iteration: 198 of 241\ttrain_loss: 2.8769\n",
      "Iteration: 200 of 241\ttrain_loss: 2.3273\n",
      "Iteration: 202 of 241\ttrain_loss: 2.2364\n",
      "Iteration: 204 of 241\ttrain_loss: 2.3882\n",
      "Iteration: 206 of 241\ttrain_loss: 2.7626\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4718\n",
      "Iteration: 210 of 241\ttrain_loss: 2.4070\n",
      "Iteration: 212 of 241\ttrain_loss: 2.7234\n",
      "Iteration: 214 of 241\ttrain_loss: 2.6427\n",
      "Iteration: 216 of 241\ttrain_loss: 2.8076\n",
      "Iteration: 218 of 241\ttrain_loss: 2.8139\n",
      "Iteration: 220 of 241\ttrain_loss: 2.2831\n",
      "Iteration: 222 of 241\ttrain_loss: 2.2538\n",
      "Iteration: 224 of 241\ttrain_loss: 2.3439\n",
      "Iteration: 226 of 241\ttrain_loss: 2.6286\n",
      "Iteration: 228 of 241\ttrain_loss: 2.3843\n",
      "Iteration: 230 of 241\ttrain_loss: 2.5539\n",
      "Iteration: 232 of 241\ttrain_loss: 2.5514\n",
      "Iteration: 234 of 241\ttrain_loss: 2.6483\n",
      "Iteration: 236 of 241\ttrain_loss: 2.5464\n",
      "Iteration: 238 of 241\ttrain_loss: 2.4675\n",
      "Iteration: 240 of 241\ttrain_loss: 2.6942\n",
      "Iteration: 241 of 241\ttrain_loss: 2.1978\n",
      "Average Score for this Epoch: 2.3986051082611084\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 57 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.2610\n",
      "Iteration: 2 of 241\ttrain_loss: 1.9139\n",
      "Iteration: 4 of 241\ttrain_loss: 2.2087\n",
      "Iteration: 6 of 241\ttrain_loss: 2.2355\n",
      "Iteration: 8 of 241\ttrain_loss: 2.6929\n",
      "Iteration: 10 of 241\ttrain_loss: 2.4459\n",
      "Iteration: 12 of 241\ttrain_loss: 2.3879\n",
      "Iteration: 14 of 241\ttrain_loss: 2.3755\n",
      "Iteration: 16 of 241\ttrain_loss: 2.8828\n",
      "Iteration: 18 of 241\ttrain_loss: 2.2048\n",
      "Iteration: 20 of 241\ttrain_loss: 2.1075\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1930\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0681\n",
      "Iteration: 26 of 241\ttrain_loss: 2.2212\n",
      "Iteration: 28 of 241\ttrain_loss: 2.2525\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9581\n",
      "Iteration: 32 of 241\ttrain_loss: 2.2478\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1560\n",
      "Iteration: 36 of 241\ttrain_loss: 2.1550\n",
      "Iteration: 38 of 241\ttrain_loss: 1.9910\n",
      "Iteration: 40 of 241\ttrain_loss: 2.6343\n",
      "Iteration: 42 of 241\ttrain_loss: 2.4055\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1449\n",
      "Iteration: 46 of 241\ttrain_loss: 2.5396\n",
      "Iteration: 48 of 241\ttrain_loss: 2.2740\n",
      "Iteration: 50 of 241\ttrain_loss: 2.2276\n",
      "Iteration: 52 of 241\ttrain_loss: 2.3313\n",
      "Iteration: 54 of 241\ttrain_loss: 2.2890\n",
      "Iteration: 56 of 241\ttrain_loss: 2.0267\n",
      "Iteration: 58 of 241\ttrain_loss: 2.6551\n",
      "Iteration: 60 of 241\ttrain_loss: 2.7407\n",
      "Iteration: 62 of 241\ttrain_loss: 2.6752\n",
      "Iteration: 64 of 241\ttrain_loss: 2.0060\n",
      "Iteration: 66 of 241\ttrain_loss: 2.4704\n",
      "Iteration: 68 of 241\ttrain_loss: 2.0391\n",
      "Iteration: 70 of 241\ttrain_loss: 2.1239\n",
      "Iteration: 72 of 241\ttrain_loss: 2.1801\n",
      "Iteration: 74 of 241\ttrain_loss: 2.3401\n",
      "Iteration: 76 of 241\ttrain_loss: 2.3340\n",
      "Iteration: 78 of 241\ttrain_loss: 2.4639\n",
      "Iteration: 80 of 241\ttrain_loss: 2.2942\n",
      "Iteration: 82 of 241\ttrain_loss: 2.5179\n",
      "Iteration: 84 of 241\ttrain_loss: 2.6797\n",
      "Iteration: 86 of 241\ttrain_loss: 2.3162\n",
      "Iteration: 88 of 241\ttrain_loss: 2.5274\n",
      "Iteration: 90 of 241\ttrain_loss: 2.5135\n",
      "Iteration: 92 of 241\ttrain_loss: 2.4917\n",
      "Iteration: 94 of 241\ttrain_loss: 2.3767\n",
      "Iteration: 96 of 241\ttrain_loss: 2.3362\n",
      "Iteration: 98 of 241\ttrain_loss: 2.3896\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4947\n",
      "Iteration: 102 of 241\ttrain_loss: 2.4065\n",
      "Iteration: 104 of 241\ttrain_loss: 2.2797\n",
      "Iteration: 106 of 241\ttrain_loss: 2.5760\n",
      "Iteration: 108 of 241\ttrain_loss: 2.6300\n",
      "Iteration: 110 of 241\ttrain_loss: 2.2035\n",
      "Iteration: 112 of 241\ttrain_loss: 2.3426\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0544\n",
      "Iteration: 116 of 241\ttrain_loss: 2.4741\n",
      "Iteration: 118 of 241\ttrain_loss: 2.3334\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3066\n",
      "Iteration: 122 of 241\ttrain_loss: 2.4716\n",
      "Iteration: 124 of 241\ttrain_loss: 2.3304\n",
      "Iteration: 126 of 241\ttrain_loss: 2.4881\n",
      "Iteration: 128 of 241\ttrain_loss: 2.3274\n",
      "Iteration: 130 of 241\ttrain_loss: 2.4238\n",
      "Iteration: 132 of 241\ttrain_loss: 2.3215\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0810\n",
      "Iteration: 136 of 241\ttrain_loss: 2.7850\n",
      "Iteration: 138 of 241\ttrain_loss: 2.9811\n",
      "Iteration: 140 of 241\ttrain_loss: 2.3127\n",
      "Iteration: 142 of 241\ttrain_loss: 2.2866\n",
      "Iteration: 144 of 241\ttrain_loss: 2.1601\n",
      "Iteration: 146 of 241\ttrain_loss: 2.9164\n",
      "Iteration: 148 of 241\ttrain_loss: 2.5232\n",
      "Iteration: 150 of 241\ttrain_loss: 2.4207\n",
      "Iteration: 152 of 241\ttrain_loss: 2.1303\n",
      "Iteration: 154 of 241\ttrain_loss: 2.5906\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9949\n",
      "Iteration: 158 of 241\ttrain_loss: 2.6237\n",
      "Iteration: 160 of 241\ttrain_loss: 2.5144\n",
      "Iteration: 162 of 241\ttrain_loss: 2.5343\n",
      "Iteration: 164 of 241\ttrain_loss: 2.4138\n",
      "Iteration: 166 of 241\ttrain_loss: 2.0968\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2022\n",
      "Iteration: 170 of 241\ttrain_loss: 2.2811\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0084\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0625\n",
      "Iteration: 176 of 241\ttrain_loss: 2.3031\n",
      "Iteration: 178 of 241\ttrain_loss: 2.7860\n",
      "Iteration: 180 of 241\ttrain_loss: 2.4813\n",
      "Iteration: 182 of 241\ttrain_loss: 2.4252\n",
      "Iteration: 184 of 241\ttrain_loss: 2.1585\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2575\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8805\n",
      "Iteration: 190 of 241\ttrain_loss: 2.3367\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1151\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2951\n",
      "Iteration: 196 of 241\ttrain_loss: 2.9250\n",
      "Iteration: 198 of 241\ttrain_loss: 2.4893\n",
      "Iteration: 200 of 241\ttrain_loss: 2.0459\n",
      "Iteration: 202 of 241\ttrain_loss: 2.4470\n",
      "Iteration: 204 of 241\ttrain_loss: 2.1864\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0451\n",
      "Iteration: 208 of 241\ttrain_loss: 2.6592\n",
      "Iteration: 210 of 241\ttrain_loss: 2.5002\n",
      "Iteration: 212 of 241\ttrain_loss: 2.2299\n",
      "Iteration: 214 of 241\ttrain_loss: 2.5470\n",
      "Iteration: 216 of 241\ttrain_loss: 2.6561\n",
      "Iteration: 218 of 241\ttrain_loss: 2.4037\n",
      "Iteration: 220 of 241\ttrain_loss: 2.0866\n",
      "Iteration: 222 of 241\ttrain_loss: 2.6526\n",
      "Iteration: 224 of 241\ttrain_loss: 2.1359\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5309\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2780\n",
      "Iteration: 230 of 241\ttrain_loss: 2.0344\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1032\n",
      "Iteration: 234 of 241\ttrain_loss: 2.3575\n",
      "Iteration: 236 of 241\ttrain_loss: 2.0715\n",
      "Iteration: 238 of 241\ttrain_loss: 2.6735\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3061\n",
      "Iteration: 241 of 241\ttrain_loss: 2.8992\n",
      "Average Score for this Epoch: 2.3777403831481934\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 58 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.6470\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8270\n",
      "Iteration: 4 of 241\ttrain_loss: 2.2982\n",
      "Iteration: 6 of 241\ttrain_loss: 2.0605\n",
      "Iteration: 8 of 241\ttrain_loss: 2.1902\n",
      "Iteration: 10 of 241\ttrain_loss: 2.4433\n",
      "Iteration: 12 of 241\ttrain_loss: 2.3498\n",
      "Iteration: 14 of 241\ttrain_loss: 2.3277\n",
      "Iteration: 16 of 241\ttrain_loss: 2.1709\n",
      "Iteration: 18 of 241\ttrain_loss: 2.8428\n",
      "Iteration: 20 of 241\ttrain_loss: 2.3293\n",
      "Iteration: 22 of 241\ttrain_loss: 2.0464\n",
      "Iteration: 24 of 241\ttrain_loss: 2.2027\n",
      "Iteration: 26 of 241\ttrain_loss: 2.4812\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1797\n",
      "Iteration: 30 of 241\ttrain_loss: 2.5721\n",
      "Iteration: 32 of 241\ttrain_loss: 2.3894\n",
      "Iteration: 34 of 241\ttrain_loss: 2.3082\n",
      "Iteration: 36 of 241\ttrain_loss: 2.4002\n",
      "Iteration: 38 of 241\ttrain_loss: 2.1767\n",
      "Iteration: 40 of 241\ttrain_loss: 2.2839\n",
      "Iteration: 42 of 241\ttrain_loss: 3.0687\n",
      "Iteration: 44 of 241\ttrain_loss: 1.9488\n",
      "Iteration: 46 of 241\ttrain_loss: 2.1225\n",
      "Iteration: 48 of 241\ttrain_loss: 2.5815\n",
      "Iteration: 50 of 241\ttrain_loss: 2.3304\n",
      "Iteration: 52 of 241\ttrain_loss: 1.9704\n",
      "Iteration: 54 of 241\ttrain_loss: 2.1491\n",
      "Iteration: 56 of 241\ttrain_loss: 2.3203\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 60 of 241\ttrain_loss: 2.5534\n",
      "Iteration: 62 of 241\ttrain_loss: 2.2441\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4609\n",
      "Iteration: 66 of 241\ttrain_loss: 2.6361\n",
      "Iteration: 68 of 241\ttrain_loss: 2.5968\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0395\n",
      "Iteration: 72 of 241\ttrain_loss: 2.1269\n",
      "Iteration: 74 of 241\ttrain_loss: 2.3678\n",
      "Iteration: 76 of 241\ttrain_loss: 1.8749\n",
      "Iteration: 78 of 241\ttrain_loss: 2.3103\n",
      "Iteration: 80 of 241\ttrain_loss: 2.3611\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4523\n",
      "Iteration: 84 of 241\ttrain_loss: 2.5548\n",
      "Iteration: 86 of 241\ttrain_loss: 2.4057\n",
      "Iteration: 88 of 241\ttrain_loss: 2.5459\n",
      "Iteration: 90 of 241\ttrain_loss: 1.8678\n",
      "Iteration: 92 of 241\ttrain_loss: 2.6706\n",
      "Iteration: 94 of 241\ttrain_loss: 2.5598\n",
      "Iteration: 96 of 241\ttrain_loss: 2.3100\n",
      "Iteration: 98 of 241\ttrain_loss: 2.1620\n",
      "Iteration: 100 of 241\ttrain_loss: 2.5398\n",
      "Iteration: 102 of 241\ttrain_loss: 2.3057\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4375\n",
      "Iteration: 106 of 241\ttrain_loss: 2.1574\n",
      "Iteration: 108 of 241\ttrain_loss: 2.2394\n",
      "Iteration: 110 of 241\ttrain_loss: 2.2796\n",
      "Iteration: 112 of 241\ttrain_loss: 2.3162\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0738\n",
      "Iteration: 116 of 241\ttrain_loss: 2.3277\n",
      "Iteration: 118 of 241\ttrain_loss: 2.6805\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3390\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2000\n",
      "Iteration: 124 of 241\ttrain_loss: 2.4736\n",
      "Iteration: 126 of 241\ttrain_loss: 2.2206\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9433\n",
      "Iteration: 130 of 241\ttrain_loss: 2.9787\n",
      "Iteration: 132 of 241\ttrain_loss: 2.9173\n",
      "Iteration: 134 of 241\ttrain_loss: 2.2344\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9738\n",
      "Iteration: 138 of 241\ttrain_loss: 2.1747\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2025\n",
      "Iteration: 142 of 241\ttrain_loss: 2.2934\n",
      "Iteration: 144 of 241\ttrain_loss: 2.3707\n",
      "Iteration: 146 of 241\ttrain_loss: 2.7345\n",
      "Iteration: 148 of 241\ttrain_loss: 2.4712\n",
      "Iteration: 150 of 241\ttrain_loss: 2.3301\n",
      "Iteration: 152 of 241\ttrain_loss: 1.9764\n",
      "Iteration: 154 of 241\ttrain_loss: 2.2154\n",
      "Iteration: 156 of 241\ttrain_loss: 2.0259\n",
      "Iteration: 158 of 241\ttrain_loss: 2.1630\n",
      "Iteration: 160 of 241\ttrain_loss: 2.7603\n",
      "Iteration: 162 of 241\ttrain_loss: 2.6597\n",
      "Iteration: 164 of 241\ttrain_loss: 2.6919\n",
      "Iteration: 166 of 241\ttrain_loss: 2.3167\n",
      "Iteration: 168 of 241\ttrain_loss: 2.5309\n",
      "Iteration: 170 of 241\ttrain_loss: 1.9202\n",
      "Iteration: 172 of 241\ttrain_loss: 2.4246\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0350\n",
      "Iteration: 176 of 241\ttrain_loss: 2.2573\n",
      "Iteration: 178 of 241\ttrain_loss: 2.2594\n",
      "Iteration: 180 of 241\ttrain_loss: 2.2612\n",
      "Iteration: 182 of 241\ttrain_loss: 2.3431\n",
      "Iteration: 184 of 241\ttrain_loss: 2.3182\n",
      "Iteration: 186 of 241\ttrain_loss: 2.3673\n",
      "Iteration: 188 of 241\ttrain_loss: 1.9356\n",
      "Iteration: 190 of 241\ttrain_loss: 2.3189\n",
      "Iteration: 192 of 241\ttrain_loss: 2.4744\n",
      "Iteration: 194 of 241\ttrain_loss: 2.7866\n",
      "Iteration: 196 of 241\ttrain_loss: 2.7168\n",
      "Iteration: 198 of 241\ttrain_loss: 2.5045\n",
      "Iteration: 200 of 241\ttrain_loss: 2.4604\n",
      "Iteration: 202 of 241\ttrain_loss: 2.0848\n",
      "Iteration: 204 of 241\ttrain_loss: 2.2569\n",
      "Iteration: 206 of 241\ttrain_loss: 2.4892\n",
      "Iteration: 208 of 241\ttrain_loss: 2.3562\n",
      "Iteration: 210 of 241\ttrain_loss: 2.5409\n",
      "Iteration: 212 of 241\ttrain_loss: 2.2253\n",
      "Iteration: 214 of 241\ttrain_loss: 2.0953\n",
      "Iteration: 216 of 241\ttrain_loss: 2.6018\n",
      "Iteration: 218 of 241\ttrain_loss: 2.7447\n",
      "Iteration: 220 of 241\ttrain_loss: 2.4190\n",
      "Iteration: 222 of 241\ttrain_loss: 2.0785\n",
      "Iteration: 224 of 241\ttrain_loss: 1.9945\n",
      "Iteration: 226 of 241\ttrain_loss: 2.4036\n",
      "Iteration: 228 of 241\ttrain_loss: 2.5685\n",
      "Iteration: 230 of 241\ttrain_loss: 2.7959\n",
      "Iteration: 232 of 241\ttrain_loss: 2.8015\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2445\n",
      "Iteration: 236 of 241\ttrain_loss: 2.4936\n",
      "Iteration: 238 of 241\ttrain_loss: 2.4561\n",
      "Iteration: 240 of 241\ttrain_loss: 2.2958\n",
      "Iteration: 241 of 241\ttrain_loss: 2.0012\n",
      "Average Score for this Epoch: 2.353743553161621\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 59 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 3.3635\n",
      "Iteration: 2 of 241\ttrain_loss: 2.5579\n",
      "Iteration: 4 of 241\ttrain_loss: 2.4695\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9408\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8530\n",
      "Iteration: 10 of 241\ttrain_loss: 1.9838\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0999\n",
      "Iteration: 14 of 241\ttrain_loss: 2.2002\n",
      "Iteration: 16 of 241\ttrain_loss: 2.1715\n",
      "Iteration: 18 of 241\ttrain_loss: 2.4300\n",
      "Iteration: 20 of 241\ttrain_loss: 1.9279\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8632\n",
      "Iteration: 24 of 241\ttrain_loss: 2.4406\n",
      "Iteration: 26 of 241\ttrain_loss: 2.7584\n",
      "Iteration: 28 of 241\ttrain_loss: 1.9697\n",
      "Iteration: 30 of 241\ttrain_loss: 2.2566\n",
      "Iteration: 32 of 241\ttrain_loss: 2.6319\n",
      "Iteration: 34 of 241\ttrain_loss: 1.9715\n",
      "Iteration: 36 of 241\ttrain_loss: 2.5787\n",
      "Iteration: 38 of 241\ttrain_loss: 2.2199\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9579\n",
      "Iteration: 42 of 241\ttrain_loss: 2.0473\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1582\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9702\n",
      "Iteration: 48 of 241\ttrain_loss: 2.3003\n",
      "Iteration: 50 of 241\ttrain_loss: 2.4422\n",
      "Iteration: 52 of 241\ttrain_loss: 2.4111\n",
      "Iteration: 54 of 241\ttrain_loss: 2.2610\n",
      "Iteration: 56 of 241\ttrain_loss: 2.4403\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1355\n",
      "Iteration: 60 of 241\ttrain_loss: 2.1892\n",
      "Iteration: 62 of 241\ttrain_loss: 2.5948\n",
      "Iteration: 64 of 241\ttrain_loss: 2.8197\n",
      "Iteration: 66 of 241\ttrain_loss: 2.3217\n",
      "Iteration: 68 of 241\ttrain_loss: 2.3466\n",
      "Iteration: 70 of 241\ttrain_loss: 2.1345\n",
      "Iteration: 72 of 241\ttrain_loss: 1.9942\n",
      "Iteration: 74 of 241\ttrain_loss: 2.6172\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0950\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1822\n",
      "Iteration: 80 of 241\ttrain_loss: 2.2437\n",
      "Iteration: 82 of 241\ttrain_loss: 2.2388\n",
      "Iteration: 84 of 241\ttrain_loss: 2.2332\n",
      "Iteration: 86 of 241\ttrain_loss: 2.6214\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9126\n",
      "Iteration: 90 of 241\ttrain_loss: 2.2055\n",
      "Iteration: 92 of 241\ttrain_loss: 2.5128\n",
      "Iteration: 94 of 241\ttrain_loss: 2.4635\n",
      "Iteration: 96 of 241\ttrain_loss: 2.5996\n",
      "Iteration: 98 of 241\ttrain_loss: 2.2820\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4627\n",
      "Iteration: 102 of 241\ttrain_loss: 2.3308\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4167\n",
      "Iteration: 106 of 241\ttrain_loss: 2.0411\n",
      "Iteration: 108 of 241\ttrain_loss: 3.0869\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8795\n",
      "Iteration: 112 of 241\ttrain_loss: 2.2786\n",
      "Iteration: 114 of 241\ttrain_loss: 2.2215\n",
      "Iteration: 116 of 241\ttrain_loss: 1.8484\n",
      "Iteration: 118 of 241\ttrain_loss: 2.4517\n",
      "Iteration: 120 of 241\ttrain_loss: 2.4192\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2921\n",
      "Iteration: 124 of 241\ttrain_loss: 1.9668\n",
      "Iteration: 126 of 241\ttrain_loss: 2.5541\n",
      "Iteration: 128 of 241\ttrain_loss: 2.3664\n",
      "Iteration: 130 of 241\ttrain_loss: 2.3724\n",
      "Iteration: 132 of 241\ttrain_loss: 2.3002\n",
      "Iteration: 134 of 241\ttrain_loss: 2.3502\n",
      "Iteration: 136 of 241\ttrain_loss: 2.4733\n",
      "Iteration: 138 of 241\ttrain_loss: 2.0819\n",
      "Iteration: 140 of 241\ttrain_loss: 2.4912\n",
      "Iteration: 142 of 241\ttrain_loss: 2.3269\n",
      "Iteration: 144 of 241\ttrain_loss: 1.9198\n",
      "Iteration: 146 of 241\ttrain_loss: 2.3620\n",
      "Iteration: 148 of 241\ttrain_loss: 2.6532\n",
      "Iteration: 150 of 241\ttrain_loss: 2.2547\n",
      "Iteration: 152 of 241\ttrain_loss: 2.6145\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1706\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4487\n",
      "Iteration: 158 of 241\ttrain_loss: 2.3350\n",
      "Iteration: 160 of 241\ttrain_loss: 2.1782\n",
      "Iteration: 162 of 241\ttrain_loss: 2.3338\n",
      "Iteration: 164 of 241\ttrain_loss: 2.8208\n",
      "Iteration: 166 of 241\ttrain_loss: 2.3812\n",
      "Iteration: 168 of 241\ttrain_loss: 2.3138\n",
      "Iteration: 170 of 241\ttrain_loss: 2.4094\n",
      "Iteration: 172 of 241\ttrain_loss: 2.1714\n",
      "Iteration: 174 of 241\ttrain_loss: 2.1781\n",
      "Iteration: 176 of 241\ttrain_loss: 2.2955\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1985\n",
      "Iteration: 180 of 241\ttrain_loss: 2.8257\n",
      "Iteration: 182 of 241\ttrain_loss: 2.8860\n",
      "Iteration: 184 of 241\ttrain_loss: 2.4591\n",
      "Iteration: 186 of 241\ttrain_loss: 2.3527\n",
      "Iteration: 188 of 241\ttrain_loss: 2.2996\n",
      "Iteration: 190 of 241\ttrain_loss: 2.5261\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1527\n",
      "Iteration: 194 of 241\ttrain_loss: 2.7242\n",
      "Iteration: 196 of 241\ttrain_loss: 2.1124\n",
      "Iteration: 198 of 241\ttrain_loss: 2.5834\n",
      "Iteration: 200 of 241\ttrain_loss: 2.0097\n",
      "Iteration: 202 of 241\ttrain_loss: 2.0681\n",
      "Iteration: 204 of 241\ttrain_loss: 3.0834\n",
      "Iteration: 206 of 241\ttrain_loss: 2.5978\n",
      "Iteration: 208 of 241\ttrain_loss: 2.3514\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9994\n",
      "Iteration: 212 of 241\ttrain_loss: 2.1564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 214 of 241\ttrain_loss: 2.4863\n",
      "Iteration: 216 of 241\ttrain_loss: 2.3715\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2332\n",
      "Iteration: 220 of 241\ttrain_loss: 2.6450\n",
      "Iteration: 222 of 241\ttrain_loss: 2.3866\n",
      "Iteration: 224 of 241\ttrain_loss: 2.6892\n",
      "Iteration: 226 of 241\ttrain_loss: 2.3330\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1085\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1088\n",
      "Iteration: 232 of 241\ttrain_loss: 2.2601\n",
      "Iteration: 234 of 241\ttrain_loss: 2.4441\n",
      "Iteration: 236 of 241\ttrain_loss: 2.5323\n",
      "Iteration: 238 of 241\ttrain_loss: 2.2465\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3634\n",
      "Iteration: 241 of 241\ttrain_loss: 2.9990\n",
      "Average Score for this Epoch: 2.34195876121521\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 60 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.4153\n",
      "Iteration: 2 of 241\ttrain_loss: 2.0503\n",
      "Iteration: 4 of 241\ttrain_loss: 2.4251\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9242\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8203\n",
      "Iteration: 10 of 241\ttrain_loss: 2.3861\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0511\n",
      "Iteration: 14 of 241\ttrain_loss: 2.5523\n",
      "Iteration: 16 of 241\ttrain_loss: 1.9749\n",
      "Iteration: 18 of 241\ttrain_loss: 1.9339\n",
      "Iteration: 20 of 241\ttrain_loss: 2.1001\n",
      "Iteration: 22 of 241\ttrain_loss: 2.7204\n",
      "Iteration: 24 of 241\ttrain_loss: 3.1704\n",
      "Iteration: 26 of 241\ttrain_loss: 2.3163\n",
      "Iteration: 28 of 241\ttrain_loss: 2.2051\n",
      "Iteration: 30 of 241\ttrain_loss: 2.2905\n",
      "Iteration: 32 of 241\ttrain_loss: 2.2523\n",
      "Iteration: 34 of 241\ttrain_loss: 1.7410\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9739\n",
      "Iteration: 38 of 241\ttrain_loss: 2.2680\n",
      "Iteration: 40 of 241\ttrain_loss: 2.4325\n",
      "Iteration: 42 of 241\ttrain_loss: 2.2796\n",
      "Iteration: 44 of 241\ttrain_loss: 2.6328\n",
      "Iteration: 46 of 241\ttrain_loss: 2.0116\n",
      "Iteration: 48 of 241\ttrain_loss: 2.2345\n",
      "Iteration: 50 of 241\ttrain_loss: 2.1545\n",
      "Iteration: 52 of 241\ttrain_loss: 1.9396\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9055\n",
      "Iteration: 56 of 241\ttrain_loss: 2.2969\n",
      "Iteration: 58 of 241\ttrain_loss: 2.2566\n",
      "Iteration: 60 of 241\ttrain_loss: 2.3224\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7723\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4583\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1032\n",
      "Iteration: 68 of 241\ttrain_loss: 2.0534\n",
      "Iteration: 70 of 241\ttrain_loss: 2.2487\n",
      "Iteration: 72 of 241\ttrain_loss: 2.2173\n",
      "Iteration: 74 of 241\ttrain_loss: 2.1892\n",
      "Iteration: 76 of 241\ttrain_loss: 2.1324\n",
      "Iteration: 78 of 241\ttrain_loss: 2.5465\n",
      "Iteration: 80 of 241\ttrain_loss: 2.3946\n",
      "Iteration: 82 of 241\ttrain_loss: 2.3624\n",
      "Iteration: 84 of 241\ttrain_loss: 2.1812\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2396\n",
      "Iteration: 88 of 241\ttrain_loss: 2.3933\n",
      "Iteration: 90 of 241\ttrain_loss: 2.2448\n",
      "Iteration: 92 of 241\ttrain_loss: 2.3382\n",
      "Iteration: 94 of 241\ttrain_loss: 2.0537\n",
      "Iteration: 96 of 241\ttrain_loss: 2.3042\n",
      "Iteration: 98 of 241\ttrain_loss: 2.3572\n",
      "Iteration: 100 of 241\ttrain_loss: 2.2477\n",
      "Iteration: 102 of 241\ttrain_loss: 2.1417\n",
      "Iteration: 104 of 241\ttrain_loss: 2.8967\n",
      "Iteration: 106 of 241\ttrain_loss: 2.1697\n",
      "Iteration: 108 of 241\ttrain_loss: 1.9148\n",
      "Iteration: 110 of 241\ttrain_loss: 2.4625\n",
      "Iteration: 112 of 241\ttrain_loss: 1.9903\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0397\n",
      "Iteration: 116 of 241\ttrain_loss: 2.4199\n",
      "Iteration: 118 of 241\ttrain_loss: 2.1401\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3958\n",
      "Iteration: 122 of 241\ttrain_loss: 1.8954\n",
      "Iteration: 124 of 241\ttrain_loss: 2.4036\n",
      "Iteration: 126 of 241\ttrain_loss: 2.2960\n",
      "Iteration: 128 of 241\ttrain_loss: 2.7226\n",
      "Iteration: 130 of 241\ttrain_loss: 2.0883\n",
      "Iteration: 132 of 241\ttrain_loss: 2.2587\n",
      "Iteration: 134 of 241\ttrain_loss: 1.9182\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3145\n",
      "Iteration: 138 of 241\ttrain_loss: 2.7328\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2374\n",
      "Iteration: 142 of 241\ttrain_loss: 2.1935\n",
      "Iteration: 144 of 241\ttrain_loss: 2.6489\n",
      "Iteration: 146 of 241\ttrain_loss: 2.5021\n",
      "Iteration: 148 of 241\ttrain_loss: 2.2284\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0378\n",
      "Iteration: 152 of 241\ttrain_loss: 2.6227\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1098\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4487\n",
      "Iteration: 158 of 241\ttrain_loss: 2.2338\n",
      "Iteration: 160 of 241\ttrain_loss: 2.2946\n",
      "Iteration: 162 of 241\ttrain_loss: 2.3412\n",
      "Iteration: 164 of 241\ttrain_loss: 1.7712\n",
      "Iteration: 166 of 241\ttrain_loss: 2.0968\n",
      "Iteration: 168 of 241\ttrain_loss: 1.9549\n",
      "Iteration: 170 of 241\ttrain_loss: 2.0984\n",
      "Iteration: 172 of 241\ttrain_loss: 2.2727\n",
      "Iteration: 174 of 241\ttrain_loss: 2.2779\n",
      "Iteration: 176 of 241\ttrain_loss: 2.0497\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1499\n",
      "Iteration: 180 of 241\ttrain_loss: 2.1191\n",
      "Iteration: 182 of 241\ttrain_loss: 3.1909\n",
      "Iteration: 184 of 241\ttrain_loss: 2.3197\n",
      "Iteration: 186 of 241\ttrain_loss: 2.6162\n",
      "Iteration: 188 of 241\ttrain_loss: 2.3461\n",
      "Iteration: 190 of 241\ttrain_loss: 2.5086\n",
      "Iteration: 192 of 241\ttrain_loss: 2.4148\n",
      "Iteration: 194 of 241\ttrain_loss: 2.3319\n",
      "Iteration: 196 of 241\ttrain_loss: 2.5007\n",
      "Iteration: 198 of 241\ttrain_loss: 2.4408\n",
      "Iteration: 200 of 241\ttrain_loss: 2.3368\n",
      "Iteration: 202 of 241\ttrain_loss: 2.3014\n",
      "Iteration: 204 of 241\ttrain_loss: 2.4409\n",
      "Iteration: 206 of 241\ttrain_loss: 2.1529\n",
      "Iteration: 208 of 241\ttrain_loss: 2.5813\n",
      "Iteration: 210 of 241\ttrain_loss: 2.2516\n",
      "Iteration: 212 of 241\ttrain_loss: 2.3279\n",
      "Iteration: 214 of 241\ttrain_loss: 2.5384\n",
      "Iteration: 216 of 241\ttrain_loss: 2.3496\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2859\n",
      "Iteration: 220 of 241\ttrain_loss: 2.4065\n",
      "Iteration: 222 of 241\ttrain_loss: 2.1186\n",
      "Iteration: 224 of 241\ttrain_loss: 2.6855\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5171\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9927\n",
      "Iteration: 230 of 241\ttrain_loss: 2.2278\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1605\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2510\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9479\n",
      "Iteration: 238 of 241\ttrain_loss: 2.4303\n",
      "Iteration: 240 of 241\ttrain_loss: 2.5856\n",
      "Iteration: 241 of 241\ttrain_loss: 2.7145\n",
      "Average Score for this Epoch: 2.26838755607605\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 61 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.4854\n",
      "Iteration: 2 of 241\ttrain_loss: 2.2484\n",
      "Iteration: 4 of 241\ttrain_loss: 3.1576\n",
      "Iteration: 6 of 241\ttrain_loss: 1.8579\n",
      "Iteration: 8 of 241\ttrain_loss: 2.4062\n",
      "Iteration: 10 of 241\ttrain_loss: 2.3555\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0616\n",
      "Iteration: 14 of 241\ttrain_loss: 2.2668\n",
      "Iteration: 16 of 241\ttrain_loss: 2.2267\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8117\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0156\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1646\n",
      "Iteration: 24 of 241\ttrain_loss: 1.5910\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8349\n",
      "Iteration: 28 of 241\ttrain_loss: 1.8323\n",
      "Iteration: 30 of 241\ttrain_loss: 2.0839\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0228\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1510\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0090\n",
      "Iteration: 38 of 241\ttrain_loss: 2.3570\n",
      "Iteration: 40 of 241\ttrain_loss: 2.2245\n",
      "Iteration: 42 of 241\ttrain_loss: 2.1196\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1455\n",
      "Iteration: 46 of 241\ttrain_loss: 2.0994\n",
      "Iteration: 48 of 241\ttrain_loss: 2.2598\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8758\n",
      "Iteration: 52 of 241\ttrain_loss: 1.9305\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0747\n",
      "Iteration: 56 of 241\ttrain_loss: 1.9977\n",
      "Iteration: 58 of 241\ttrain_loss: 2.5748\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8303\n",
      "Iteration: 62 of 241\ttrain_loss: 1.9676\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4147\n",
      "Iteration: 66 of 241\ttrain_loss: 2.4747\n",
      "Iteration: 68 of 241\ttrain_loss: 2.2937\n",
      "Iteration: 70 of 241\ttrain_loss: 1.9410\n",
      "Iteration: 72 of 241\ttrain_loss: 2.2497\n",
      "Iteration: 74 of 241\ttrain_loss: 2.6771\n",
      "Iteration: 76 of 241\ttrain_loss: 1.9621\n",
      "Iteration: 78 of 241\ttrain_loss: 2.3169\n",
      "Iteration: 80 of 241\ttrain_loss: 1.9366\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0969\n",
      "Iteration: 84 of 241\ttrain_loss: 1.7467\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2609\n",
      "Iteration: 88 of 241\ttrain_loss: 2.0757\n",
      "Iteration: 90 of 241\ttrain_loss: 2.3884\n",
      "Iteration: 92 of 241\ttrain_loss: 2.1218\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9157\n",
      "Iteration: 96 of 241\ttrain_loss: 2.9854\n",
      "Iteration: 98 of 241\ttrain_loss: 2.2661\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4332\n",
      "Iteration: 102 of 241\ttrain_loss: 2.2072\n",
      "Iteration: 104 of 241\ttrain_loss: 2.0857\n",
      "Iteration: 106 of 241\ttrain_loss: 2.1731\n",
      "Iteration: 108 of 241\ttrain_loss: 2.2153\n",
      "Iteration: 110 of 241\ttrain_loss: 2.3561\n",
      "Iteration: 112 of 241\ttrain_loss: 2.1627\n",
      "Iteration: 114 of 241\ttrain_loss: 1.6901\n",
      "Iteration: 116 of 241\ttrain_loss: 2.2283\n",
      "Iteration: 118 of 241\ttrain_loss: 2.3884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 120 of 241\ttrain_loss: 1.8366\n",
      "Iteration: 122 of 241\ttrain_loss: 2.4390\n",
      "Iteration: 124 of 241\ttrain_loss: 2.3708\n",
      "Iteration: 126 of 241\ttrain_loss: 2.1942\n",
      "Iteration: 128 of 241\ttrain_loss: 2.7556\n",
      "Iteration: 130 of 241\ttrain_loss: 2.4445\n",
      "Iteration: 132 of 241\ttrain_loss: 2.0801\n",
      "Iteration: 134 of 241\ttrain_loss: 2.2716\n",
      "Iteration: 136 of 241\ttrain_loss: 2.0687\n",
      "Iteration: 138 of 241\ttrain_loss: 2.2091\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0193\n",
      "Iteration: 142 of 241\ttrain_loss: 2.3885\n",
      "Iteration: 144 of 241\ttrain_loss: 2.4777\n",
      "Iteration: 146 of 241\ttrain_loss: 2.0793\n",
      "Iteration: 148 of 241\ttrain_loss: 2.3482\n",
      "Iteration: 150 of 241\ttrain_loss: 1.9573\n",
      "Iteration: 152 of 241\ttrain_loss: 2.4165\n",
      "Iteration: 154 of 241\ttrain_loss: 2.3157\n",
      "Iteration: 156 of 241\ttrain_loss: 2.0756\n",
      "Iteration: 158 of 241\ttrain_loss: 2.3213\n",
      "Iteration: 160 of 241\ttrain_loss: 2.2727\n",
      "Iteration: 162 of 241\ttrain_loss: 3.1056\n",
      "Iteration: 164 of 241\ttrain_loss: 2.5142\n",
      "Iteration: 166 of 241\ttrain_loss: 2.4506\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2002\n",
      "Iteration: 170 of 241\ttrain_loss: 2.4000\n",
      "Iteration: 172 of 241\ttrain_loss: 2.6563\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0544\n",
      "Iteration: 176 of 241\ttrain_loss: 2.3822\n",
      "Iteration: 178 of 241\ttrain_loss: 2.7053\n",
      "Iteration: 180 of 241\ttrain_loss: 2.1916\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7457\n",
      "Iteration: 184 of 241\ttrain_loss: 2.6825\n",
      "Iteration: 186 of 241\ttrain_loss: 2.3309\n",
      "Iteration: 188 of 241\ttrain_loss: 1.9570\n",
      "Iteration: 190 of 241\ttrain_loss: 2.7590\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1355\n",
      "Iteration: 194 of 241\ttrain_loss: 2.1534\n",
      "Iteration: 196 of 241\ttrain_loss: 2.1634\n",
      "Iteration: 198 of 241\ttrain_loss: 2.6106\n",
      "Iteration: 200 of 241\ttrain_loss: 2.3165\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7140\n",
      "Iteration: 204 of 241\ttrain_loss: 2.1861\n",
      "Iteration: 206 of 241\ttrain_loss: 2.4117\n",
      "Iteration: 208 of 241\ttrain_loss: 2.5169\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9547\n",
      "Iteration: 212 of 241\ttrain_loss: 2.9496\n",
      "Iteration: 214 of 241\ttrain_loss: 2.5279\n",
      "Iteration: 216 of 241\ttrain_loss: 2.2220\n",
      "Iteration: 218 of 241\ttrain_loss: 2.7753\n",
      "Iteration: 220 of 241\ttrain_loss: 2.3673\n",
      "Iteration: 222 of 241\ttrain_loss: 2.3742\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2775\n",
      "Iteration: 226 of 241\ttrain_loss: 2.4803\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2237\n",
      "Iteration: 230 of 241\ttrain_loss: 2.5630\n",
      "Iteration: 232 of 241\ttrain_loss: 2.2831\n",
      "Iteration: 234 of 241\ttrain_loss: 2.0382\n",
      "Iteration: 236 of 241\ttrain_loss: 2.0789\n",
      "Iteration: 238 of 241\ttrain_loss: 2.3158\n",
      "Iteration: 240 of 241\ttrain_loss: 2.2109\n",
      "Iteration: 241 of 241\ttrain_loss: 2.4327\n",
      "Average Score for this Epoch: 2.2370357513427734\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 62 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.0045\n",
      "Iteration: 2 of 241\ttrain_loss: 2.3727\n",
      "Iteration: 4 of 241\ttrain_loss: 1.9530\n",
      "Iteration: 6 of 241\ttrain_loss: 2.2906\n",
      "Iteration: 8 of 241\ttrain_loss: 1.7049\n",
      "Iteration: 10 of 241\ttrain_loss: 2.1224\n",
      "Iteration: 12 of 241\ttrain_loss: 2.1770\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9240\n",
      "Iteration: 16 of 241\ttrain_loss: 2.1796\n",
      "Iteration: 18 of 241\ttrain_loss: 2.2767\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0910\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1287\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0165\n",
      "Iteration: 26 of 241\ttrain_loss: 2.1433\n",
      "Iteration: 28 of 241\ttrain_loss: 1.9983\n",
      "Iteration: 30 of 241\ttrain_loss: 2.2030\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0859\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1937\n",
      "Iteration: 36 of 241\ttrain_loss: 1.7293\n",
      "Iteration: 38 of 241\ttrain_loss: 2.3680\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9347\n",
      "Iteration: 42 of 241\ttrain_loss: 2.6517\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1853\n",
      "Iteration: 46 of 241\ttrain_loss: 2.0704\n",
      "Iteration: 48 of 241\ttrain_loss: 2.1329\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8860\n",
      "Iteration: 52 of 241\ttrain_loss: 2.1228\n",
      "Iteration: 54 of 241\ttrain_loss: 2.3784\n",
      "Iteration: 56 of 241\ttrain_loss: 2.2642\n",
      "Iteration: 58 of 241\ttrain_loss: 1.8194\n",
      "Iteration: 60 of 241\ttrain_loss: 2.2090\n",
      "Iteration: 62 of 241\ttrain_loss: 2.0440\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8659\n",
      "Iteration: 66 of 241\ttrain_loss: 2.2887\n",
      "Iteration: 68 of 241\ttrain_loss: 2.0767\n",
      "Iteration: 70 of 241\ttrain_loss: 2.2216\n",
      "Iteration: 72 of 241\ttrain_loss: 2.5058\n",
      "Iteration: 74 of 241\ttrain_loss: 2.2711\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0865\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1676\n",
      "Iteration: 80 of 241\ttrain_loss: 2.4838\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4779\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0254\n",
      "Iteration: 86 of 241\ttrain_loss: 1.8478\n",
      "Iteration: 88 of 241\ttrain_loss: 1.8727\n",
      "Iteration: 90 of 241\ttrain_loss: 2.1091\n",
      "Iteration: 92 of 241\ttrain_loss: 2.3175\n",
      "Iteration: 94 of 241\ttrain_loss: 2.3266\n",
      "Iteration: 96 of 241\ttrain_loss: 2.4785\n",
      "Iteration: 98 of 241\ttrain_loss: 2.3059\n",
      "Iteration: 100 of 241\ttrain_loss: 2.4554\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9344\n",
      "Iteration: 104 of 241\ttrain_loss: 1.8420\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8611\n",
      "Iteration: 108 of 241\ttrain_loss: 2.6315\n",
      "Iteration: 110 of 241\ttrain_loss: 2.2003\n",
      "Iteration: 112 of 241\ttrain_loss: 2.4936\n",
      "Iteration: 114 of 241\ttrain_loss: 1.9544\n",
      "Iteration: 116 of 241\ttrain_loss: 2.2474\n",
      "Iteration: 118 of 241\ttrain_loss: 2.3178\n",
      "Iteration: 120 of 241\ttrain_loss: 2.1793\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2641\n",
      "Iteration: 124 of 241\ttrain_loss: 2.2833\n",
      "Iteration: 126 of 241\ttrain_loss: 2.0495\n",
      "Iteration: 128 of 241\ttrain_loss: 2.2188\n",
      "Iteration: 130 of 241\ttrain_loss: 2.0649\n",
      "Iteration: 132 of 241\ttrain_loss: 1.8935\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0290\n",
      "Iteration: 136 of 241\ttrain_loss: 2.5407\n",
      "Iteration: 138 of 241\ttrain_loss: 2.4052\n",
      "Iteration: 140 of 241\ttrain_loss: 2.3187\n",
      "Iteration: 142 of 241\ttrain_loss: 2.5397\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5121\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9525\n",
      "Iteration: 148 of 241\ttrain_loss: 2.3368\n",
      "Iteration: 150 of 241\ttrain_loss: 2.2075\n",
      "Iteration: 152 of 241\ttrain_loss: 2.0841\n",
      "Iteration: 154 of 241\ttrain_loss: 2.0841\n",
      "Iteration: 156 of 241\ttrain_loss: 2.1831\n",
      "Iteration: 158 of 241\ttrain_loss: 2.3709\n",
      "Iteration: 160 of 241\ttrain_loss: 2.4428\n",
      "Iteration: 162 of 241\ttrain_loss: 1.8976\n",
      "Iteration: 164 of 241\ttrain_loss: 2.6542\n",
      "Iteration: 166 of 241\ttrain_loss: 1.9368\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2287\n",
      "Iteration: 170 of 241\ttrain_loss: 2.3037\n",
      "Iteration: 172 of 241\ttrain_loss: 2.3826\n",
      "Iteration: 174 of 241\ttrain_loss: 2.1567\n",
      "Iteration: 176 of 241\ttrain_loss: 2.4702\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1178\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8834\n",
      "Iteration: 182 of 241\ttrain_loss: 2.5339\n",
      "Iteration: 184 of 241\ttrain_loss: 2.3026\n",
      "Iteration: 186 of 241\ttrain_loss: 2.6309\n",
      "Iteration: 188 of 241\ttrain_loss: 2.4570\n",
      "Iteration: 190 of 241\ttrain_loss: 2.2397\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1243\n",
      "Iteration: 194 of 241\ttrain_loss: 2.4477\n",
      "Iteration: 196 of 241\ttrain_loss: 1.9975\n",
      "Iteration: 198 of 241\ttrain_loss: 2.2335\n",
      "Iteration: 200 of 241\ttrain_loss: 1.9339\n",
      "Iteration: 202 of 241\ttrain_loss: 2.4939\n",
      "Iteration: 204 of 241\ttrain_loss: 2.3828\n",
      "Iteration: 206 of 241\ttrain_loss: 2.2082\n",
      "Iteration: 208 of 241\ttrain_loss: 2.3858\n",
      "Iteration: 210 of 241\ttrain_loss: 2.3721\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8050\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7781\n",
      "Iteration: 216 of 241\ttrain_loss: 2.6548\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0947\n",
      "Iteration: 220 of 241\ttrain_loss: 2.7651\n",
      "Iteration: 222 of 241\ttrain_loss: 2.1875\n",
      "Iteration: 224 of 241\ttrain_loss: 2.1966\n",
      "Iteration: 226 of 241\ttrain_loss: 2.1431\n",
      "Iteration: 228 of 241\ttrain_loss: 2.8669\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1221\n",
      "Iteration: 232 of 241\ttrain_loss: 2.4159\n",
      "Iteration: 234 of 241\ttrain_loss: 2.1184\n",
      "Iteration: 236 of 241\ttrain_loss: 2.5329\n",
      "Iteration: 238 of 241\ttrain_loss: 2.2843\n",
      "Iteration: 240 of 241\ttrain_loss: 2.2161\n",
      "Iteration: 241 of 241\ttrain_loss: 2.1802\n",
      "Average Score for this Epoch: 2.210249900817871\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 63 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9463\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8920\n",
      "Iteration: 4 of 241\ttrain_loss: 1.9146\n",
      "Iteration: 6 of 241\ttrain_loss: 1.7744\n",
      "Iteration: 8 of 241\ttrain_loss: 2.1590\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8782\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9233\n",
      "Iteration: 14 of 241\ttrain_loss: 2.0231\n",
      "Iteration: 16 of 241\ttrain_loss: 1.9517\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6927\n",
      "Iteration: 20 of 241\ttrain_loss: 2.2985\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 24 of 241\ttrain_loss: 2.3997\n",
      "Iteration: 26 of 241\ttrain_loss: 2.2662\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7884\n",
      "Iteration: 30 of 241\ttrain_loss: 2.2689\n",
      "Iteration: 32 of 241\ttrain_loss: 2.2851\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1420\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6921\n",
      "Iteration: 38 of 241\ttrain_loss: 2.5277\n",
      "Iteration: 40 of 241\ttrain_loss: 2.0741\n",
      "Iteration: 42 of 241\ttrain_loss: 2.0785\n",
      "Iteration: 44 of 241\ttrain_loss: 2.0231\n",
      "Iteration: 46 of 241\ttrain_loss: 3.1296\n",
      "Iteration: 48 of 241\ttrain_loss: 2.3069\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0963\n",
      "Iteration: 52 of 241\ttrain_loss: 2.1422\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9730\n",
      "Iteration: 56 of 241\ttrain_loss: 2.1987\n",
      "Iteration: 58 of 241\ttrain_loss: 1.9952\n",
      "Iteration: 60 of 241\ttrain_loss: 2.4898\n",
      "Iteration: 62 of 241\ttrain_loss: 2.1284\n",
      "Iteration: 64 of 241\ttrain_loss: 2.6962\n",
      "Iteration: 66 of 241\ttrain_loss: 2.6565\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9944\n",
      "Iteration: 70 of 241\ttrain_loss: 2.4347\n",
      "Iteration: 72 of 241\ttrain_loss: 2.3682\n",
      "Iteration: 74 of 241\ttrain_loss: 2.2786\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0283\n",
      "Iteration: 78 of 241\ttrain_loss: 2.4617\n",
      "Iteration: 80 of 241\ttrain_loss: 2.3247\n",
      "Iteration: 82 of 241\ttrain_loss: 2.1408\n",
      "Iteration: 84 of 241\ttrain_loss: 2.2245\n",
      "Iteration: 86 of 241\ttrain_loss: 2.5729\n",
      "Iteration: 88 of 241\ttrain_loss: 2.3713\n",
      "Iteration: 90 of 241\ttrain_loss: 1.9698\n",
      "Iteration: 92 of 241\ttrain_loss: 2.0616\n",
      "Iteration: 94 of 241\ttrain_loss: 2.7565\n",
      "Iteration: 96 of 241\ttrain_loss: 1.6457\n",
      "Iteration: 98 of 241\ttrain_loss: 2.2154\n",
      "Iteration: 100 of 241\ttrain_loss: 2.3562\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9465\n",
      "Iteration: 104 of 241\ttrain_loss: 2.3537\n",
      "Iteration: 106 of 241\ttrain_loss: 1.9484\n",
      "Iteration: 108 of 241\ttrain_loss: 2.1646\n",
      "Iteration: 110 of 241\ttrain_loss: 2.6467\n",
      "Iteration: 112 of 241\ttrain_loss: 2.5177\n",
      "Iteration: 114 of 241\ttrain_loss: 2.1047\n",
      "Iteration: 116 of 241\ttrain_loss: 2.4377\n",
      "Iteration: 118 of 241\ttrain_loss: 2.2564\n",
      "Iteration: 120 of 241\ttrain_loss: 2.1433\n",
      "Iteration: 122 of 241\ttrain_loss: 1.9339\n",
      "Iteration: 124 of 241\ttrain_loss: 2.1433\n",
      "Iteration: 126 of 241\ttrain_loss: 2.3140\n",
      "Iteration: 128 of 241\ttrain_loss: 2.1775\n",
      "Iteration: 130 of 241\ttrain_loss: 2.6598\n",
      "Iteration: 132 of 241\ttrain_loss: 2.3131\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8745\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9066\n",
      "Iteration: 138 of 241\ttrain_loss: 2.2514\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0023\n",
      "Iteration: 142 of 241\ttrain_loss: 1.7930\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7627\n",
      "Iteration: 146 of 241\ttrain_loss: 2.2394\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9853\n",
      "Iteration: 150 of 241\ttrain_loss: 2.1888\n",
      "Iteration: 152 of 241\ttrain_loss: 2.2910\n",
      "Iteration: 154 of 241\ttrain_loss: 1.9576\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9167\n",
      "Iteration: 158 of 241\ttrain_loss: 2.4022\n",
      "Iteration: 160 of 241\ttrain_loss: 1.9578\n",
      "Iteration: 162 of 241\ttrain_loss: 2.5742\n",
      "Iteration: 164 of 241\ttrain_loss: 2.3304\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1031\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7908\n",
      "Iteration: 170 of 241\ttrain_loss: 1.9331\n",
      "Iteration: 172 of 241\ttrain_loss: 1.8911\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0175\n",
      "Iteration: 176 of 241\ttrain_loss: 2.1032\n",
      "Iteration: 178 of 241\ttrain_loss: 2.3461\n",
      "Iteration: 180 of 241\ttrain_loss: 2.5695\n",
      "Iteration: 182 of 241\ttrain_loss: 1.9348\n",
      "Iteration: 184 of 241\ttrain_loss: 2.1791\n",
      "Iteration: 186 of 241\ttrain_loss: 2.6142\n",
      "Iteration: 188 of 241\ttrain_loss: 2.3593\n",
      "Iteration: 190 of 241\ttrain_loss: 2.3241\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0748\n",
      "Iteration: 194 of 241\ttrain_loss: 2.1562\n",
      "Iteration: 196 of 241\ttrain_loss: 2.3988\n",
      "Iteration: 198 of 241\ttrain_loss: 2.0967\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8580\n",
      "Iteration: 202 of 241\ttrain_loss: 2.1310\n",
      "Iteration: 204 of 241\ttrain_loss: 2.3690\n",
      "Iteration: 206 of 241\ttrain_loss: 2.3321\n",
      "Iteration: 208 of 241\ttrain_loss: 2.2919\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9638\n",
      "Iteration: 212 of 241\ttrain_loss: 2.3116\n",
      "Iteration: 214 of 241\ttrain_loss: 2.4185\n",
      "Iteration: 216 of 241\ttrain_loss: 2.2480\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0349\n",
      "Iteration: 220 of 241\ttrain_loss: 2.0026\n",
      "Iteration: 222 of 241\ttrain_loss: 1.9181\n",
      "Iteration: 224 of 241\ttrain_loss: 2.4821\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9107\n",
      "Iteration: 228 of 241\ttrain_loss: 2.4102\n",
      "Iteration: 230 of 241\ttrain_loss: 2.3220\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1183\n",
      "Iteration: 234 of 241\ttrain_loss: 2.3087\n",
      "Iteration: 236 of 241\ttrain_loss: 2.6173\n",
      "Iteration: 238 of 241\ttrain_loss: 2.0445\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3470\n",
      "Iteration: 241 of 241\ttrain_loss: 2.0645\n",
      "Average Score for this Epoch: 2.183894634246826\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 64 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7535\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8100\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7415\n",
      "Iteration: 6 of 241\ttrain_loss: 2.3684\n",
      "Iteration: 8 of 241\ttrain_loss: 2.2330\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8224\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0772\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7168\n",
      "Iteration: 16 of 241\ttrain_loss: 2.0352\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8566\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0449\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1304\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0794\n",
      "Iteration: 26 of 241\ttrain_loss: 2.2466\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7851\n",
      "Iteration: 30 of 241\ttrain_loss: 2.4904\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0771\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1261\n",
      "Iteration: 36 of 241\ttrain_loss: 2.3527\n",
      "Iteration: 38 of 241\ttrain_loss: 1.8756\n",
      "Iteration: 40 of 241\ttrain_loss: 1.8037\n",
      "Iteration: 42 of 241\ttrain_loss: 2.2370\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1492\n",
      "Iteration: 46 of 241\ttrain_loss: 2.1355\n",
      "Iteration: 48 of 241\ttrain_loss: 2.0005\n",
      "Iteration: 50 of 241\ttrain_loss: 2.1614\n",
      "Iteration: 52 of 241\ttrain_loss: 2.3968\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9719\n",
      "Iteration: 56 of 241\ttrain_loss: 2.3484\n",
      "Iteration: 58 of 241\ttrain_loss: 2.0467\n",
      "Iteration: 60 of 241\ttrain_loss: 2.1065\n",
      "Iteration: 62 of 241\ttrain_loss: 2.3008\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4044\n",
      "Iteration: 66 of 241\ttrain_loss: 2.2690\n",
      "Iteration: 68 of 241\ttrain_loss: 2.4347\n",
      "Iteration: 70 of 241\ttrain_loss: 2.3845\n",
      "Iteration: 72 of 241\ttrain_loss: 2.3524\n",
      "Iteration: 74 of 241\ttrain_loss: 2.1591\n",
      "Iteration: 76 of 241\ttrain_loss: 2.3540\n",
      "Iteration: 78 of 241\ttrain_loss: 2.8769\n",
      "Iteration: 80 of 241\ttrain_loss: 1.9835\n",
      "Iteration: 82 of 241\ttrain_loss: 2.4231\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9190\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2023\n",
      "Iteration: 88 of 241\ttrain_loss: 2.2369\n",
      "Iteration: 90 of 241\ttrain_loss: 1.9561\n",
      "Iteration: 92 of 241\ttrain_loss: 2.4838\n",
      "Iteration: 94 of 241\ttrain_loss: 2.2850\n",
      "Iteration: 96 of 241\ttrain_loss: 1.8886\n",
      "Iteration: 98 of 241\ttrain_loss: 1.8898\n",
      "Iteration: 100 of 241\ttrain_loss: 2.1963\n",
      "Iteration: 102 of 241\ttrain_loss: 2.7301\n",
      "Iteration: 104 of 241\ttrain_loss: 1.6743\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8480\n",
      "Iteration: 108 of 241\ttrain_loss: 1.9442\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1777\n",
      "Iteration: 112 of 241\ttrain_loss: 2.0024\n",
      "Iteration: 114 of 241\ttrain_loss: 2.2611\n",
      "Iteration: 116 of 241\ttrain_loss: 1.8643\n",
      "Iteration: 118 of 241\ttrain_loss: 2.3898\n",
      "Iteration: 120 of 241\ttrain_loss: 2.0514\n",
      "Iteration: 122 of 241\ttrain_loss: 1.6840\n",
      "Iteration: 124 of 241\ttrain_loss: 1.8043\n",
      "Iteration: 126 of 241\ttrain_loss: 2.2580\n",
      "Iteration: 128 of 241\ttrain_loss: 2.2903\n",
      "Iteration: 130 of 241\ttrain_loss: 2.3909\n",
      "Iteration: 132 of 241\ttrain_loss: 1.9383\n",
      "Iteration: 134 of 241\ttrain_loss: 1.9950\n",
      "Iteration: 136 of 241\ttrain_loss: 2.5794\n",
      "Iteration: 138 of 241\ttrain_loss: 2.4589\n",
      "Iteration: 140 of 241\ttrain_loss: 1.6957\n",
      "Iteration: 142 of 241\ttrain_loss: 2.4692\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5829\n",
      "Iteration: 146 of 241\ttrain_loss: 1.6366\n",
      "Iteration: 148 of 241\ttrain_loss: 2.7211\n",
      "Iteration: 150 of 241\ttrain_loss: 2.7617\n",
      "Iteration: 152 of 241\ttrain_loss: 2.1900\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8776\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9638\n",
      "Iteration: 158 of 241\ttrain_loss: 1.8167\n",
      "Iteration: 160 of 241\ttrain_loss: 2.0335\n",
      "Iteration: 162 of 241\ttrain_loss: 2.1304\n",
      "Iteration: 164 of 241\ttrain_loss: 2.2764\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1733\n",
      "Iteration: 168 of 241\ttrain_loss: 2.0759\n",
      "Iteration: 170 of 241\ttrain_loss: 2.2461\n",
      "Iteration: 172 of 241\ttrain_loss: 1.8795\n",
      "Iteration: 174 of 241\ttrain_loss: 2.2967\n",
      "Iteration: 176 of 241\ttrain_loss: 2.3951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 178 of 241\ttrain_loss: 1.6593\n",
      "Iteration: 180 of 241\ttrain_loss: 2.3542\n",
      "Iteration: 182 of 241\ttrain_loss: 2.1814\n",
      "Iteration: 184 of 241\ttrain_loss: 2.4545\n",
      "Iteration: 186 of 241\ttrain_loss: 2.1647\n",
      "Iteration: 188 of 241\ttrain_loss: 2.0230\n",
      "Iteration: 190 of 241\ttrain_loss: 2.2615\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0559\n",
      "Iteration: 194 of 241\ttrain_loss: 2.0163\n",
      "Iteration: 196 of 241\ttrain_loss: 2.0281\n",
      "Iteration: 198 of 241\ttrain_loss: 2.9607\n",
      "Iteration: 200 of 241\ttrain_loss: 2.6677\n",
      "Iteration: 202 of 241\ttrain_loss: 2.1192\n",
      "Iteration: 204 of 241\ttrain_loss: 2.4678\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0514\n",
      "Iteration: 208 of 241\ttrain_loss: 2.6051\n",
      "Iteration: 210 of 241\ttrain_loss: 2.2241\n",
      "Iteration: 212 of 241\ttrain_loss: 2.7335\n",
      "Iteration: 214 of 241\ttrain_loss: 1.8712\n",
      "Iteration: 216 of 241\ttrain_loss: 2.1849\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0924\n",
      "Iteration: 220 of 241\ttrain_loss: 2.0544\n",
      "Iteration: 222 of 241\ttrain_loss: 2.0482\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2993\n",
      "Iteration: 226 of 241\ttrain_loss: 2.3138\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2277\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1894\n",
      "Iteration: 232 of 241\ttrain_loss: 1.9905\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2314\n",
      "Iteration: 236 of 241\ttrain_loss: 2.2581\n",
      "Iteration: 238 of 241\ttrain_loss: 2.3943\n",
      "Iteration: 240 of 241\ttrain_loss: 2.0005\n",
      "Iteration: 241 of 241\ttrain_loss: 1.7443\n",
      "Average Score for this Epoch: 2.1517179012298584\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 65 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.1558\n",
      "Iteration: 2 of 241\ttrain_loss: 2.3015\n",
      "Iteration: 4 of 241\ttrain_loss: 2.3397\n",
      "Iteration: 6 of 241\ttrain_loss: 2.1062\n",
      "Iteration: 8 of 241\ttrain_loss: 2.3812\n",
      "Iteration: 10 of 241\ttrain_loss: 2.1572\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7101\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9894\n",
      "Iteration: 16 of 241\ttrain_loss: 2.3978\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8452\n",
      "Iteration: 20 of 241\ttrain_loss: 2.4500\n",
      "Iteration: 22 of 241\ttrain_loss: 2.0100\n",
      "Iteration: 24 of 241\ttrain_loss: 1.9932\n",
      "Iteration: 26 of 241\ttrain_loss: 2.1418\n",
      "Iteration: 28 of 241\ttrain_loss: 2.6882\n",
      "Iteration: 30 of 241\ttrain_loss: 2.4122\n",
      "Iteration: 32 of 241\ttrain_loss: 2.1702\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1252\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9470\n",
      "Iteration: 38 of 241\ttrain_loss: 2.1176\n",
      "Iteration: 40 of 241\ttrain_loss: 2.0847\n",
      "Iteration: 42 of 241\ttrain_loss: 2.1365\n",
      "Iteration: 44 of 241\ttrain_loss: 1.9273\n",
      "Iteration: 46 of 241\ttrain_loss: 1.7010\n",
      "Iteration: 48 of 241\ttrain_loss: 2.0862\n",
      "Iteration: 50 of 241\ttrain_loss: 1.9365\n",
      "Iteration: 52 of 241\ttrain_loss: 2.3771\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0138\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8404\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7368\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7066\n",
      "Iteration: 62 of 241\ttrain_loss: 2.5096\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8066\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1568\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9917\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0772\n",
      "Iteration: 72 of 241\ttrain_loss: 2.3651\n",
      "Iteration: 74 of 241\ttrain_loss: 1.9236\n",
      "Iteration: 76 of 241\ttrain_loss: 1.7805\n",
      "Iteration: 78 of 241\ttrain_loss: 2.2426\n",
      "Iteration: 80 of 241\ttrain_loss: 2.1667\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0254\n",
      "Iteration: 84 of 241\ttrain_loss: 1.8395\n",
      "Iteration: 86 of 241\ttrain_loss: 1.9416\n",
      "Iteration: 88 of 241\ttrain_loss: 2.1161\n",
      "Iteration: 90 of 241\ttrain_loss: 2.1819\n",
      "Iteration: 92 of 241\ttrain_loss: 2.8546\n",
      "Iteration: 94 of 241\ttrain_loss: 2.1774\n",
      "Iteration: 96 of 241\ttrain_loss: 2.2309\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9854\n",
      "Iteration: 100 of 241\ttrain_loss: 1.8317\n",
      "Iteration: 102 of 241\ttrain_loss: 2.0702\n",
      "Iteration: 104 of 241\ttrain_loss: 1.8380\n",
      "Iteration: 106 of 241\ttrain_loss: 2.5616\n",
      "Iteration: 108 of 241\ttrain_loss: 2.5772\n",
      "Iteration: 110 of 241\ttrain_loss: 2.0133\n",
      "Iteration: 112 of 241\ttrain_loss: 2.3841\n",
      "Iteration: 114 of 241\ttrain_loss: 1.6712\n",
      "Iteration: 116 of 241\ttrain_loss: 2.3430\n",
      "Iteration: 118 of 241\ttrain_loss: 2.2133\n",
      "Iteration: 120 of 241\ttrain_loss: 2.0826\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2183\n",
      "Iteration: 124 of 241\ttrain_loss: 1.9229\n",
      "Iteration: 126 of 241\ttrain_loss: 2.1730\n",
      "Iteration: 128 of 241\ttrain_loss: 2.2260\n",
      "Iteration: 130 of 241\ttrain_loss: 1.9294\n",
      "Iteration: 132 of 241\ttrain_loss: 1.8241\n",
      "Iteration: 134 of 241\ttrain_loss: 2.1098\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3478\n",
      "Iteration: 138 of 241\ttrain_loss: 1.8910\n",
      "Iteration: 140 of 241\ttrain_loss: 2.5336\n",
      "Iteration: 142 of 241\ttrain_loss: 2.4123\n",
      "Iteration: 144 of 241\ttrain_loss: 1.9205\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9857\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9116\n",
      "Iteration: 150 of 241\ttrain_loss: 1.9735\n",
      "Iteration: 152 of 241\ttrain_loss: 1.9810\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8307\n",
      "Iteration: 156 of 241\ttrain_loss: 2.8972\n",
      "Iteration: 158 of 241\ttrain_loss: 2.6106\n",
      "Iteration: 160 of 241\ttrain_loss: 2.0833\n",
      "Iteration: 162 of 241\ttrain_loss: 2.0267\n",
      "Iteration: 164 of 241\ttrain_loss: 2.1070\n",
      "Iteration: 166 of 241\ttrain_loss: 1.9907\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2997\n",
      "Iteration: 170 of 241\ttrain_loss: 2.1182\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0433\n",
      "Iteration: 174 of 241\ttrain_loss: 1.7283\n",
      "Iteration: 176 of 241\ttrain_loss: 2.4545\n",
      "Iteration: 178 of 241\ttrain_loss: 2.0992\n",
      "Iteration: 180 of 241\ttrain_loss: 2.3591\n",
      "Iteration: 182 of 241\ttrain_loss: 2.8111\n",
      "Iteration: 184 of 241\ttrain_loss: 2.1324\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2730\n",
      "Iteration: 188 of 241\ttrain_loss: 2.3672\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1659\n",
      "Iteration: 192 of 241\ttrain_loss: 1.9987\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2363\n",
      "Iteration: 196 of 241\ttrain_loss: 2.1488\n",
      "Iteration: 198 of 241\ttrain_loss: 2.4122\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8154\n",
      "Iteration: 202 of 241\ttrain_loss: 2.3308\n",
      "Iteration: 204 of 241\ttrain_loss: 2.2498\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0530\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4656\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9093\n",
      "Iteration: 212 of 241\ttrain_loss: 1.9970\n",
      "Iteration: 214 of 241\ttrain_loss: 2.4027\n",
      "Iteration: 216 of 241\ttrain_loss: 2.1855\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2212\n",
      "Iteration: 220 of 241\ttrain_loss: 1.8355\n",
      "Iteration: 222 of 241\ttrain_loss: 2.5100\n",
      "Iteration: 224 of 241\ttrain_loss: 2.9208\n",
      "Iteration: 226 of 241\ttrain_loss: 2.1579\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2703\n",
      "Iteration: 230 of 241\ttrain_loss: 2.0087\n",
      "Iteration: 232 of 241\ttrain_loss: 2.2174\n",
      "Iteration: 234 of 241\ttrain_loss: 2.1626\n",
      "Iteration: 236 of 241\ttrain_loss: 2.3024\n",
      "Iteration: 238 of 241\ttrain_loss: 2.4856\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3662\n",
      "Iteration: 241 of 241\ttrain_loss: 2.1515\n",
      "Average Score for this Epoch: 2.1239013671875\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 66 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7649\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7992\n",
      "Iteration: 4 of 241\ttrain_loss: 2.5082\n",
      "Iteration: 6 of 241\ttrain_loss: 2.0641\n",
      "Iteration: 8 of 241\ttrain_loss: 1.9974\n",
      "Iteration: 10 of 241\ttrain_loss: 2.3574\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9597\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8891\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5113\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8843\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0014\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1078\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8626\n",
      "Iteration: 26 of 241\ttrain_loss: 1.9028\n",
      "Iteration: 28 of 241\ttrain_loss: 1.9883\n",
      "Iteration: 30 of 241\ttrain_loss: 2.1830\n",
      "Iteration: 32 of 241\ttrain_loss: 2.4538\n",
      "Iteration: 34 of 241\ttrain_loss: 1.9297\n",
      "Iteration: 36 of 241\ttrain_loss: 2.2831\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0733\n",
      "Iteration: 40 of 241\ttrain_loss: 2.0635\n",
      "Iteration: 42 of 241\ttrain_loss: 1.8025\n",
      "Iteration: 44 of 241\ttrain_loss: 1.8397\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8017\n",
      "Iteration: 48 of 241\ttrain_loss: 1.8021\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0904\n",
      "Iteration: 52 of 241\ttrain_loss: 2.1484\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0745\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7725\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7174\n",
      "Iteration: 60 of 241\ttrain_loss: 2.1791\n",
      "Iteration: 62 of 241\ttrain_loss: 1.9325\n",
      "Iteration: 64 of 241\ttrain_loss: 2.0787\n",
      "Iteration: 66 of 241\ttrain_loss: 2.0208\n",
      "Iteration: 68 of 241\ttrain_loss: 2.0498\n",
      "Iteration: 70 of 241\ttrain_loss: 1.8372\n",
      "Iteration: 72 of 241\ttrain_loss: 1.9072\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8974\n",
      "Iteration: 76 of 241\ttrain_loss: 1.7473\n",
      "Iteration: 78 of 241\ttrain_loss: 1.9545\n",
      "Iteration: 80 of 241\ttrain_loss: 2.1272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 82 of 241\ttrain_loss: 1.8851\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9102\n",
      "Iteration: 86 of 241\ttrain_loss: 1.9458\n",
      "Iteration: 88 of 241\ttrain_loss: 1.7703\n",
      "Iteration: 90 of 241\ttrain_loss: 1.9500\n",
      "Iteration: 92 of 241\ttrain_loss: 1.8315\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6718\n",
      "Iteration: 96 of 241\ttrain_loss: 2.2202\n",
      "Iteration: 98 of 241\ttrain_loss: 1.7499\n",
      "Iteration: 100 of 241\ttrain_loss: 2.5366\n",
      "Iteration: 102 of 241\ttrain_loss: 2.1911\n",
      "Iteration: 104 of 241\ttrain_loss: 2.0627\n",
      "Iteration: 106 of 241\ttrain_loss: 2.2411\n",
      "Iteration: 108 of 241\ttrain_loss: 2.1557\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1588\n",
      "Iteration: 112 of 241\ttrain_loss: 2.4780\n",
      "Iteration: 114 of 241\ttrain_loss: 2.1730\n",
      "Iteration: 116 of 241\ttrain_loss: 1.9176\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7773\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3837\n",
      "Iteration: 122 of 241\ttrain_loss: 2.1930\n",
      "Iteration: 124 of 241\ttrain_loss: 1.9251\n",
      "Iteration: 126 of 241\ttrain_loss: 2.1662\n",
      "Iteration: 128 of 241\ttrain_loss: 2.1840\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1968\n",
      "Iteration: 132 of 241\ttrain_loss: 2.1311\n",
      "Iteration: 134 of 241\ttrain_loss: 1.6818\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3279\n",
      "Iteration: 138 of 241\ttrain_loss: 2.1086\n",
      "Iteration: 140 of 241\ttrain_loss: 2.3327\n",
      "Iteration: 142 of 241\ttrain_loss: 2.2682\n",
      "Iteration: 144 of 241\ttrain_loss: 2.3483\n",
      "Iteration: 146 of 241\ttrain_loss: 2.5353\n",
      "Iteration: 148 of 241\ttrain_loss: 1.6263\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8959\n",
      "Iteration: 152 of 241\ttrain_loss: 1.7636\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1440\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4825\n",
      "Iteration: 158 of 241\ttrain_loss: 2.1872\n",
      "Iteration: 160 of 241\ttrain_loss: 2.1797\n",
      "Iteration: 162 of 241\ttrain_loss: 2.2554\n",
      "Iteration: 164 of 241\ttrain_loss: 2.3583\n",
      "Iteration: 166 of 241\ttrain_loss: 2.5350\n",
      "Iteration: 168 of 241\ttrain_loss: 2.0405\n",
      "Iteration: 170 of 241\ttrain_loss: 2.5554\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0421\n",
      "Iteration: 174 of 241\ttrain_loss: 2.2937\n",
      "Iteration: 176 of 241\ttrain_loss: 2.1265\n",
      "Iteration: 178 of 241\ttrain_loss: 2.0854\n",
      "Iteration: 180 of 241\ttrain_loss: 2.2952\n",
      "Iteration: 182 of 241\ttrain_loss: 1.9580\n",
      "Iteration: 184 of 241\ttrain_loss: 2.2804\n",
      "Iteration: 186 of 241\ttrain_loss: 2.0167\n",
      "Iteration: 188 of 241\ttrain_loss: 1.9789\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1524\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0412\n",
      "Iteration: 194 of 241\ttrain_loss: 2.4108\n",
      "Iteration: 196 of 241\ttrain_loss: 2.0578\n",
      "Iteration: 198 of 241\ttrain_loss: 1.8431\n",
      "Iteration: 200 of 241\ttrain_loss: 2.3988\n",
      "Iteration: 202 of 241\ttrain_loss: 1.9819\n",
      "Iteration: 204 of 241\ttrain_loss: 2.1748\n",
      "Iteration: 206 of 241\ttrain_loss: 2.2604\n",
      "Iteration: 208 of 241\ttrain_loss: 2.2544\n",
      "Iteration: 210 of 241\ttrain_loss: 2.5108\n",
      "Iteration: 212 of 241\ttrain_loss: 2.0776\n",
      "Iteration: 214 of 241\ttrain_loss: 2.2770\n",
      "Iteration: 216 of 241\ttrain_loss: 2.2219\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0933\n",
      "Iteration: 220 of 241\ttrain_loss: 1.8891\n",
      "Iteration: 222 of 241\ttrain_loss: 2.2559\n",
      "Iteration: 224 of 241\ttrain_loss: 1.9724\n",
      "Iteration: 226 of 241\ttrain_loss: 2.5420\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9104\n",
      "Iteration: 230 of 241\ttrain_loss: 1.9886\n",
      "Iteration: 232 of 241\ttrain_loss: 2.3405\n",
      "Iteration: 234 of 241\ttrain_loss: 2.4779\n",
      "Iteration: 236 of 241\ttrain_loss: 1.8689\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9868\n",
      "Iteration: 240 of 241\ttrain_loss: 2.5716\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9838\n",
      "Average Score for this Epoch: 2.0841166973114014\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 67 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.5558\n",
      "Iteration: 2 of 241\ttrain_loss: 2.1057\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8167\n",
      "Iteration: 6 of 241\ttrain_loss: 1.8610\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8985\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8990\n",
      "Iteration: 12 of 241\ttrain_loss: 2.1888\n",
      "Iteration: 14 of 241\ttrain_loss: 2.1719\n",
      "Iteration: 16 of 241\ttrain_loss: 1.6987\n",
      "Iteration: 18 of 241\ttrain_loss: 2.2954\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0349\n",
      "Iteration: 22 of 241\ttrain_loss: 2.0015\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0640\n",
      "Iteration: 26 of 241\ttrain_loss: 1.9335\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1632\n",
      "Iteration: 30 of 241\ttrain_loss: 2.1436\n",
      "Iteration: 32 of 241\ttrain_loss: 2.1203\n",
      "Iteration: 34 of 241\ttrain_loss: 2.3406\n",
      "Iteration: 36 of 241\ttrain_loss: 1.7221\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0473\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9284\n",
      "Iteration: 42 of 241\ttrain_loss: 2.3446\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1709\n",
      "Iteration: 46 of 241\ttrain_loss: 2.2472\n",
      "Iteration: 48 of 241\ttrain_loss: 1.7887\n",
      "Iteration: 50 of 241\ttrain_loss: 2.2043\n",
      "Iteration: 52 of 241\ttrain_loss: 2.2074\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9467\n",
      "Iteration: 56 of 241\ttrain_loss: 2.0781\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7806\n",
      "Iteration: 60 of 241\ttrain_loss: 1.6386\n",
      "Iteration: 62 of 241\ttrain_loss: 2.1759\n",
      "Iteration: 64 of 241\ttrain_loss: 1.6496\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1811\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9178\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0542\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8310\n",
      "Iteration: 74 of 241\ttrain_loss: 1.4890\n",
      "Iteration: 76 of 241\ttrain_loss: 2.1142\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1087\n",
      "Iteration: 80 of 241\ttrain_loss: 2.2074\n",
      "Iteration: 82 of 241\ttrain_loss: 2.1996\n",
      "Iteration: 84 of 241\ttrain_loss: 2.2336\n",
      "Iteration: 86 of 241\ttrain_loss: 2.1884\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9724\n",
      "Iteration: 90 of 241\ttrain_loss: 2.1885\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7575\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6480\n",
      "Iteration: 96 of 241\ttrain_loss: 2.8051\n",
      "Iteration: 98 of 241\ttrain_loss: 2.2299\n",
      "Iteration: 100 of 241\ttrain_loss: 2.2700\n",
      "Iteration: 102 of 241\ttrain_loss: 2.0239\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9853\n",
      "Iteration: 106 of 241\ttrain_loss: 1.7926\n",
      "Iteration: 108 of 241\ttrain_loss: 1.9059\n",
      "Iteration: 110 of 241\ttrain_loss: 2.3748\n",
      "Iteration: 112 of 241\ttrain_loss: 2.1373\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0530\n",
      "Iteration: 116 of 241\ttrain_loss: 2.2043\n",
      "Iteration: 118 of 241\ttrain_loss: 2.8220\n",
      "Iteration: 120 of 241\ttrain_loss: 2.1786\n",
      "Iteration: 122 of 241\ttrain_loss: 2.0031\n",
      "Iteration: 124 of 241\ttrain_loss: 1.9021\n",
      "Iteration: 126 of 241\ttrain_loss: 1.9310\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8128\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1805\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7478\n",
      "Iteration: 134 of 241\ttrain_loss: 1.9543\n",
      "Iteration: 136 of 241\ttrain_loss: 2.2250\n",
      "Iteration: 138 of 241\ttrain_loss: 1.8852\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2219\n",
      "Iteration: 142 of 241\ttrain_loss: 2.3118\n",
      "Iteration: 144 of 241\ttrain_loss: 1.8271\n",
      "Iteration: 146 of 241\ttrain_loss: 2.3195\n",
      "Iteration: 148 of 241\ttrain_loss: 2.4090\n",
      "Iteration: 150 of 241\ttrain_loss: 2.5026\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8232\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1841\n",
      "Iteration: 156 of 241\ttrain_loss: 2.2409\n",
      "Iteration: 158 of 241\ttrain_loss: 1.9174\n",
      "Iteration: 160 of 241\ttrain_loss: 2.3257\n",
      "Iteration: 162 of 241\ttrain_loss: 2.3331\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9853\n",
      "Iteration: 166 of 241\ttrain_loss: 2.2896\n",
      "Iteration: 168 of 241\ttrain_loss: 2.2197\n",
      "Iteration: 170 of 241\ttrain_loss: 1.8797\n",
      "Iteration: 172 of 241\ttrain_loss: 3.0050\n",
      "Iteration: 174 of 241\ttrain_loss: 2.1711\n",
      "Iteration: 176 of 241\ttrain_loss: 2.2240\n",
      "Iteration: 178 of 241\ttrain_loss: 1.5915\n",
      "Iteration: 180 of 241\ttrain_loss: 2.3179\n",
      "Iteration: 182 of 241\ttrain_loss: 2.3606\n",
      "Iteration: 184 of 241\ttrain_loss: 2.0657\n",
      "Iteration: 186 of 241\ttrain_loss: 1.8341\n",
      "Iteration: 188 of 241\ttrain_loss: 2.4145\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1186\n",
      "Iteration: 192 of 241\ttrain_loss: 1.7736\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2562\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8062\n",
      "Iteration: 198 of 241\ttrain_loss: 1.6914\n",
      "Iteration: 200 of 241\ttrain_loss: 2.1930\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8327\n",
      "Iteration: 204 of 241\ttrain_loss: 1.9245\n",
      "Iteration: 206 of 241\ttrain_loss: 1.7478\n",
      "Iteration: 208 of 241\ttrain_loss: 2.2741\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9422\n",
      "Iteration: 212 of 241\ttrain_loss: 2.7136\n",
      "Iteration: 214 of 241\ttrain_loss: 1.9633\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8894\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2766\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1456\n",
      "Iteration: 222 of 241\ttrain_loss: 1.7403\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2154\n",
      "Iteration: 226 of 241\ttrain_loss: 2.0374\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1351\n",
      "Iteration: 230 of 241\ttrain_loss: 2.4485\n",
      "Iteration: 232 of 241\ttrain_loss: 2.3204\n",
      "Iteration: 234 of 241\ttrain_loss: 2.1954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 236 of 241\ttrain_loss: 2.1905\n",
      "Iteration: 238 of 241\ttrain_loss: 2.0115\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3030\n",
      "Iteration: 241 of 241\ttrain_loss: 1.8431\n",
      "Average Score for this Epoch: 2.0701398849487305\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 68 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.8875\n",
      "Iteration: 2 of 241\ttrain_loss: 2.1152\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7231\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9113\n",
      "Iteration: 8 of 241\ttrain_loss: 1.9762\n",
      "Iteration: 10 of 241\ttrain_loss: 1.5118\n",
      "Iteration: 12 of 241\ttrain_loss: 2.3436\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7450\n",
      "Iteration: 16 of 241\ttrain_loss: 2.1256\n",
      "Iteration: 18 of 241\ttrain_loss: 1.9540\n",
      "Iteration: 20 of 241\ttrain_loss: 1.9170\n",
      "Iteration: 22 of 241\ttrain_loss: 1.9973\n",
      "Iteration: 24 of 241\ttrain_loss: 2.2930\n",
      "Iteration: 26 of 241\ttrain_loss: 2.1743\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1930\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9265\n",
      "Iteration: 32 of 241\ttrain_loss: 1.8820\n",
      "Iteration: 34 of 241\ttrain_loss: 1.5287\n",
      "Iteration: 36 of 241\ttrain_loss: 1.7820\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0868\n",
      "Iteration: 40 of 241\ttrain_loss: 1.4955\n",
      "Iteration: 42 of 241\ttrain_loss: 2.2458\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1064\n",
      "Iteration: 46 of 241\ttrain_loss: 2.1066\n",
      "Iteration: 48 of 241\ttrain_loss: 2.1945\n",
      "Iteration: 50 of 241\ttrain_loss: 2.1039\n",
      "Iteration: 52 of 241\ttrain_loss: 2.2382\n",
      "Iteration: 54 of 241\ttrain_loss: 2.2575\n",
      "Iteration: 56 of 241\ttrain_loss: 2.0044\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1697\n",
      "Iteration: 60 of 241\ttrain_loss: 2.3362\n",
      "Iteration: 62 of 241\ttrain_loss: 2.1431\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9174\n",
      "Iteration: 66 of 241\ttrain_loss: 2.5386\n",
      "Iteration: 68 of 241\ttrain_loss: 1.6565\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0927\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8435\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7252\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0377\n",
      "Iteration: 78 of 241\ttrain_loss: 2.2108\n",
      "Iteration: 80 of 241\ttrain_loss: 1.8564\n",
      "Iteration: 82 of 241\ttrain_loss: 2.2474\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0995\n",
      "Iteration: 86 of 241\ttrain_loss: 2.0778\n",
      "Iteration: 88 of 241\ttrain_loss: 2.0476\n",
      "Iteration: 90 of 241\ttrain_loss: 1.6684\n",
      "Iteration: 92 of 241\ttrain_loss: 2.2331\n",
      "Iteration: 94 of 241\ttrain_loss: 2.0520\n",
      "Iteration: 96 of 241\ttrain_loss: 2.2376\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9363\n",
      "Iteration: 100 of 241\ttrain_loss: 1.8543\n",
      "Iteration: 102 of 241\ttrain_loss: 2.3115\n",
      "Iteration: 104 of 241\ttrain_loss: 2.0286\n",
      "Iteration: 106 of 241\ttrain_loss: 2.0508\n",
      "Iteration: 108 of 241\ttrain_loss: 2.4052\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9968\n",
      "Iteration: 112 of 241\ttrain_loss: 2.4070\n",
      "Iteration: 114 of 241\ttrain_loss: 2.2687\n",
      "Iteration: 116 of 241\ttrain_loss: 2.0702\n",
      "Iteration: 118 of 241\ttrain_loss: 2.5453\n",
      "Iteration: 120 of 241\ttrain_loss: 2.0880\n",
      "Iteration: 122 of 241\ttrain_loss: 1.9877\n",
      "Iteration: 124 of 241\ttrain_loss: 2.4990\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8084\n",
      "Iteration: 128 of 241\ttrain_loss: 2.0639\n",
      "Iteration: 130 of 241\ttrain_loss: 1.9036\n",
      "Iteration: 132 of 241\ttrain_loss: 2.0050\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8260\n",
      "Iteration: 136 of 241\ttrain_loss: 2.3087\n",
      "Iteration: 138 of 241\ttrain_loss: 1.8743\n",
      "Iteration: 140 of 241\ttrain_loss: 1.9487\n",
      "Iteration: 142 of 241\ttrain_loss: 1.9977\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7616\n",
      "Iteration: 146 of 241\ttrain_loss: 2.1407\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9015\n",
      "Iteration: 150 of 241\ttrain_loss: 2.2592\n",
      "Iteration: 152 of 241\ttrain_loss: 1.7526\n",
      "Iteration: 154 of 241\ttrain_loss: 1.7796\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4720\n",
      "Iteration: 158 of 241\ttrain_loss: 2.0635\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6555\n",
      "Iteration: 162 of 241\ttrain_loss: 1.7788\n",
      "Iteration: 164 of 241\ttrain_loss: 2.6198\n",
      "Iteration: 166 of 241\ttrain_loss: 1.7912\n",
      "Iteration: 168 of 241\ttrain_loss: 1.8317\n",
      "Iteration: 170 of 241\ttrain_loss: 2.0887\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0985\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0711\n",
      "Iteration: 176 of 241\ttrain_loss: 2.1571\n",
      "Iteration: 178 of 241\ttrain_loss: 1.9981\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8426\n",
      "Iteration: 182 of 241\ttrain_loss: 2.4033\n",
      "Iteration: 184 of 241\ttrain_loss: 1.9230\n",
      "Iteration: 186 of 241\ttrain_loss: 2.0053\n",
      "Iteration: 188 of 241\ttrain_loss: 2.0862\n",
      "Iteration: 190 of 241\ttrain_loss: 1.5549\n",
      "Iteration: 192 of 241\ttrain_loss: 1.8112\n",
      "Iteration: 194 of 241\ttrain_loss: 1.7819\n",
      "Iteration: 196 of 241\ttrain_loss: 2.1460\n",
      "Iteration: 198 of 241\ttrain_loss: 2.3130\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8533\n",
      "Iteration: 202 of 241\ttrain_loss: 1.9473\n",
      "Iteration: 204 of 241\ttrain_loss: 2.3693\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0547\n",
      "Iteration: 208 of 241\ttrain_loss: 2.1627\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8335\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8870\n",
      "Iteration: 214 of 241\ttrain_loss: 2.2245\n",
      "Iteration: 216 of 241\ttrain_loss: 2.4094\n",
      "Iteration: 218 of 241\ttrain_loss: 2.3347\n",
      "Iteration: 220 of 241\ttrain_loss: 2.4130\n",
      "Iteration: 222 of 241\ttrain_loss: 2.0777\n",
      "Iteration: 224 of 241\ttrain_loss: 2.6324\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9917\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1009\n",
      "Iteration: 230 of 241\ttrain_loss: 1.7966\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8399\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2843\n",
      "Iteration: 236 of 241\ttrain_loss: 1.8644\n",
      "Iteration: 238 of 241\ttrain_loss: 2.2763\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8746\n",
      "Iteration: 241 of 241\ttrain_loss: 2.0862\n",
      "Average Score for this Epoch: 2.073953151702881\n",
      "-------------------- Epoch 69 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7007\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8531\n",
      "Iteration: 4 of 241\ttrain_loss: 1.6735\n",
      "Iteration: 6 of 241\ttrain_loss: 2.3343\n",
      "Iteration: 8 of 241\ttrain_loss: 2.2363\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6903\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9378\n",
      "Iteration: 14 of 241\ttrain_loss: 2.0736\n",
      "Iteration: 16 of 241\ttrain_loss: 1.9286\n",
      "Iteration: 18 of 241\ttrain_loss: 2.2796\n",
      "Iteration: 20 of 241\ttrain_loss: 1.8102\n",
      "Iteration: 22 of 241\ttrain_loss: 1.9719\n",
      "Iteration: 24 of 241\ttrain_loss: 2.1383\n",
      "Iteration: 26 of 241\ttrain_loss: 1.9504\n",
      "Iteration: 28 of 241\ttrain_loss: 1.8555\n",
      "Iteration: 30 of 241\ttrain_loss: 1.6785\n",
      "Iteration: 32 of 241\ttrain_loss: 1.8890\n",
      "Iteration: 34 of 241\ttrain_loss: 1.8750\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6477\n",
      "Iteration: 38 of 241\ttrain_loss: 2.1120\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9261\n",
      "Iteration: 42 of 241\ttrain_loss: 2.3078\n",
      "Iteration: 44 of 241\ttrain_loss: 2.0756\n",
      "Iteration: 46 of 241\ttrain_loss: 2.0007\n",
      "Iteration: 48 of 241\ttrain_loss: 2.3544\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0200\n",
      "Iteration: 52 of 241\ttrain_loss: 2.4487\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0286\n",
      "Iteration: 56 of 241\ttrain_loss: 2.0725\n",
      "Iteration: 58 of 241\ttrain_loss: 2.0564\n",
      "Iteration: 60 of 241\ttrain_loss: 2.0285\n",
      "Iteration: 62 of 241\ttrain_loss: 1.9079\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9691\n",
      "Iteration: 66 of 241\ttrain_loss: 1.8109\n",
      "Iteration: 68 of 241\ttrain_loss: 2.3724\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0056\n",
      "Iteration: 72 of 241\ttrain_loss: 2.0319\n",
      "Iteration: 74 of 241\ttrain_loss: 1.6310\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0242\n",
      "Iteration: 78 of 241\ttrain_loss: 2.0861\n",
      "Iteration: 80 of 241\ttrain_loss: 2.1342\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8717\n",
      "Iteration: 84 of 241\ttrain_loss: 2.1136\n",
      "Iteration: 86 of 241\ttrain_loss: 2.1642\n",
      "Iteration: 88 of 241\ttrain_loss: 1.6791\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0034\n",
      "Iteration: 92 of 241\ttrain_loss: 2.3181\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9111\n",
      "Iteration: 96 of 241\ttrain_loss: 2.0579\n",
      "Iteration: 98 of 241\ttrain_loss: 2.4021\n",
      "Iteration: 100 of 241\ttrain_loss: 1.7749\n",
      "Iteration: 102 of 241\ttrain_loss: 2.8173\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9368\n",
      "Iteration: 106 of 241\ttrain_loss: 2.1554\n",
      "Iteration: 108 of 241\ttrain_loss: 1.5758\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1689\n",
      "Iteration: 112 of 241\ttrain_loss: 2.1106\n",
      "Iteration: 114 of 241\ttrain_loss: 2.1977\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4306\n",
      "Iteration: 118 of 241\ttrain_loss: 1.5977\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8379\n",
      "Iteration: 122 of 241\ttrain_loss: 1.7464\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7645\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8505\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9344\n",
      "Iteration: 130 of 241\ttrain_loss: 1.9841\n",
      "Iteration: 132 of 241\ttrain_loss: 2.0024\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8680\n",
      "Iteration: 136 of 241\ttrain_loss: 1.7741\n",
      "Iteration: 138 of 241\ttrain_loss: 1.9736\n",
      "Iteration: 140 of 241\ttrain_loss: 1.6437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 142 of 241\ttrain_loss: 2.1375\n",
      "Iteration: 144 of 241\ttrain_loss: 1.8914\n",
      "Iteration: 146 of 241\ttrain_loss: 2.0723\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9395\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8595\n",
      "Iteration: 152 of 241\ttrain_loss: 2.3737\n",
      "Iteration: 154 of 241\ttrain_loss: 2.0774\n",
      "Iteration: 156 of 241\ttrain_loss: 1.6601\n",
      "Iteration: 158 of 241\ttrain_loss: 2.0788\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7534\n",
      "Iteration: 162 of 241\ttrain_loss: 2.0538\n",
      "Iteration: 164 of 241\ttrain_loss: 2.4317\n",
      "Iteration: 166 of 241\ttrain_loss: 1.7326\n",
      "Iteration: 168 of 241\ttrain_loss: 1.8661\n",
      "Iteration: 170 of 241\ttrain_loss: 2.2086\n",
      "Iteration: 172 of 241\ttrain_loss: 2.4272\n",
      "Iteration: 174 of 241\ttrain_loss: 1.9730\n",
      "Iteration: 176 of 241\ttrain_loss: 1.8459\n",
      "Iteration: 178 of 241\ttrain_loss: 1.8686\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6155\n",
      "Iteration: 182 of 241\ttrain_loss: 2.4095\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7166\n",
      "Iteration: 186 of 241\ttrain_loss: 2.0546\n",
      "Iteration: 188 of 241\ttrain_loss: 1.6285\n",
      "Iteration: 190 of 241\ttrain_loss: 2.4606\n",
      "Iteration: 192 of 241\ttrain_loss: 2.3028\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2156\n",
      "Iteration: 196 of 241\ttrain_loss: 2.5865\n",
      "Iteration: 198 of 241\ttrain_loss: 1.7773\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8499\n",
      "Iteration: 202 of 241\ttrain_loss: 2.2265\n",
      "Iteration: 204 of 241\ttrain_loss: 2.4965\n",
      "Iteration: 206 of 241\ttrain_loss: 2.1357\n",
      "Iteration: 208 of 241\ttrain_loss: 1.8823\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9377\n",
      "Iteration: 212 of 241\ttrain_loss: 2.3318\n",
      "Iteration: 214 of 241\ttrain_loss: 2.1401\n",
      "Iteration: 216 of 241\ttrain_loss: 1.7769\n",
      "Iteration: 218 of 241\ttrain_loss: 2.4525\n",
      "Iteration: 220 of 241\ttrain_loss: 2.6745\n",
      "Iteration: 222 of 241\ttrain_loss: 2.0566\n",
      "Iteration: 224 of 241\ttrain_loss: 1.9127\n",
      "Iteration: 226 of 241\ttrain_loss: 2.4350\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2218\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1248\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8236\n",
      "Iteration: 234 of 241\ttrain_loss: 1.4635\n",
      "Iteration: 236 of 241\ttrain_loss: 2.3299\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7293\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8013\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9805\n",
      "Average Score for this Epoch: 2.0274159908294678\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 70 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.7676\n",
      "Iteration: 2 of 241\ttrain_loss: 1.9788\n",
      "Iteration: 4 of 241\ttrain_loss: 2.1768\n",
      "Iteration: 6 of 241\ttrain_loss: 1.7446\n",
      "Iteration: 8 of 241\ttrain_loss: 2.2632\n",
      "Iteration: 10 of 241\ttrain_loss: 1.9181\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6932\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8843\n",
      "Iteration: 16 of 241\ttrain_loss: 1.6415\n",
      "Iteration: 18 of 241\ttrain_loss: 2.1278\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5729\n",
      "Iteration: 22 of 241\ttrain_loss: 1.7274\n",
      "Iteration: 24 of 241\ttrain_loss: 1.9514\n",
      "Iteration: 26 of 241\ttrain_loss: 1.9178\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7957\n",
      "Iteration: 30 of 241\ttrain_loss: 2.1444\n",
      "Iteration: 32 of 241\ttrain_loss: 1.9565\n",
      "Iteration: 34 of 241\ttrain_loss: 1.9676\n",
      "Iteration: 36 of 241\ttrain_loss: 1.7390\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0709\n",
      "Iteration: 40 of 241\ttrain_loss: 2.4176\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6699\n",
      "Iteration: 44 of 241\ttrain_loss: 2.0534\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9730\n",
      "Iteration: 48 of 241\ttrain_loss: 2.1879\n",
      "Iteration: 50 of 241\ttrain_loss: 2.2317\n",
      "Iteration: 52 of 241\ttrain_loss: 1.7041\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8998\n",
      "Iteration: 56 of 241\ttrain_loss: 2.1128\n",
      "Iteration: 58 of 241\ttrain_loss: 1.9943\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8410\n",
      "Iteration: 62 of 241\ttrain_loss: 2.2589\n",
      "Iteration: 64 of 241\ttrain_loss: 2.4975\n",
      "Iteration: 66 of 241\ttrain_loss: 1.7074\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9270\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6368\n",
      "Iteration: 72 of 241\ttrain_loss: 1.6714\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8860\n",
      "Iteration: 76 of 241\ttrain_loss: 1.5495\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8762\n",
      "Iteration: 80 of 241\ttrain_loss: 1.7709\n",
      "Iteration: 82 of 241\ttrain_loss: 1.7384\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0098\n",
      "Iteration: 86 of 241\ttrain_loss: 2.0728\n",
      "Iteration: 88 of 241\ttrain_loss: 1.7656\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7376\n",
      "Iteration: 92 of 241\ttrain_loss: 2.2607\n",
      "Iteration: 94 of 241\ttrain_loss: 2.0996\n",
      "Iteration: 96 of 241\ttrain_loss: 1.6683\n",
      "Iteration: 98 of 241\ttrain_loss: 1.8599\n",
      "Iteration: 100 of 241\ttrain_loss: 1.4148\n",
      "Iteration: 102 of 241\ttrain_loss: 2.2369\n",
      "Iteration: 104 of 241\ttrain_loss: 2.1213\n",
      "Iteration: 106 of 241\ttrain_loss: 2.3060\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7797\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9914\n",
      "Iteration: 112 of 241\ttrain_loss: 1.4088\n",
      "Iteration: 114 of 241\ttrain_loss: 1.8983\n",
      "Iteration: 116 of 241\ttrain_loss: 2.0287\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7908\n",
      "Iteration: 120 of 241\ttrain_loss: 2.3092\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2553\n",
      "Iteration: 124 of 241\ttrain_loss: 1.6134\n",
      "Iteration: 126 of 241\ttrain_loss: 2.6990\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8616\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7708\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7962\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7599\n",
      "Iteration: 136 of 241\ttrain_loss: 2.0119\n",
      "Iteration: 138 of 241\ttrain_loss: 1.9656\n",
      "Iteration: 140 of 241\ttrain_loss: 2.1477\n",
      "Iteration: 142 of 241\ttrain_loss: 2.1231\n",
      "Iteration: 144 of 241\ttrain_loss: 2.5647\n",
      "Iteration: 146 of 241\ttrain_loss: 2.1780\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7459\n",
      "Iteration: 150 of 241\ttrain_loss: 1.6771\n",
      "Iteration: 152 of 241\ttrain_loss: 2.2731\n",
      "Iteration: 154 of 241\ttrain_loss: 2.3782\n",
      "Iteration: 156 of 241\ttrain_loss: 1.8783\n",
      "Iteration: 158 of 241\ttrain_loss: 2.1517\n",
      "Iteration: 160 of 241\ttrain_loss: 2.1144\n",
      "Iteration: 162 of 241\ttrain_loss: 2.1100\n",
      "Iteration: 164 of 241\ttrain_loss: 2.0397\n",
      "Iteration: 166 of 241\ttrain_loss: 2.3414\n",
      "Iteration: 168 of 241\ttrain_loss: 2.0445\n",
      "Iteration: 170 of 241\ttrain_loss: 2.2367\n",
      "Iteration: 172 of 241\ttrain_loss: 2.1148\n",
      "Iteration: 174 of 241\ttrain_loss: 1.7208\n",
      "Iteration: 176 of 241\ttrain_loss: 2.1108\n",
      "Iteration: 178 of 241\ttrain_loss: 2.2534\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6779\n",
      "Iteration: 182 of 241\ttrain_loss: 1.9838\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7898\n",
      "Iteration: 186 of 241\ttrain_loss: 2.1302\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8478\n",
      "Iteration: 190 of 241\ttrain_loss: 2.5193\n",
      "Iteration: 192 of 241\ttrain_loss: 2.3898\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2566\n",
      "Iteration: 196 of 241\ttrain_loss: 2.4220\n",
      "Iteration: 198 of 241\ttrain_loss: 2.8239\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8359\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7084\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6980\n",
      "Iteration: 206 of 241\ttrain_loss: 1.9729\n",
      "Iteration: 208 of 241\ttrain_loss: 2.4766\n",
      "Iteration: 210 of 241\ttrain_loss: 2.0067\n",
      "Iteration: 212 of 241\ttrain_loss: 2.1434\n",
      "Iteration: 214 of 241\ttrain_loss: 1.9848\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9460\n",
      "Iteration: 218 of 241\ttrain_loss: 2.1674\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1267\n",
      "Iteration: 222 of 241\ttrain_loss: 1.6763\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2455\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9724\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8506\n",
      "Iteration: 230 of 241\ttrain_loss: 1.5590\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7645\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2578\n",
      "Iteration: 236 of 241\ttrain_loss: 2.2698\n",
      "Iteration: 238 of 241\ttrain_loss: 2.0544\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3908\n",
      "Iteration: 241 of 241\ttrain_loss: 2.3027\n",
      "Average Score for this Epoch: 1.9854962825775146\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 71 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.1486\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7360\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7513\n",
      "Iteration: 6 of 241\ttrain_loss: 2.0522\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8043\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8520\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6633\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7961\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7718\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8408\n",
      "Iteration: 20 of 241\ttrain_loss: 1.3355\n",
      "Iteration: 22 of 241\ttrain_loss: 2.1538\n",
      "Iteration: 24 of 241\ttrain_loss: 1.6025\n",
      "Iteration: 26 of 241\ttrain_loss: 2.1404\n",
      "Iteration: 28 of 241\ttrain_loss: 1.9408\n",
      "Iteration: 30 of 241\ttrain_loss: 1.7939\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0074\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1337\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0799\n",
      "Iteration: 38 of 241\ttrain_loss: 1.9585\n",
      "Iteration: 40 of 241\ttrain_loss: 2.0317\n",
      "Iteration: 42 of 241\ttrain_loss: 1.4300\n",
      "Iteration: 44 of 241\ttrain_loss: 1.7503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 46 of 241\ttrain_loss: 2.0939\n",
      "Iteration: 48 of 241\ttrain_loss: 1.6988\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8474\n",
      "Iteration: 52 of 241\ttrain_loss: 1.6626\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0892\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7907\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1553\n",
      "Iteration: 60 of 241\ttrain_loss: 2.1274\n",
      "Iteration: 62 of 241\ttrain_loss: 2.1244\n",
      "Iteration: 64 of 241\ttrain_loss: 2.3189\n",
      "Iteration: 66 of 241\ttrain_loss: 2.2205\n",
      "Iteration: 68 of 241\ttrain_loss: 1.5435\n",
      "Iteration: 70 of 241\ttrain_loss: 1.9124\n",
      "Iteration: 72 of 241\ttrain_loss: 1.3422\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8852\n",
      "Iteration: 76 of 241\ttrain_loss: 1.9644\n",
      "Iteration: 78 of 241\ttrain_loss: 1.9642\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6373\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5120\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0998\n",
      "Iteration: 86 of 241\ttrain_loss: 2.1126\n",
      "Iteration: 88 of 241\ttrain_loss: 2.0103\n",
      "Iteration: 90 of 241\ttrain_loss: 1.6654\n",
      "Iteration: 92 of 241\ttrain_loss: 1.9812\n",
      "Iteration: 94 of 241\ttrain_loss: 1.7115\n",
      "Iteration: 96 of 241\ttrain_loss: 2.0069\n",
      "Iteration: 98 of 241\ttrain_loss: 2.4341\n",
      "Iteration: 100 of 241\ttrain_loss: 1.5193\n",
      "Iteration: 102 of 241\ttrain_loss: 2.3058\n",
      "Iteration: 104 of 241\ttrain_loss: 2.0370\n",
      "Iteration: 106 of 241\ttrain_loss: 2.0946\n",
      "Iteration: 108 of 241\ttrain_loss: 1.8591\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1154\n",
      "Iteration: 112 of 241\ttrain_loss: 1.8670\n",
      "Iteration: 114 of 241\ttrain_loss: 1.9316\n",
      "Iteration: 116 of 241\ttrain_loss: 2.1853\n",
      "Iteration: 118 of 241\ttrain_loss: 1.9901\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8536\n",
      "Iteration: 122 of 241\ttrain_loss: 2.5221\n",
      "Iteration: 124 of 241\ttrain_loss: 2.1065\n",
      "Iteration: 126 of 241\ttrain_loss: 1.9144\n",
      "Iteration: 128 of 241\ttrain_loss: 2.3795\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1545\n",
      "Iteration: 132 of 241\ttrain_loss: 1.3401\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0156\n",
      "Iteration: 136 of 241\ttrain_loss: 2.0804\n",
      "Iteration: 138 of 241\ttrain_loss: 1.8170\n",
      "Iteration: 140 of 241\ttrain_loss: 2.4548\n",
      "Iteration: 142 of 241\ttrain_loss: 1.7393\n",
      "Iteration: 144 of 241\ttrain_loss: 2.0751\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9867\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9429\n",
      "Iteration: 150 of 241\ttrain_loss: 1.9019\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8125\n",
      "Iteration: 154 of 241\ttrain_loss: 2.3210\n",
      "Iteration: 156 of 241\ttrain_loss: 2.2105\n",
      "Iteration: 158 of 241\ttrain_loss: 1.9344\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7172\n",
      "Iteration: 162 of 241\ttrain_loss: 2.3977\n",
      "Iteration: 164 of 241\ttrain_loss: 2.7493\n",
      "Iteration: 166 of 241\ttrain_loss: 2.0746\n",
      "Iteration: 168 of 241\ttrain_loss: 1.8306\n",
      "Iteration: 170 of 241\ttrain_loss: 1.9605\n",
      "Iteration: 172 of 241\ttrain_loss: 2.1661\n",
      "Iteration: 174 of 241\ttrain_loss: 1.8125\n",
      "Iteration: 176 of 241\ttrain_loss: 2.2075\n",
      "Iteration: 178 of 241\ttrain_loss: 1.8207\n",
      "Iteration: 180 of 241\ttrain_loss: 2.2344\n",
      "Iteration: 182 of 241\ttrain_loss: 2.3999\n",
      "Iteration: 184 of 241\ttrain_loss: 1.9745\n",
      "Iteration: 186 of 241\ttrain_loss: 2.5245\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8720\n",
      "Iteration: 190 of 241\ttrain_loss: 1.9682\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1237\n",
      "Iteration: 194 of 241\ttrain_loss: 2.2081\n",
      "Iteration: 196 of 241\ttrain_loss: 1.5911\n",
      "Iteration: 198 of 241\ttrain_loss: 2.0827\n",
      "Iteration: 200 of 241\ttrain_loss: 1.7371\n",
      "Iteration: 202 of 241\ttrain_loss: 1.9212\n",
      "Iteration: 204 of 241\ttrain_loss: 2.4149\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0948\n",
      "Iteration: 208 of 241\ttrain_loss: 2.0172\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8629\n",
      "Iteration: 212 of 241\ttrain_loss: 2.0354\n",
      "Iteration: 214 of 241\ttrain_loss: 2.0766\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8640\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2733\n",
      "Iteration: 220 of 241\ttrain_loss: 1.9243\n",
      "Iteration: 222 of 241\ttrain_loss: 1.4801\n",
      "Iteration: 224 of 241\ttrain_loss: 1.9509\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9256\n",
      "Iteration: 228 of 241\ttrain_loss: 1.6273\n",
      "Iteration: 230 of 241\ttrain_loss: 2.2155\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1908\n",
      "Iteration: 234 of 241\ttrain_loss: 1.9809\n",
      "Iteration: 236 of 241\ttrain_loss: 2.0972\n",
      "Iteration: 238 of 241\ttrain_loss: 2.4263\n",
      "Iteration: 240 of 241\ttrain_loss: 2.0334\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9513\n",
      "Average Score for this Epoch: 1.969833493232727\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 72 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9617\n",
      "Iteration: 2 of 241\ttrain_loss: 2.2805\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8110\n",
      "Iteration: 6 of 241\ttrain_loss: 2.2074\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8261\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6923\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9412\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9557\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7764\n",
      "Iteration: 18 of 241\ttrain_loss: 1.5224\n",
      "Iteration: 20 of 241\ttrain_loss: 1.8297\n",
      "Iteration: 22 of 241\ttrain_loss: 1.9743\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8185\n",
      "Iteration: 26 of 241\ttrain_loss: 1.4749\n",
      "Iteration: 28 of 241\ttrain_loss: 1.6423\n",
      "Iteration: 30 of 241\ttrain_loss: 1.6718\n",
      "Iteration: 32 of 241\ttrain_loss: 1.8368\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1343\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0116\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0643\n",
      "Iteration: 40 of 241\ttrain_loss: 1.8206\n",
      "Iteration: 42 of 241\ttrain_loss: 1.9520\n",
      "Iteration: 44 of 241\ttrain_loss: 1.8570\n",
      "Iteration: 46 of 241\ttrain_loss: 1.2184\n",
      "Iteration: 48 of 241\ttrain_loss: 2.0363\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0314\n",
      "Iteration: 52 of 241\ttrain_loss: 2.6715\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9956\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7197\n",
      "Iteration: 58 of 241\ttrain_loss: 1.9228\n",
      "Iteration: 60 of 241\ttrain_loss: 1.9552\n",
      "Iteration: 62 of 241\ttrain_loss: 1.8811\n",
      "Iteration: 64 of 241\ttrain_loss: 2.3841\n",
      "Iteration: 66 of 241\ttrain_loss: 2.2720\n",
      "Iteration: 68 of 241\ttrain_loss: 2.3181\n",
      "Iteration: 70 of 241\ttrain_loss: 2.1119\n",
      "Iteration: 72 of 241\ttrain_loss: 2.2213\n",
      "Iteration: 74 of 241\ttrain_loss: 1.9925\n",
      "Iteration: 76 of 241\ttrain_loss: 2.1596\n",
      "Iteration: 78 of 241\ttrain_loss: 2.0083\n",
      "Iteration: 80 of 241\ttrain_loss: 2.0733\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0352\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9546\n",
      "Iteration: 86 of 241\ttrain_loss: 2.1903\n",
      "Iteration: 88 of 241\ttrain_loss: 2.1317\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7586\n",
      "Iteration: 92 of 241\ttrain_loss: 1.8915\n",
      "Iteration: 94 of 241\ttrain_loss: 2.3657\n",
      "Iteration: 96 of 241\ttrain_loss: 2.1849\n",
      "Iteration: 98 of 241\ttrain_loss: 1.6303\n",
      "Iteration: 100 of 241\ttrain_loss: 1.9924\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9812\n",
      "Iteration: 104 of 241\ttrain_loss: 2.1031\n",
      "Iteration: 106 of 241\ttrain_loss: 2.0279\n",
      "Iteration: 108 of 241\ttrain_loss: 1.8635\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8976\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5531\n",
      "Iteration: 114 of 241\ttrain_loss: 1.7238\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5435\n",
      "Iteration: 118 of 241\ttrain_loss: 2.1094\n",
      "Iteration: 120 of 241\ttrain_loss: 1.9372\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5420\n",
      "Iteration: 124 of 241\ttrain_loss: 2.2975\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8217\n",
      "Iteration: 128 of 241\ttrain_loss: 1.7736\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7452\n",
      "Iteration: 132 of 241\ttrain_loss: 2.0180\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0242\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9268\n",
      "Iteration: 138 of 241\ttrain_loss: 2.1104\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0827\n",
      "Iteration: 142 of 241\ttrain_loss: 2.0391\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7907\n",
      "Iteration: 146 of 241\ttrain_loss: 1.8544\n",
      "Iteration: 148 of 241\ttrain_loss: 2.0904\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8173\n",
      "Iteration: 152 of 241\ttrain_loss: 1.4834\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1940\n",
      "Iteration: 156 of 241\ttrain_loss: 2.0941\n",
      "Iteration: 158 of 241\ttrain_loss: 2.0751\n",
      "Iteration: 160 of 241\ttrain_loss: 2.0043\n",
      "Iteration: 162 of 241\ttrain_loss: 1.8644\n",
      "Iteration: 164 of 241\ttrain_loss: 2.3145\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1711\n",
      "Iteration: 168 of 241\ttrain_loss: 1.8296\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4772\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7686\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0784\n",
      "Iteration: 176 of 241\ttrain_loss: 2.0950\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1596\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6586\n",
      "Iteration: 182 of 241\ttrain_loss: 2.2606\n",
      "Iteration: 184 of 241\ttrain_loss: 1.9794\n",
      "Iteration: 186 of 241\ttrain_loss: 2.1075\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8681\n",
      "Iteration: 190 of 241\ttrain_loss: 2.0189\n",
      "Iteration: 192 of 241\ttrain_loss: 1.6533\n",
      "Iteration: 194 of 241\ttrain_loss: 2.0030\n",
      "Iteration: 196 of 241\ttrain_loss: 1.9375\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 of 241\ttrain_loss: 1.6363\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8697\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0275\n",
      "Iteration: 206 of 241\ttrain_loss: 1.8414\n",
      "Iteration: 208 of 241\ttrain_loss: 2.7924\n",
      "Iteration: 210 of 241\ttrain_loss: 1.9207\n",
      "Iteration: 212 of 241\ttrain_loss: 2.2179\n",
      "Iteration: 214 of 241\ttrain_loss: 1.5206\n",
      "Iteration: 216 of 241\ttrain_loss: 1.7380\n",
      "Iteration: 218 of 241\ttrain_loss: 1.4546\n",
      "Iteration: 220 of 241\ttrain_loss: 1.7661\n",
      "Iteration: 222 of 241\ttrain_loss: 2.1760\n",
      "Iteration: 224 of 241\ttrain_loss: 1.8024\n",
      "Iteration: 226 of 241\ttrain_loss: 2.3364\n",
      "Iteration: 228 of 241\ttrain_loss: 2.0825\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8786\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1628\n",
      "Iteration: 234 of 241\ttrain_loss: 1.5485\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9904\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7257\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8329\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9221\n",
      "Average Score for this Epoch: 1.9579098224639893\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 73 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7332\n",
      "Iteration: 2 of 241\ttrain_loss: 1.6661\n",
      "Iteration: 4 of 241\ttrain_loss: 1.6194\n",
      "Iteration: 6 of 241\ttrain_loss: 2.0332\n",
      "Iteration: 8 of 241\ttrain_loss: 1.2989\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6230\n",
      "Iteration: 12 of 241\ttrain_loss: 1.3594\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8906\n",
      "Iteration: 16 of 241\ttrain_loss: 1.8007\n",
      "Iteration: 18 of 241\ttrain_loss: 1.5194\n",
      "Iteration: 20 of 241\ttrain_loss: 1.4794\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8113\n",
      "Iteration: 24 of 241\ttrain_loss: 1.7144\n",
      "Iteration: 26 of 241\ttrain_loss: 1.7050\n",
      "Iteration: 28 of 241\ttrain_loss: 1.6888\n",
      "Iteration: 30 of 241\ttrain_loss: 2.0018\n",
      "Iteration: 32 of 241\ttrain_loss: 1.9574\n",
      "Iteration: 34 of 241\ttrain_loss: 1.5268\n",
      "Iteration: 36 of 241\ttrain_loss: 1.4204\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0713\n",
      "Iteration: 40 of 241\ttrain_loss: 1.7679\n",
      "Iteration: 42 of 241\ttrain_loss: 2.1469\n",
      "Iteration: 44 of 241\ttrain_loss: 1.9214\n",
      "Iteration: 46 of 241\ttrain_loss: 1.6185\n",
      "Iteration: 48 of 241\ttrain_loss: 1.9305\n",
      "Iteration: 50 of 241\ttrain_loss: 1.6496\n",
      "Iteration: 52 of 241\ttrain_loss: 1.7074\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8854\n",
      "Iteration: 56 of 241\ttrain_loss: 2.3078\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1272\n",
      "Iteration: 60 of 241\ttrain_loss: 1.4865\n",
      "Iteration: 62 of 241\ttrain_loss: 2.0941\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8470\n",
      "Iteration: 66 of 241\ttrain_loss: 1.8817\n",
      "Iteration: 68 of 241\ttrain_loss: 2.1409\n",
      "Iteration: 70 of 241\ttrain_loss: 1.8665\n",
      "Iteration: 72 of 241\ttrain_loss: 2.1383\n",
      "Iteration: 74 of 241\ttrain_loss: 2.3540\n",
      "Iteration: 76 of 241\ttrain_loss: 1.6711\n",
      "Iteration: 78 of 241\ttrain_loss: 2.4890\n",
      "Iteration: 80 of 241\ttrain_loss: 1.9518\n",
      "Iteration: 82 of 241\ttrain_loss: 1.4698\n",
      "Iteration: 84 of 241\ttrain_loss: 2.2416\n",
      "Iteration: 86 of 241\ttrain_loss: 2.2899\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9908\n",
      "Iteration: 90 of 241\ttrain_loss: 2.1146\n",
      "Iteration: 92 of 241\ttrain_loss: 2.2310\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9576\n",
      "Iteration: 96 of 241\ttrain_loss: 1.7179\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9164\n",
      "Iteration: 100 of 241\ttrain_loss: 2.2688\n",
      "Iteration: 102 of 241\ttrain_loss: 2.0859\n",
      "Iteration: 104 of 241\ttrain_loss: 1.6156\n",
      "Iteration: 106 of 241\ttrain_loss: 1.9309\n",
      "Iteration: 108 of 241\ttrain_loss: 2.1900\n",
      "Iteration: 110 of 241\ttrain_loss: 1.6896\n",
      "Iteration: 112 of 241\ttrain_loss: 2.0604\n",
      "Iteration: 114 of 241\ttrain_loss: 1.9310\n",
      "Iteration: 116 of 241\ttrain_loss: 2.9572\n",
      "Iteration: 118 of 241\ttrain_loss: 1.9879\n",
      "Iteration: 120 of 241\ttrain_loss: 1.6674\n",
      "Iteration: 122 of 241\ttrain_loss: 1.6322\n",
      "Iteration: 124 of 241\ttrain_loss: 2.2703\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8328\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9152\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7840\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7584\n",
      "Iteration: 134 of 241\ttrain_loss: 2.1136\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9839\n",
      "Iteration: 138 of 241\ttrain_loss: 1.9547\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2166\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6754\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7367\n",
      "Iteration: 146 of 241\ttrain_loss: 2.0385\n",
      "Iteration: 148 of 241\ttrain_loss: 2.0064\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8964\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6987\n",
      "Iteration: 154 of 241\ttrain_loss: 1.9531\n",
      "Iteration: 156 of 241\ttrain_loss: 1.6597\n",
      "Iteration: 158 of 241\ttrain_loss: 1.9255\n",
      "Iteration: 160 of 241\ttrain_loss: 1.8032\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9728\n",
      "Iteration: 164 of 241\ttrain_loss: 1.3841\n",
      "Iteration: 166 of 241\ttrain_loss: 1.9564\n",
      "Iteration: 168 of 241\ttrain_loss: 1.6288\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7396\n",
      "Iteration: 172 of 241\ttrain_loss: 1.8520\n",
      "Iteration: 174 of 241\ttrain_loss: 1.4481\n",
      "Iteration: 176 of 241\ttrain_loss: 1.9504\n",
      "Iteration: 178 of 241\ttrain_loss: 1.9337\n",
      "Iteration: 180 of 241\ttrain_loss: 2.0119\n",
      "Iteration: 182 of 241\ttrain_loss: 1.4311\n",
      "Iteration: 184 of 241\ttrain_loss: 1.6525\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6029\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8769\n",
      "Iteration: 190 of 241\ttrain_loss: 1.9370\n",
      "Iteration: 192 of 241\ttrain_loss: 1.7363\n",
      "Iteration: 194 of 241\ttrain_loss: 2.0210\n",
      "Iteration: 196 of 241\ttrain_loss: 1.9648\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1392\n",
      "Iteration: 200 of 241\ttrain_loss: 2.0017\n",
      "Iteration: 202 of 241\ttrain_loss: 2.2256\n",
      "Iteration: 204 of 241\ttrain_loss: 1.5301\n",
      "Iteration: 206 of 241\ttrain_loss: 1.7874\n",
      "Iteration: 208 of 241\ttrain_loss: 2.0021\n",
      "Iteration: 210 of 241\ttrain_loss: 2.0796\n",
      "Iteration: 212 of 241\ttrain_loss: 2.4412\n",
      "Iteration: 214 of 241\ttrain_loss: 2.7908\n",
      "Iteration: 216 of 241\ttrain_loss: 2.2469\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0694\n",
      "Iteration: 220 of 241\ttrain_loss: 1.9062\n",
      "Iteration: 222 of 241\ttrain_loss: 1.6071\n",
      "Iteration: 224 of 241\ttrain_loss: 2.0666\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9955\n",
      "Iteration: 228 of 241\ttrain_loss: 1.7979\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8168\n",
      "Iteration: 232 of 241\ttrain_loss: 2.1473\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2833\n",
      "Iteration: 236 of 241\ttrain_loss: 2.1187\n",
      "Iteration: 238 of 241\ttrain_loss: 2.1252\n",
      "Iteration: 240 of 241\ttrain_loss: 1.9390\n",
      "Iteration: 241 of 241\ttrain_loss: 2.0532\n",
      "Average Score for this Epoch: 1.9349416494369507\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 74 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7830\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8300\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7269\n",
      "Iteration: 6 of 241\ttrain_loss: 1.2272\n",
      "Iteration: 8 of 241\ttrain_loss: 1.5892\n",
      "Iteration: 10 of 241\ttrain_loss: 1.7971\n",
      "Iteration: 12 of 241\ttrain_loss: 1.8544\n",
      "Iteration: 14 of 241\ttrain_loss: 1.6776\n",
      "Iteration: 16 of 241\ttrain_loss: 1.6373\n",
      "Iteration: 18 of 241\ttrain_loss: 1.9449\n",
      "Iteration: 20 of 241\ttrain_loss: 1.7134\n",
      "Iteration: 22 of 241\ttrain_loss: 1.9169\n",
      "Iteration: 24 of 241\ttrain_loss: 2.1914\n",
      "Iteration: 26 of 241\ttrain_loss: 2.0130\n",
      "Iteration: 28 of 241\ttrain_loss: 2.3797\n",
      "Iteration: 30 of 241\ttrain_loss: 1.8825\n",
      "Iteration: 32 of 241\ttrain_loss: 1.7333\n",
      "Iteration: 34 of 241\ttrain_loss: 2.2222\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9245\n",
      "Iteration: 38 of 241\ttrain_loss: 2.0374\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9578\n",
      "Iteration: 42 of 241\ttrain_loss: 1.8622\n",
      "Iteration: 44 of 241\ttrain_loss: 2.0642\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8116\n",
      "Iteration: 48 of 241\ttrain_loss: 1.9653\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0466\n",
      "Iteration: 52 of 241\ttrain_loss: 1.8409\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9043\n",
      "Iteration: 56 of 241\ttrain_loss: 1.3929\n",
      "Iteration: 58 of 241\ttrain_loss: 2.1202\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8734\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7708\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9704\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6559\n",
      "Iteration: 68 of 241\ttrain_loss: 1.8624\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0538\n",
      "Iteration: 72 of 241\ttrain_loss: 1.6611\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7327\n",
      "Iteration: 76 of 241\ttrain_loss: 1.5261\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8306\n",
      "Iteration: 80 of 241\ttrain_loss: 1.8332\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0802\n",
      "Iteration: 84 of 241\ttrain_loss: 1.5962\n",
      "Iteration: 86 of 241\ttrain_loss: 2.1238\n",
      "Iteration: 88 of 241\ttrain_loss: 1.8815\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0631\n",
      "Iteration: 92 of 241\ttrain_loss: 1.9376\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9496\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5255\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9963\n",
      "Iteration: 100 of 241\ttrain_loss: 2.0918\n",
      "Iteration: 102 of 241\ttrain_loss: 1.6672\n",
      "Iteration: 104 of 241\ttrain_loss: 2.0987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 106 of 241\ttrain_loss: 2.0877\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7444\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9387\n",
      "Iteration: 112 of 241\ttrain_loss: 1.2635\n",
      "Iteration: 114 of 241\ttrain_loss: 2.1301\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5548\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6562\n",
      "Iteration: 120 of 241\ttrain_loss: 2.0011\n",
      "Iteration: 122 of 241\ttrain_loss: 1.7459\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5523\n",
      "Iteration: 126 of 241\ttrain_loss: 1.3537\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8722\n",
      "Iteration: 130 of 241\ttrain_loss: 1.9266\n",
      "Iteration: 132 of 241\ttrain_loss: 1.8571\n",
      "Iteration: 134 of 241\ttrain_loss: 1.4632\n",
      "Iteration: 136 of 241\ttrain_loss: 2.1029\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6505\n",
      "Iteration: 140 of 241\ttrain_loss: 1.9219\n",
      "Iteration: 142 of 241\ttrain_loss: 1.4487\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6894\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9392\n",
      "Iteration: 148 of 241\ttrain_loss: 1.6470\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8049\n",
      "Iteration: 152 of 241\ttrain_loss: 2.1932\n",
      "Iteration: 154 of 241\ttrain_loss: 1.6320\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7612\n",
      "Iteration: 158 of 241\ttrain_loss: 1.8568\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6024\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9235\n",
      "Iteration: 164 of 241\ttrain_loss: 1.8176\n",
      "Iteration: 166 of 241\ttrain_loss: 1.7040\n",
      "Iteration: 168 of 241\ttrain_loss: 2.1768\n",
      "Iteration: 170 of 241\ttrain_loss: 2.5566\n",
      "Iteration: 172 of 241\ttrain_loss: 1.5764\n",
      "Iteration: 174 of 241\ttrain_loss: 1.8455\n",
      "Iteration: 176 of 241\ttrain_loss: 1.9650\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1346\n",
      "Iteration: 180 of 241\ttrain_loss: 1.7342\n",
      "Iteration: 182 of 241\ttrain_loss: 2.1663\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7473\n",
      "Iteration: 186 of 241\ttrain_loss: 1.9577\n",
      "Iteration: 188 of 241\ttrain_loss: 1.6900\n",
      "Iteration: 190 of 241\ttrain_loss: 1.8571\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0011\n",
      "Iteration: 194 of 241\ttrain_loss: 1.9158\n",
      "Iteration: 196 of 241\ttrain_loss: 1.7486\n",
      "Iteration: 198 of 241\ttrain_loss: 2.0107\n",
      "Iteration: 200 of 241\ttrain_loss: 1.7459\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7919\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0665\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0334\n",
      "Iteration: 208 of 241\ttrain_loss: 2.1106\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8038\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6617\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7041\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9469\n",
      "Iteration: 218 of 241\ttrain_loss: 2.2688\n",
      "Iteration: 220 of 241\ttrain_loss: 2.2893\n",
      "Iteration: 222 of 241\ttrain_loss: 2.2009\n",
      "Iteration: 224 of 241\ttrain_loss: 2.1943\n",
      "Iteration: 226 of 241\ttrain_loss: 2.1327\n",
      "Iteration: 228 of 241\ttrain_loss: 1.6349\n",
      "Iteration: 230 of 241\ttrain_loss: 2.6416\n",
      "Iteration: 232 of 241\ttrain_loss: 2.3162\n",
      "Iteration: 234 of 241\ttrain_loss: 2.0192\n",
      "Iteration: 236 of 241\ttrain_loss: 1.8488\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9890\n",
      "Iteration: 240 of 241\ttrain_loss: 2.3406\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9453\n",
      "Average Score for this Epoch: 1.9007800817489624\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 75 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.8680\n",
      "Iteration: 2 of 241\ttrain_loss: 1.6944\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4336\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6443\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8613\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8854\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9853\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9900\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7194\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6859\n",
      "Iteration: 20 of 241\ttrain_loss: 1.6581\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5504\n",
      "Iteration: 24 of 241\ttrain_loss: 1.9358\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6163\n",
      "Iteration: 28 of 241\ttrain_loss: 1.8186\n",
      "Iteration: 30 of 241\ttrain_loss: 1.8404\n",
      "Iteration: 32 of 241\ttrain_loss: 2.3426\n",
      "Iteration: 34 of 241\ttrain_loss: 1.8090\n",
      "Iteration: 36 of 241\ttrain_loss: 1.7325\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7508\n",
      "Iteration: 40 of 241\ttrain_loss: 1.8772\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6911\n",
      "Iteration: 44 of 241\ttrain_loss: 1.4870\n",
      "Iteration: 46 of 241\ttrain_loss: 1.4805\n",
      "Iteration: 48 of 241\ttrain_loss: 2.0690\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8343\n",
      "Iteration: 52 of 241\ttrain_loss: 2.0655\n",
      "Iteration: 54 of 241\ttrain_loss: 2.0130\n",
      "Iteration: 56 of 241\ttrain_loss: 1.4097\n",
      "Iteration: 58 of 241\ttrain_loss: 2.0810\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8566\n",
      "Iteration: 62 of 241\ttrain_loss: 1.4957\n",
      "Iteration: 64 of 241\ttrain_loss: 1.4489\n",
      "Iteration: 66 of 241\ttrain_loss: 1.8433\n",
      "Iteration: 68 of 241\ttrain_loss: 1.8393\n",
      "Iteration: 70 of 241\ttrain_loss: 1.8007\n",
      "Iteration: 72 of 241\ttrain_loss: 1.7436\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8062\n",
      "Iteration: 76 of 241\ttrain_loss: 2.1455\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8190\n",
      "Iteration: 80 of 241\ttrain_loss: 1.7334\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8679\n",
      "Iteration: 84 of 241\ttrain_loss: 1.8853\n",
      "Iteration: 86 of 241\ttrain_loss: 2.3860\n",
      "Iteration: 88 of 241\ttrain_loss: 1.8203\n",
      "Iteration: 90 of 241\ttrain_loss: 1.8025\n",
      "Iteration: 92 of 241\ttrain_loss: 2.0212\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6084\n",
      "Iteration: 96 of 241\ttrain_loss: 1.8485\n",
      "Iteration: 98 of 241\ttrain_loss: 1.6143\n",
      "Iteration: 100 of 241\ttrain_loss: 1.8362\n",
      "Iteration: 102 of 241\ttrain_loss: 2.2979\n",
      "Iteration: 104 of 241\ttrain_loss: 1.8233\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8979\n",
      "Iteration: 108 of 241\ttrain_loss: 1.4310\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1912\n",
      "Iteration: 112 of 241\ttrain_loss: 1.7154\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0491\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5541\n",
      "Iteration: 118 of 241\ttrain_loss: 1.8572\n",
      "Iteration: 120 of 241\ttrain_loss: 1.7251\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2488\n",
      "Iteration: 124 of 241\ttrain_loss: 1.3370\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6688\n",
      "Iteration: 128 of 241\ttrain_loss: 2.0127\n",
      "Iteration: 130 of 241\ttrain_loss: 2.0130\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7582\n",
      "Iteration: 134 of 241\ttrain_loss: 1.9148\n",
      "Iteration: 136 of 241\ttrain_loss: 2.0300\n",
      "Iteration: 138 of 241\ttrain_loss: 2.0373\n",
      "Iteration: 140 of 241\ttrain_loss: 2.2805\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5639\n",
      "Iteration: 144 of 241\ttrain_loss: 1.8482\n",
      "Iteration: 146 of 241\ttrain_loss: 2.1619\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9267\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8632\n",
      "Iteration: 152 of 241\ttrain_loss: 1.7101\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8423\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7185\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5810\n",
      "Iteration: 160 of 241\ttrain_loss: 2.0701\n",
      "Iteration: 162 of 241\ttrain_loss: 2.1064\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6443\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1963\n",
      "Iteration: 168 of 241\ttrain_loss: 1.5929\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4600\n",
      "Iteration: 172 of 241\ttrain_loss: 1.9695\n",
      "Iteration: 174 of 241\ttrain_loss: 1.8717\n",
      "Iteration: 176 of 241\ttrain_loss: 2.1779\n",
      "Iteration: 178 of 241\ttrain_loss: 1.8737\n",
      "Iteration: 180 of 241\ttrain_loss: 2.1589\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6374\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7114\n",
      "Iteration: 186 of 241\ttrain_loss: 2.3109\n",
      "Iteration: 188 of 241\ttrain_loss: 1.4747\n",
      "Iteration: 190 of 241\ttrain_loss: 1.9961\n",
      "Iteration: 192 of 241\ttrain_loss: 2.2101\n",
      "Iteration: 194 of 241\ttrain_loss: 1.6586\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8786\n",
      "Iteration: 198 of 241\ttrain_loss: 1.8574\n",
      "Iteration: 200 of 241\ttrain_loss: 2.2037\n",
      "Iteration: 202 of 241\ttrain_loss: 1.9065\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0149\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0760\n",
      "Iteration: 208 of 241\ttrain_loss: 1.7498\n",
      "Iteration: 210 of 241\ttrain_loss: 1.7878\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6920\n",
      "Iteration: 214 of 241\ttrain_loss: 2.0964\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9434\n",
      "Iteration: 218 of 241\ttrain_loss: 2.3663\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1195\n",
      "Iteration: 222 of 241\ttrain_loss: 1.9038\n",
      "Iteration: 224 of 241\ttrain_loss: 1.6691\n",
      "Iteration: 226 of 241\ttrain_loss: 2.3422\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9191\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1111\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8217\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7049\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7644\n",
      "Iteration: 238 of 241\ttrain_loss: 1.6722\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8763\n",
      "Iteration: 241 of 241\ttrain_loss: 2.3426\n",
      "Average Score for this Epoch: 1.8732895851135254\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 76 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9632\n",
      "Iteration: 2 of 241\ttrain_loss: 1.9862\n",
      "Iteration: 4 of 241\ttrain_loss: 1.6210\n",
      "Iteration: 6 of 241\ttrain_loss: 1.8406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8 of 241\ttrain_loss: 1.8820\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6744\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6367\n",
      "Iteration: 14 of 241\ttrain_loss: 1.6314\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4209\n",
      "Iteration: 18 of 241\ttrain_loss: 1.7765\n",
      "Iteration: 20 of 241\ttrain_loss: 1.4597\n",
      "Iteration: 22 of 241\ttrain_loss: 1.9044\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8506\n",
      "Iteration: 26 of 241\ttrain_loss: 2.0439\n",
      "Iteration: 28 of 241\ttrain_loss: 1.5700\n",
      "Iteration: 30 of 241\ttrain_loss: 1.6432\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0091\n",
      "Iteration: 34 of 241\ttrain_loss: 1.4417\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9140\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7406\n",
      "Iteration: 40 of 241\ttrain_loss: 1.7265\n",
      "Iteration: 42 of 241\ttrain_loss: 1.9265\n",
      "Iteration: 44 of 241\ttrain_loss: 1.8845\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9895\n",
      "Iteration: 48 of 241\ttrain_loss: 1.8335\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8160\n",
      "Iteration: 52 of 241\ttrain_loss: 1.6926\n",
      "Iteration: 54 of 241\ttrain_loss: 1.7658\n",
      "Iteration: 56 of 241\ttrain_loss: 1.4754\n",
      "Iteration: 58 of 241\ttrain_loss: 1.5094\n",
      "Iteration: 60 of 241\ttrain_loss: 2.1005\n",
      "Iteration: 62 of 241\ttrain_loss: 1.4534\n",
      "Iteration: 64 of 241\ttrain_loss: 1.6370\n",
      "Iteration: 66 of 241\ttrain_loss: 1.5576\n",
      "Iteration: 68 of 241\ttrain_loss: 2.0999\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0471\n",
      "Iteration: 72 of 241\ttrain_loss: 1.7214\n",
      "Iteration: 74 of 241\ttrain_loss: 1.4823\n",
      "Iteration: 76 of 241\ttrain_loss: 1.8058\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1832\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6435\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0181\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9054\n",
      "Iteration: 86 of 241\ttrain_loss: 1.5975\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9221\n",
      "Iteration: 90 of 241\ttrain_loss: 1.9043\n",
      "Iteration: 92 of 241\ttrain_loss: 1.5071\n",
      "Iteration: 94 of 241\ttrain_loss: 2.2655\n",
      "Iteration: 96 of 241\ttrain_loss: 2.4423\n",
      "Iteration: 98 of 241\ttrain_loss: 1.8868\n",
      "Iteration: 100 of 241\ttrain_loss: 2.0482\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9501\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4870\n",
      "Iteration: 106 of 241\ttrain_loss: 1.7488\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7011\n",
      "Iteration: 110 of 241\ttrain_loss: 2.2085\n",
      "Iteration: 112 of 241\ttrain_loss: 1.9561\n",
      "Iteration: 114 of 241\ttrain_loss: 1.7212\n",
      "Iteration: 116 of 241\ttrain_loss: 2.2185\n",
      "Iteration: 118 of 241\ttrain_loss: 2.5858\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8160\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5751\n",
      "Iteration: 124 of 241\ttrain_loss: 2.6001\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8620\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8225\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1527\n",
      "Iteration: 132 of 241\ttrain_loss: 1.9174\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8436\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8671\n",
      "Iteration: 138 of 241\ttrain_loss: 1.3051\n",
      "Iteration: 140 of 241\ttrain_loss: 1.6918\n",
      "Iteration: 142 of 241\ttrain_loss: 1.9680\n",
      "Iteration: 144 of 241\ttrain_loss: 1.4314\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9639\n",
      "Iteration: 148 of 241\ttrain_loss: 1.8814\n",
      "Iteration: 150 of 241\ttrain_loss: 1.6815\n",
      "Iteration: 152 of 241\ttrain_loss: 1.9272\n",
      "Iteration: 154 of 241\ttrain_loss: 1.9187\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9674\n",
      "Iteration: 158 of 241\ttrain_loss: 1.6994\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7671\n",
      "Iteration: 162 of 241\ttrain_loss: 1.8962\n",
      "Iteration: 164 of 241\ttrain_loss: 2.0956\n",
      "Iteration: 166 of 241\ttrain_loss: 2.2489\n",
      "Iteration: 168 of 241\ttrain_loss: 1.8141\n",
      "Iteration: 170 of 241\ttrain_loss: 1.6354\n",
      "Iteration: 172 of 241\ttrain_loss: 2.5598\n",
      "Iteration: 174 of 241\ttrain_loss: 1.9432\n",
      "Iteration: 176 of 241\ttrain_loss: 1.8164\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6881\n",
      "Iteration: 180 of 241\ttrain_loss: 1.7854\n",
      "Iteration: 182 of 241\ttrain_loss: 2.2420\n",
      "Iteration: 184 of 241\ttrain_loss: 2.4584\n",
      "Iteration: 186 of 241\ttrain_loss: 2.0467\n",
      "Iteration: 188 of 241\ttrain_loss: 1.9001\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7794\n",
      "Iteration: 192 of 241\ttrain_loss: 1.8655\n",
      "Iteration: 194 of 241\ttrain_loss: 2.1019\n",
      "Iteration: 196 of 241\ttrain_loss: 1.6677\n",
      "Iteration: 198 of 241\ttrain_loss: 1.5788\n",
      "Iteration: 200 of 241\ttrain_loss: 1.6842\n",
      "Iteration: 202 of 241\ttrain_loss: 1.6943\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0516\n",
      "Iteration: 206 of 241\ttrain_loss: 1.6075\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6038\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8581\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8301\n",
      "Iteration: 214 of 241\ttrain_loss: 2.0100\n",
      "Iteration: 216 of 241\ttrain_loss: 1.6965\n",
      "Iteration: 218 of 241\ttrain_loss: 2.3183\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1948\n",
      "Iteration: 222 of 241\ttrain_loss: 1.4899\n",
      "Iteration: 224 of 241\ttrain_loss: 1.6663\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9158\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8435\n",
      "Iteration: 230 of 241\ttrain_loss: 1.9382\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8752\n",
      "Iteration: 234 of 241\ttrain_loss: 2.2241\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7199\n",
      "Iteration: 238 of 241\ttrain_loss: 1.8857\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6220\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9884\n",
      "Average Score for this Epoch: 1.8513286113739014\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 77 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9539\n",
      "Iteration: 2 of 241\ttrain_loss: 1.4170\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7223\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9159\n",
      "Iteration: 8 of 241\ttrain_loss: 1.8237\n",
      "Iteration: 10 of 241\ttrain_loss: 1.5304\n",
      "Iteration: 12 of 241\ttrain_loss: 1.8342\n",
      "Iteration: 14 of 241\ttrain_loss: 1.4951\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4445\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6374\n",
      "Iteration: 20 of 241\ttrain_loss: 2.0043\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5494\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8776\n",
      "Iteration: 26 of 241\ttrain_loss: 2.6053\n",
      "Iteration: 28 of 241\ttrain_loss: 1.3797\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9684\n",
      "Iteration: 32 of 241\ttrain_loss: 1.9085\n",
      "Iteration: 34 of 241\ttrain_loss: 1.8960\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9433\n",
      "Iteration: 38 of 241\ttrain_loss: 2.1000\n",
      "Iteration: 40 of 241\ttrain_loss: 1.7345\n",
      "Iteration: 42 of 241\ttrain_loss: 1.7404\n",
      "Iteration: 44 of 241\ttrain_loss: 1.6461\n",
      "Iteration: 46 of 241\ttrain_loss: 2.3046\n",
      "Iteration: 48 of 241\ttrain_loss: 1.7775\n",
      "Iteration: 50 of 241\ttrain_loss: 1.3718\n",
      "Iteration: 52 of 241\ttrain_loss: 1.6575\n",
      "Iteration: 54 of 241\ttrain_loss: 2.1225\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7909\n",
      "Iteration: 58 of 241\ttrain_loss: 1.3296\n",
      "Iteration: 60 of 241\ttrain_loss: 1.6861\n",
      "Iteration: 62 of 241\ttrain_loss: 1.6843\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7953\n",
      "Iteration: 66 of 241\ttrain_loss: 1.7708\n",
      "Iteration: 68 of 241\ttrain_loss: 1.5339\n",
      "Iteration: 70 of 241\ttrain_loss: 1.9246\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8355\n",
      "Iteration: 74 of 241\ttrain_loss: 2.3433\n",
      "Iteration: 76 of 241\ttrain_loss: 2.1192\n",
      "Iteration: 78 of 241\ttrain_loss: 2.1753\n",
      "Iteration: 80 of 241\ttrain_loss: 1.7237\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8092\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9390\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6007\n",
      "Iteration: 88 of 241\ttrain_loss: 1.5523\n",
      "Iteration: 90 of 241\ttrain_loss: 1.9577\n",
      "Iteration: 92 of 241\ttrain_loss: 1.4244\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9095\n",
      "Iteration: 96 of 241\ttrain_loss: 1.7482\n",
      "Iteration: 98 of 241\ttrain_loss: 1.6431\n",
      "Iteration: 100 of 241\ttrain_loss: 1.8364\n",
      "Iteration: 102 of 241\ttrain_loss: 1.8020\n",
      "Iteration: 104 of 241\ttrain_loss: 2.5330\n",
      "Iteration: 106 of 241\ttrain_loss: 2.2994\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7705\n",
      "Iteration: 110 of 241\ttrain_loss: 1.7280\n",
      "Iteration: 112 of 241\ttrain_loss: 2.2249\n",
      "Iteration: 114 of 241\ttrain_loss: 1.6732\n",
      "Iteration: 116 of 241\ttrain_loss: 1.9557\n",
      "Iteration: 118 of 241\ttrain_loss: 2.0679\n",
      "Iteration: 120 of 241\ttrain_loss: 1.9549\n",
      "Iteration: 122 of 241\ttrain_loss: 1.8400\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7971\n",
      "Iteration: 126 of 241\ttrain_loss: 2.5403\n",
      "Iteration: 128 of 241\ttrain_loss: 2.2274\n",
      "Iteration: 130 of 241\ttrain_loss: 2.3841\n",
      "Iteration: 132 of 241\ttrain_loss: 1.9943\n",
      "Iteration: 134 of 241\ttrain_loss: 2.2359\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8930\n",
      "Iteration: 138 of 241\ttrain_loss: 1.9147\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0793\n",
      "Iteration: 142 of 241\ttrain_loss: 1.7626\n",
      "Iteration: 144 of 241\ttrain_loss: 1.9989\n",
      "Iteration: 146 of 241\ttrain_loss: 1.7000\n",
      "Iteration: 148 of 241\ttrain_loss: 1.6269\n",
      "Iteration: 150 of 241\ttrain_loss: 1.7583\n",
      "Iteration: 152 of 241\ttrain_loss: 1.4778\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1199\n",
      "Iteration: 156 of 241\ttrain_loss: 2.4749\n",
      "Iteration: 158 of 241\ttrain_loss: 2.4163\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7123\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 164 of 241\ttrain_loss: 2.2831\n",
      "Iteration: 166 of 241\ttrain_loss: 1.7864\n",
      "Iteration: 168 of 241\ttrain_loss: 1.5109\n",
      "Iteration: 170 of 241\ttrain_loss: 1.6008\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0348\n",
      "Iteration: 174 of 241\ttrain_loss: 2.1971\n",
      "Iteration: 176 of 241\ttrain_loss: 1.9660\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6061\n",
      "Iteration: 180 of 241\ttrain_loss: 2.0025\n",
      "Iteration: 182 of 241\ttrain_loss: 1.8899\n",
      "Iteration: 184 of 241\ttrain_loss: 2.0550\n",
      "Iteration: 186 of 241\ttrain_loss: 2.2287\n",
      "Iteration: 188 of 241\ttrain_loss: 1.7280\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7581\n",
      "Iteration: 192 of 241\ttrain_loss: 1.9877\n",
      "Iteration: 194 of 241\ttrain_loss: 1.6920\n",
      "Iteration: 196 of 241\ttrain_loss: 1.6699\n",
      "Iteration: 198 of 241\ttrain_loss: 2.3740\n",
      "Iteration: 200 of 241\ttrain_loss: 1.9193\n",
      "Iteration: 202 of 241\ttrain_loss: 1.5128\n",
      "Iteration: 204 of 241\ttrain_loss: 1.9326\n",
      "Iteration: 206 of 241\ttrain_loss: 1.6166\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6844\n",
      "Iteration: 210 of 241\ttrain_loss: 1.6700\n",
      "Iteration: 212 of 241\ttrain_loss: 2.2849\n",
      "Iteration: 214 of 241\ttrain_loss: 2.1324\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9047\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0088\n",
      "Iteration: 220 of 241\ttrain_loss: 1.8751\n",
      "Iteration: 222 of 241\ttrain_loss: 1.6680\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2433\n",
      "Iteration: 226 of 241\ttrain_loss: 2.1444\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8846\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8460\n",
      "Iteration: 232 of 241\ttrain_loss: 1.9868\n",
      "Iteration: 234 of 241\ttrain_loss: 2.0215\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7122\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9584\n",
      "Iteration: 240 of 241\ttrain_loss: 2.1435\n",
      "Iteration: 241 of 241\ttrain_loss: 1.9040\n",
      "Average Score for this Epoch: 1.8510463237762451\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 78 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6038\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7177\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7293\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9405\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6968\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8974\n",
      "Iteration: 12 of 241\ttrain_loss: 2.0608\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8943\n",
      "Iteration: 16 of 241\ttrain_loss: 1.6388\n",
      "Iteration: 18 of 241\ttrain_loss: 1.7925\n",
      "Iteration: 20 of 241\ttrain_loss: 1.7715\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8542\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8473\n",
      "Iteration: 26 of 241\ttrain_loss: 1.9443\n",
      "Iteration: 28 of 241\ttrain_loss: 1.6042\n",
      "Iteration: 30 of 241\ttrain_loss: 1.7924\n",
      "Iteration: 32 of 241\ttrain_loss: 1.7889\n",
      "Iteration: 34 of 241\ttrain_loss: 1.5863\n",
      "Iteration: 36 of 241\ttrain_loss: 1.8694\n",
      "Iteration: 38 of 241\ttrain_loss: 1.6476\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5716\n",
      "Iteration: 42 of 241\ttrain_loss: 1.9870\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5328\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9506\n",
      "Iteration: 48 of 241\ttrain_loss: 1.4756\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8171\n",
      "Iteration: 52 of 241\ttrain_loss: 1.8082\n",
      "Iteration: 54 of 241\ttrain_loss: 1.7081\n",
      "Iteration: 56 of 241\ttrain_loss: 1.9321\n",
      "Iteration: 58 of 241\ttrain_loss: 1.8921\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7572\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7051\n",
      "Iteration: 64 of 241\ttrain_loss: 2.3069\n",
      "Iteration: 66 of 241\ttrain_loss: 1.7208\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9047\n",
      "Iteration: 70 of 241\ttrain_loss: 2.2810\n",
      "Iteration: 72 of 241\ttrain_loss: 1.6185\n",
      "Iteration: 74 of 241\ttrain_loss: 1.6342\n",
      "Iteration: 76 of 241\ttrain_loss: 1.6161\n",
      "Iteration: 78 of 241\ttrain_loss: 1.4847\n",
      "Iteration: 80 of 241\ttrain_loss: 1.4997\n",
      "Iteration: 82 of 241\ttrain_loss: 1.7879\n",
      "Iteration: 84 of 241\ttrain_loss: 1.9809\n",
      "Iteration: 86 of 241\ttrain_loss: 1.5567\n",
      "Iteration: 88 of 241\ttrain_loss: 1.8778\n",
      "Iteration: 90 of 241\ttrain_loss: 1.5367\n",
      "Iteration: 92 of 241\ttrain_loss: 2.0610\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9040\n",
      "Iteration: 96 of 241\ttrain_loss: 1.6496\n",
      "Iteration: 98 of 241\ttrain_loss: 2.0364\n",
      "Iteration: 100 of 241\ttrain_loss: 1.7830\n",
      "Iteration: 102 of 241\ttrain_loss: 1.7777\n",
      "Iteration: 104 of 241\ttrain_loss: 1.6657\n",
      "Iteration: 106 of 241\ttrain_loss: 1.6190\n",
      "Iteration: 108 of 241\ttrain_loss: 1.9121\n",
      "Iteration: 110 of 241\ttrain_loss: 2.1382\n",
      "Iteration: 112 of 241\ttrain_loss: 2.0528\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4923\n",
      "Iteration: 116 of 241\ttrain_loss: 2.0190\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6830\n",
      "Iteration: 120 of 241\ttrain_loss: 1.2810\n",
      "Iteration: 122 of 241\ttrain_loss: 2.7114\n",
      "Iteration: 124 of 241\ttrain_loss: 1.8536\n",
      "Iteration: 126 of 241\ttrain_loss: 1.9772\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9195\n",
      "Iteration: 130 of 241\ttrain_loss: 1.8591\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6295\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8485\n",
      "Iteration: 136 of 241\ttrain_loss: 1.5970\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7488\n",
      "Iteration: 140 of 241\ttrain_loss: 1.5602\n",
      "Iteration: 142 of 241\ttrain_loss: 1.2105\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7489\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4045\n",
      "Iteration: 148 of 241\ttrain_loss: 2.0891\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0236\n",
      "Iteration: 152 of 241\ttrain_loss: 2.1884\n",
      "Iteration: 154 of 241\ttrain_loss: 1.6938\n",
      "Iteration: 156 of 241\ttrain_loss: 1.8049\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5908\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7856\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9246\n",
      "Iteration: 164 of 241\ttrain_loss: 2.3992\n",
      "Iteration: 166 of 241\ttrain_loss: 1.6992\n",
      "Iteration: 168 of 241\ttrain_loss: 1.5760\n",
      "Iteration: 170 of 241\ttrain_loss: 1.3902\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0894\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0126\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5316\n",
      "Iteration: 178 of 241\ttrain_loss: 2.2394\n",
      "Iteration: 180 of 241\ttrain_loss: 1.4219\n",
      "Iteration: 182 of 241\ttrain_loss: 2.1005\n",
      "Iteration: 184 of 241\ttrain_loss: 1.8305\n",
      "Iteration: 186 of 241\ttrain_loss: 2.3697\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8246\n",
      "Iteration: 190 of 241\ttrain_loss: 2.0450\n",
      "Iteration: 192 of 241\ttrain_loss: 1.9832\n",
      "Iteration: 194 of 241\ttrain_loss: 2.1256\n",
      "Iteration: 196 of 241\ttrain_loss: 1.7715\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1005\n",
      "Iteration: 200 of 241\ttrain_loss: 1.9850\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7614\n",
      "Iteration: 204 of 241\ttrain_loss: 1.7847\n",
      "Iteration: 206 of 241\ttrain_loss: 1.5377\n",
      "Iteration: 208 of 241\ttrain_loss: 1.9268\n",
      "Iteration: 210 of 241\ttrain_loss: 1.7731\n",
      "Iteration: 212 of 241\ttrain_loss: 1.4855\n",
      "Iteration: 214 of 241\ttrain_loss: 1.5771\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9052\n",
      "Iteration: 218 of 241\ttrain_loss: 1.9353\n",
      "Iteration: 220 of 241\ttrain_loss: 1.5330\n",
      "Iteration: 222 of 241\ttrain_loss: 2.0054\n",
      "Iteration: 224 of 241\ttrain_loss: 1.8284\n",
      "Iteration: 226 of 241\ttrain_loss: 1.6518\n",
      "Iteration: 228 of 241\ttrain_loss: 2.0061\n",
      "Iteration: 230 of 241\ttrain_loss: 1.5660\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7977\n",
      "Iteration: 234 of 241\ttrain_loss: 2.0620\n",
      "Iteration: 236 of 241\ttrain_loss: 2.2861\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9154\n",
      "Iteration: 240 of 241\ttrain_loss: 1.5148\n",
      "Iteration: 241 of 241\ttrain_loss: 1.7458\n",
      "Average Score for this Epoch: 1.8235361576080322\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 79 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.5482\n",
      "Iteration: 2 of 241\ttrain_loss: 1.2137\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4535\n",
      "Iteration: 6 of 241\ttrain_loss: 1.7151\n",
      "Iteration: 8 of 241\ttrain_loss: 2.3284\n",
      "Iteration: 10 of 241\ttrain_loss: 2.0495\n",
      "Iteration: 12 of 241\ttrain_loss: 2.4056\n",
      "Iteration: 14 of 241\ttrain_loss: 1.5836\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4469\n",
      "Iteration: 18 of 241\ttrain_loss: 1.9492\n",
      "Iteration: 20 of 241\ttrain_loss: 1.8737\n",
      "Iteration: 22 of 241\ttrain_loss: 1.4804\n",
      "Iteration: 24 of 241\ttrain_loss: 1.3569\n",
      "Iteration: 26 of 241\ttrain_loss: 1.4322\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7546\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9894\n",
      "Iteration: 32 of 241\ttrain_loss: 1.5047\n",
      "Iteration: 34 of 241\ttrain_loss: 1.7659\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6227\n",
      "Iteration: 38 of 241\ttrain_loss: 1.6087\n",
      "Iteration: 40 of 241\ttrain_loss: 1.8215\n",
      "Iteration: 42 of 241\ttrain_loss: 2.0955\n",
      "Iteration: 44 of 241\ttrain_loss: 1.8795\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9250\n",
      "Iteration: 48 of 241\ttrain_loss: 1.6726\n",
      "Iteration: 50 of 241\ttrain_loss: 2.0170\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5678\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9030\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7248\n",
      "Iteration: 58 of 241\ttrain_loss: 1.4922\n",
      "Iteration: 60 of 241\ttrain_loss: 2.0906\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7322\n",
      "Iteration: 64 of 241\ttrain_loss: 1.5801\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 68 of 241\ttrain_loss: 1.8890\n",
      "Iteration: 70 of 241\ttrain_loss: 1.4131\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8081\n",
      "Iteration: 74 of 241\ttrain_loss: 1.5164\n",
      "Iteration: 76 of 241\ttrain_loss: 1.7824\n",
      "Iteration: 78 of 241\ttrain_loss: 1.7621\n",
      "Iteration: 80 of 241\ttrain_loss: 1.3544\n",
      "Iteration: 82 of 241\ttrain_loss: 2.0667\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3998\n",
      "Iteration: 86 of 241\ttrain_loss: 1.9737\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9212\n",
      "Iteration: 90 of 241\ttrain_loss: 1.3121\n",
      "Iteration: 92 of 241\ttrain_loss: 1.6031\n",
      "Iteration: 94 of 241\ttrain_loss: 2.0021\n",
      "Iteration: 96 of 241\ttrain_loss: 1.8543\n",
      "Iteration: 98 of 241\ttrain_loss: 1.8006\n",
      "Iteration: 100 of 241\ttrain_loss: 2.8814\n",
      "Iteration: 102 of 241\ttrain_loss: 1.4552\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9143\n",
      "Iteration: 106 of 241\ttrain_loss: 1.9933\n",
      "Iteration: 108 of 241\ttrain_loss: 2.2562\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9760\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5750\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5920\n",
      "Iteration: 116 of 241\ttrain_loss: 1.8541\n",
      "Iteration: 118 of 241\ttrain_loss: 1.9580\n",
      "Iteration: 120 of 241\ttrain_loss: 1.9778\n",
      "Iteration: 122 of 241\ttrain_loss: 1.8541\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5550\n",
      "Iteration: 126 of 241\ttrain_loss: 1.5517\n",
      "Iteration: 128 of 241\ttrain_loss: 1.7261\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1570\n",
      "Iteration: 132 of 241\ttrain_loss: 1.3771\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7137\n",
      "Iteration: 136 of 241\ttrain_loss: 1.7748\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7808\n",
      "Iteration: 140 of 241\ttrain_loss: 1.7925\n",
      "Iteration: 142 of 241\ttrain_loss: 1.9581\n",
      "Iteration: 144 of 241\ttrain_loss: 1.5132\n",
      "Iteration: 146 of 241\ttrain_loss: 1.9565\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7245\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8898\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6150\n",
      "Iteration: 154 of 241\ttrain_loss: 1.9575\n",
      "Iteration: 156 of 241\ttrain_loss: 1.8405\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5282\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6355\n",
      "Iteration: 162 of 241\ttrain_loss: 2.0852\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9332\n",
      "Iteration: 166 of 241\ttrain_loss: 1.8013\n",
      "Iteration: 168 of 241\ttrain_loss: 1.6899\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7756\n",
      "Iteration: 172 of 241\ttrain_loss: 2.0087\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5217\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6025\n",
      "Iteration: 178 of 241\ttrain_loss: 1.9449\n",
      "Iteration: 180 of 241\ttrain_loss: 2.0138\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7903\n",
      "Iteration: 184 of 241\ttrain_loss: 1.8515\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6697\n",
      "Iteration: 188 of 241\ttrain_loss: 1.7225\n",
      "Iteration: 190 of 241\ttrain_loss: 1.8926\n",
      "Iteration: 192 of 241\ttrain_loss: 1.8137\n",
      "Iteration: 194 of 241\ttrain_loss: 2.1873\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8637\n",
      "Iteration: 198 of 241\ttrain_loss: 1.7509\n",
      "Iteration: 200 of 241\ttrain_loss: 2.2084\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7714\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0216\n",
      "Iteration: 206 of 241\ttrain_loss: 1.9249\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6285\n",
      "Iteration: 210 of 241\ttrain_loss: 2.6399\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6139\n",
      "Iteration: 214 of 241\ttrain_loss: 1.9018\n",
      "Iteration: 216 of 241\ttrain_loss: 2.0453\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0499\n",
      "Iteration: 220 of 241\ttrain_loss: 2.5923\n",
      "Iteration: 222 of 241\ttrain_loss: 1.6525\n",
      "Iteration: 224 of 241\ttrain_loss: 2.1190\n",
      "Iteration: 226 of 241\ttrain_loss: 1.8753\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9954\n",
      "Iteration: 230 of 241\ttrain_loss: 1.5527\n",
      "Iteration: 232 of 241\ttrain_loss: 1.9815\n",
      "Iteration: 234 of 241\ttrain_loss: 1.6126\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9697\n",
      "Iteration: 238 of 241\ttrain_loss: 1.4839\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6616\n",
      "Iteration: 241 of 241\ttrain_loss: 1.8329\n",
      "Average Score for this Epoch: 1.8200559616088867\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 80 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.5144\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8609\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4610\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6089\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6301\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6794\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7730\n",
      "Iteration: 14 of 241\ttrain_loss: 1.5246\n",
      "Iteration: 16 of 241\ttrain_loss: 1.9832\n",
      "Iteration: 18 of 241\ttrain_loss: 2.2791\n",
      "Iteration: 20 of 241\ttrain_loss: 1.9438\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5897\n",
      "Iteration: 24 of 241\ttrain_loss: 1.6918\n",
      "Iteration: 26 of 241\ttrain_loss: 1.7996\n",
      "Iteration: 28 of 241\ttrain_loss: 2.0045\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9394\n",
      "Iteration: 32 of 241\ttrain_loss: 2.5196\n",
      "Iteration: 34 of 241\ttrain_loss: 1.7722\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0321\n",
      "Iteration: 38 of 241\ttrain_loss: 1.8387\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5296\n",
      "Iteration: 42 of 241\ttrain_loss: 1.7036\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5499\n",
      "Iteration: 46 of 241\ttrain_loss: 1.5703\n",
      "Iteration: 48 of 241\ttrain_loss: 1.8422\n",
      "Iteration: 50 of 241\ttrain_loss: 1.9164\n",
      "Iteration: 52 of 241\ttrain_loss: 1.4871\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8053\n",
      "Iteration: 56 of 241\ttrain_loss: 1.7994\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7375\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7020\n",
      "Iteration: 62 of 241\ttrain_loss: 1.5656\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9258\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6152\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7920\n",
      "Iteration: 70 of 241\ttrain_loss: 1.4933\n",
      "Iteration: 72 of 241\ttrain_loss: 1.6746\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7009\n",
      "Iteration: 76 of 241\ttrain_loss: 1.5109\n",
      "Iteration: 78 of 241\ttrain_loss: 1.7798\n",
      "Iteration: 80 of 241\ttrain_loss: 1.3453\n",
      "Iteration: 82 of 241\ttrain_loss: 2.3930\n",
      "Iteration: 84 of 241\ttrain_loss: 1.4644\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6118\n",
      "Iteration: 88 of 241\ttrain_loss: 2.0192\n",
      "Iteration: 90 of 241\ttrain_loss: 1.3005\n",
      "Iteration: 92 of 241\ttrain_loss: 1.6256\n",
      "Iteration: 94 of 241\ttrain_loss: 2.0457\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5286\n",
      "Iteration: 98 of 241\ttrain_loss: 1.2734\n",
      "Iteration: 100 of 241\ttrain_loss: 2.0851\n",
      "Iteration: 102 of 241\ttrain_loss: 1.8927\n",
      "Iteration: 104 of 241\ttrain_loss: 1.7316\n",
      "Iteration: 106 of 241\ttrain_loss: 1.9707\n",
      "Iteration: 108 of 241\ttrain_loss: 1.9128\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8912\n",
      "Iteration: 112 of 241\ttrain_loss: 2.1444\n",
      "Iteration: 114 of 241\ttrain_loss: 1.8351\n",
      "Iteration: 116 of 241\ttrain_loss: 1.9364\n",
      "Iteration: 118 of 241\ttrain_loss: 1.8989\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8085\n",
      "Iteration: 122 of 241\ttrain_loss: 1.7325\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5031\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6143\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9442\n",
      "Iteration: 130 of 241\ttrain_loss: 2.0724\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7064\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7946\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9978\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7741\n",
      "Iteration: 140 of 241\ttrain_loss: 1.8619\n",
      "Iteration: 142 of 241\ttrain_loss: 1.8887\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6614\n",
      "Iteration: 146 of 241\ttrain_loss: 1.5896\n",
      "Iteration: 148 of 241\ttrain_loss: 2.3447\n",
      "Iteration: 150 of 241\ttrain_loss: 1.5214\n",
      "Iteration: 152 of 241\ttrain_loss: 2.2555\n",
      "Iteration: 154 of 241\ttrain_loss: 1.5708\n",
      "Iteration: 156 of 241\ttrain_loss: 2.0058\n",
      "Iteration: 158 of 241\ttrain_loss: 1.7716\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7833\n",
      "Iteration: 162 of 241\ttrain_loss: 2.2597\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9993\n",
      "Iteration: 166 of 241\ttrain_loss: 2.0442\n",
      "Iteration: 168 of 241\ttrain_loss: 1.9225\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7907\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7315\n",
      "Iteration: 174 of 241\ttrain_loss: 1.6123\n",
      "Iteration: 176 of 241\ttrain_loss: 2.0380\n",
      "Iteration: 178 of 241\ttrain_loss: 1.7530\n",
      "Iteration: 180 of 241\ttrain_loss: 1.5707\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7040\n",
      "Iteration: 184 of 241\ttrain_loss: 1.8011\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4532\n",
      "Iteration: 188 of 241\ttrain_loss: 2.1387\n",
      "Iteration: 190 of 241\ttrain_loss: 2.0519\n",
      "Iteration: 192 of 241\ttrain_loss: 1.4955\n",
      "Iteration: 194 of 241\ttrain_loss: 1.6686\n",
      "Iteration: 196 of 241\ttrain_loss: 1.9420\n",
      "Iteration: 198 of 241\ttrain_loss: 1.4348\n",
      "Iteration: 200 of 241\ttrain_loss: 2.0520\n",
      "Iteration: 202 of 241\ttrain_loss: 1.6543\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0409\n",
      "Iteration: 206 of 241\ttrain_loss: 2.2011\n",
      "Iteration: 208 of 241\ttrain_loss: 1.8229\n",
      "Iteration: 210 of 241\ttrain_loss: 1.3628\n",
      "Iteration: 212 of 241\ttrain_loss: 1.7881\n",
      "Iteration: 214 of 241\ttrain_loss: 1.5533\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8789\n",
      "Iteration: 218 of 241\ttrain_loss: 1.7886\n",
      "Iteration: 220 of 241\ttrain_loss: 1.7119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 222 of 241\ttrain_loss: 1.5783\n",
      "Iteration: 224 of 241\ttrain_loss: 1.7259\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9063\n",
      "Iteration: 228 of 241\ttrain_loss: 2.0869\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8312\n",
      "Iteration: 232 of 241\ttrain_loss: 1.5337\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7405\n",
      "Iteration: 236 of 241\ttrain_loss: 1.4442\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9941\n",
      "Iteration: 240 of 241\ttrain_loss: 1.9961\n",
      "Iteration: 241 of 241\ttrain_loss: 1.6391\n",
      "Average Score for this Epoch: 1.777562141418457\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 81 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9099\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7996\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8439\n",
      "Iteration: 6 of 241\ttrain_loss: 1.3642\n",
      "Iteration: 8 of 241\ttrain_loss: 1.3263\n",
      "Iteration: 10 of 241\ttrain_loss: 1.9671\n",
      "Iteration: 12 of 241\ttrain_loss: 1.5456\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9443\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5860\n",
      "Iteration: 18 of 241\ttrain_loss: 1.5206\n",
      "Iteration: 20 of 241\ttrain_loss: 1.6937\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8203\n",
      "Iteration: 24 of 241\ttrain_loss: 1.4433\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6587\n",
      "Iteration: 28 of 241\ttrain_loss: 2.1836\n",
      "Iteration: 30 of 241\ttrain_loss: 1.5668\n",
      "Iteration: 32 of 241\ttrain_loss: 1.6053\n",
      "Iteration: 34 of 241\ttrain_loss: 1.8809\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0419\n",
      "Iteration: 38 of 241\ttrain_loss: 1.5784\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5096\n",
      "Iteration: 42 of 241\ttrain_loss: 2.5306\n",
      "Iteration: 44 of 241\ttrain_loss: 1.7045\n",
      "Iteration: 46 of 241\ttrain_loss: 1.5446\n",
      "Iteration: 48 of 241\ttrain_loss: 1.5877\n",
      "Iteration: 50 of 241\ttrain_loss: 1.4029\n",
      "Iteration: 52 of 241\ttrain_loss: 1.9347\n",
      "Iteration: 54 of 241\ttrain_loss: 1.5385\n",
      "Iteration: 56 of 241\ttrain_loss: 2.2607\n",
      "Iteration: 58 of 241\ttrain_loss: 2.6532\n",
      "Iteration: 60 of 241\ttrain_loss: 1.5755\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7907\n",
      "Iteration: 64 of 241\ttrain_loss: 1.4778\n",
      "Iteration: 66 of 241\ttrain_loss: 1.5879\n",
      "Iteration: 68 of 241\ttrain_loss: 1.6779\n",
      "Iteration: 70 of 241\ttrain_loss: 1.7296\n",
      "Iteration: 72 of 241\ttrain_loss: 1.6102\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8050\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0330\n",
      "Iteration: 78 of 241\ttrain_loss: 1.4459\n",
      "Iteration: 80 of 241\ttrain_loss: 1.9196\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8743\n",
      "Iteration: 84 of 241\ttrain_loss: 2.1269\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6146\n",
      "Iteration: 88 of 241\ttrain_loss: 1.9287\n",
      "Iteration: 90 of 241\ttrain_loss: 1.6081\n",
      "Iteration: 92 of 241\ttrain_loss: 2.2243\n",
      "Iteration: 94 of 241\ttrain_loss: 1.8002\n",
      "Iteration: 96 of 241\ttrain_loss: 1.8510\n",
      "Iteration: 98 of 241\ttrain_loss: 1.7715\n",
      "Iteration: 100 of 241\ttrain_loss: 1.7976\n",
      "Iteration: 102 of 241\ttrain_loss: 1.6894\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9290\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8861\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7944\n",
      "Iteration: 110 of 241\ttrain_loss: 1.4589\n",
      "Iteration: 112 of 241\ttrain_loss: 1.6883\n",
      "Iteration: 114 of 241\ttrain_loss: 1.6940\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5356\n",
      "Iteration: 118 of 241\ttrain_loss: 1.4459\n",
      "Iteration: 120 of 241\ttrain_loss: 1.9709\n",
      "Iteration: 122 of 241\ttrain_loss: 2.2351\n",
      "Iteration: 124 of 241\ttrain_loss: 1.6790\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8721\n",
      "Iteration: 128 of 241\ttrain_loss: 1.9364\n",
      "Iteration: 130 of 241\ttrain_loss: 1.5002\n",
      "Iteration: 132 of 241\ttrain_loss: 2.3731\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8871\n",
      "Iteration: 136 of 241\ttrain_loss: 2.1180\n",
      "Iteration: 138 of 241\ttrain_loss: 2.1498\n",
      "Iteration: 140 of 241\ttrain_loss: 1.4658\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5604\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6024\n",
      "Iteration: 146 of 241\ttrain_loss: 1.6920\n",
      "Iteration: 148 of 241\ttrain_loss: 1.5097\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0094\n",
      "Iteration: 152 of 241\ttrain_loss: 2.1756\n",
      "Iteration: 154 of 241\ttrain_loss: 1.7464\n",
      "Iteration: 156 of 241\ttrain_loss: 1.4763\n",
      "Iteration: 158 of 241\ttrain_loss: 1.9422\n",
      "Iteration: 160 of 241\ttrain_loss: 1.8861\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9018\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6899\n",
      "Iteration: 166 of 241\ttrain_loss: 2.5870\n",
      "Iteration: 168 of 241\ttrain_loss: 2.0059\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7460\n",
      "Iteration: 172 of 241\ttrain_loss: 1.9592\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0104\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5759\n",
      "Iteration: 178 of 241\ttrain_loss: 1.5861\n",
      "Iteration: 180 of 241\ttrain_loss: 2.0661\n",
      "Iteration: 182 of 241\ttrain_loss: 1.8427\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7327\n",
      "Iteration: 186 of 241\ttrain_loss: 1.7670\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8634\n",
      "Iteration: 190 of 241\ttrain_loss: 1.5817\n",
      "Iteration: 192 of 241\ttrain_loss: 1.7593\n",
      "Iteration: 194 of 241\ttrain_loss: 1.7044\n",
      "Iteration: 196 of 241\ttrain_loss: 1.6553\n",
      "Iteration: 198 of 241\ttrain_loss: 1.6143\n",
      "Iteration: 200 of 241\ttrain_loss: 1.2830\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7778\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6704\n",
      "Iteration: 206 of 241\ttrain_loss: 1.7659\n",
      "Iteration: 208 of 241\ttrain_loss: 2.1792\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8461\n",
      "Iteration: 212 of 241\ttrain_loss: 1.5295\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7122\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8431\n",
      "Iteration: 218 of 241\ttrain_loss: 2.1857\n",
      "Iteration: 220 of 241\ttrain_loss: 1.4138\n",
      "Iteration: 222 of 241\ttrain_loss: 2.6383\n",
      "Iteration: 224 of 241\ttrain_loss: 2.1874\n",
      "Iteration: 226 of 241\ttrain_loss: 2.0256\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8818\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8102\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8330\n",
      "Iteration: 234 of 241\ttrain_loss: 1.3391\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7809\n",
      "Iteration: 238 of 241\ttrain_loss: 1.4946\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6869\n",
      "Iteration: 241 of 241\ttrain_loss: 1.3545\n",
      "Average Score for this Epoch: 1.7862164974212646\n",
      "-------------------- Epoch 82 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6676\n",
      "Iteration: 2 of 241\ttrain_loss: 1.3993\n",
      "Iteration: 4 of 241\ttrain_loss: 1.5683\n",
      "Iteration: 6 of 241\ttrain_loss: 1.7468\n",
      "Iteration: 8 of 241\ttrain_loss: 1.5959\n",
      "Iteration: 10 of 241\ttrain_loss: 1.9719\n",
      "Iteration: 12 of 241\ttrain_loss: 2.4292\n",
      "Iteration: 14 of 241\ttrain_loss: 1.5142\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7264\n",
      "Iteration: 18 of 241\ttrain_loss: 1.5106\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5314\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8144\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8088\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8478\n",
      "Iteration: 28 of 241\ttrain_loss: 1.4539\n",
      "Iteration: 30 of 241\ttrain_loss: 1.8007\n",
      "Iteration: 32 of 241\ttrain_loss: 1.6109\n",
      "Iteration: 34 of 241\ttrain_loss: 1.5547\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6720\n",
      "Iteration: 38 of 241\ttrain_loss: 1.5354\n",
      "Iteration: 40 of 241\ttrain_loss: 1.7198\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6554\n",
      "Iteration: 44 of 241\ttrain_loss: 1.4536\n",
      "Iteration: 46 of 241\ttrain_loss: 1.7566\n",
      "Iteration: 48 of 241\ttrain_loss: 1.7649\n",
      "Iteration: 50 of 241\ttrain_loss: 1.5123\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5830\n",
      "Iteration: 54 of 241\ttrain_loss: 1.6706\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8071\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7243\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7523\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7195\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7049\n",
      "Iteration: 66 of 241\ttrain_loss: 2.0617\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7989\n",
      "Iteration: 70 of 241\ttrain_loss: 1.7891\n",
      "Iteration: 72 of 241\ttrain_loss: 2.6897\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7119\n",
      "Iteration: 76 of 241\ttrain_loss: 1.4099\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8241\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6268\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8809\n",
      "Iteration: 84 of 241\ttrain_loss: 1.6789\n",
      "Iteration: 86 of 241\ttrain_loss: 1.5538\n",
      "Iteration: 88 of 241\ttrain_loss: 2.1712\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7114\n",
      "Iteration: 92 of 241\ttrain_loss: 1.4619\n",
      "Iteration: 94 of 241\ttrain_loss: 1.8787\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5731\n",
      "Iteration: 98 of 241\ttrain_loss: 2.0351\n",
      "Iteration: 100 of 241\ttrain_loss: 2.3920\n",
      "Iteration: 102 of 241\ttrain_loss: 2.0940\n",
      "Iteration: 104 of 241\ttrain_loss: 1.7985\n",
      "Iteration: 106 of 241\ttrain_loss: 1.6698\n",
      "Iteration: 108 of 241\ttrain_loss: 1.8308\n",
      "Iteration: 110 of 241\ttrain_loss: 2.2413\n",
      "Iteration: 112 of 241\ttrain_loss: 2.0736\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4592\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4923\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6321\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8415\n",
      "Iteration: 122 of 241\ttrain_loss: 1.9562\n",
      "Iteration: 124 of 241\ttrain_loss: 1.2868\n",
      "Iteration: 126 of 241\ttrain_loss: 2.2893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 128 of 241\ttrain_loss: 1.6867\n",
      "Iteration: 130 of 241\ttrain_loss: 2.1962\n",
      "Iteration: 132 of 241\ttrain_loss: 1.4941\n",
      "Iteration: 134 of 241\ttrain_loss: 1.5123\n",
      "Iteration: 136 of 241\ttrain_loss: 1.5803\n",
      "Iteration: 138 of 241\ttrain_loss: 1.5513\n",
      "Iteration: 140 of 241\ttrain_loss: 1.8201\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5308\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6912\n",
      "Iteration: 146 of 241\ttrain_loss: 1.6248\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7403\n",
      "Iteration: 150 of 241\ttrain_loss: 1.7659\n",
      "Iteration: 152 of 241\ttrain_loss: 2.0936\n",
      "Iteration: 154 of 241\ttrain_loss: 1.4688\n",
      "Iteration: 156 of 241\ttrain_loss: 1.8117\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5032\n",
      "Iteration: 160 of 241\ttrain_loss: 1.5969\n",
      "Iteration: 162 of 241\ttrain_loss: 1.4576\n",
      "Iteration: 164 of 241\ttrain_loss: 1.7212\n",
      "Iteration: 166 of 241\ttrain_loss: 1.9089\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7336\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4720\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7084\n",
      "Iteration: 174 of 241\ttrain_loss: 1.4278\n",
      "Iteration: 176 of 241\ttrain_loss: 1.8948\n",
      "Iteration: 178 of 241\ttrain_loss: 1.7810\n",
      "Iteration: 180 of 241\ttrain_loss: 1.7766\n",
      "Iteration: 182 of 241\ttrain_loss: 1.5800\n",
      "Iteration: 184 of 241\ttrain_loss: 2.0331\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6764\n",
      "Iteration: 188 of 241\ttrain_loss: 1.7325\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7955\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1897\n",
      "Iteration: 194 of 241\ttrain_loss: 1.4759\n",
      "Iteration: 196 of 241\ttrain_loss: 1.9366\n",
      "Iteration: 198 of 241\ttrain_loss: 1.9434\n",
      "Iteration: 200 of 241\ttrain_loss: 1.6420\n",
      "Iteration: 202 of 241\ttrain_loss: 2.1839\n",
      "Iteration: 204 of 241\ttrain_loss: 1.8318\n",
      "Iteration: 206 of 241\ttrain_loss: 1.6752\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6584\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8015\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8974\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7576\n",
      "Iteration: 216 of 241\ttrain_loss: 2.1899\n",
      "Iteration: 218 of 241\ttrain_loss: 1.6854\n",
      "Iteration: 220 of 241\ttrain_loss: 1.8368\n",
      "Iteration: 222 of 241\ttrain_loss: 1.9696\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5410\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9264\n",
      "Iteration: 228 of 241\ttrain_loss: 2.2280\n",
      "Iteration: 230 of 241\ttrain_loss: 1.4337\n",
      "Iteration: 232 of 241\ttrain_loss: 2.0299\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7160\n",
      "Iteration: 236 of 241\ttrain_loss: 1.5369\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9345\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8982\n",
      "Iteration: 241 of 241\ttrain_loss: 1.5324\n",
      "Average Score for this Epoch: 1.748045563697815\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 83 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.5533\n",
      "Iteration: 2 of 241\ttrain_loss: 1.1413\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7546\n",
      "Iteration: 6 of 241\ttrain_loss: 1.5973\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6962\n",
      "Iteration: 10 of 241\ttrain_loss: 1.7268\n",
      "Iteration: 12 of 241\ttrain_loss: 1.3617\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8602\n",
      "Iteration: 16 of 241\ttrain_loss: 1.9806\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6139\n",
      "Iteration: 20 of 241\ttrain_loss: 1.9203\n",
      "Iteration: 22 of 241\ttrain_loss: 1.7358\n",
      "Iteration: 24 of 241\ttrain_loss: 1.5719\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8571\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7416\n",
      "Iteration: 30 of 241\ttrain_loss: 1.8731\n",
      "Iteration: 32 of 241\ttrain_loss: 2.0205\n",
      "Iteration: 34 of 241\ttrain_loss: 1.9074\n",
      "Iteration: 36 of 241\ttrain_loss: 1.5716\n",
      "Iteration: 38 of 241\ttrain_loss: 1.9465\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5235\n",
      "Iteration: 42 of 241\ttrain_loss: 2.0966\n",
      "Iteration: 44 of 241\ttrain_loss: 1.3828\n",
      "Iteration: 46 of 241\ttrain_loss: 1.9030\n",
      "Iteration: 48 of 241\ttrain_loss: 1.9827\n",
      "Iteration: 50 of 241\ttrain_loss: 1.6720\n",
      "Iteration: 52 of 241\ttrain_loss: 1.3552\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8430\n",
      "Iteration: 56 of 241\ttrain_loss: 1.6276\n",
      "Iteration: 58 of 241\ttrain_loss: 1.2408\n",
      "Iteration: 60 of 241\ttrain_loss: 2.0649\n",
      "Iteration: 62 of 241\ttrain_loss: 1.8136\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9568\n",
      "Iteration: 66 of 241\ttrain_loss: 1.9673\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7034\n",
      "Iteration: 70 of 241\ttrain_loss: 1.9370\n",
      "Iteration: 72 of 241\ttrain_loss: 2.1225\n",
      "Iteration: 74 of 241\ttrain_loss: 1.6101\n",
      "Iteration: 76 of 241\ttrain_loss: 1.9668\n",
      "Iteration: 78 of 241\ttrain_loss: 1.9286\n",
      "Iteration: 80 of 241\ttrain_loss: 1.9675\n",
      "Iteration: 82 of 241\ttrain_loss: 1.9279\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3351\n",
      "Iteration: 86 of 241\ttrain_loss: 1.4473\n",
      "Iteration: 88 of 241\ttrain_loss: 1.6172\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7797\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7187\n",
      "Iteration: 94 of 241\ttrain_loss: 1.8408\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4293\n",
      "Iteration: 98 of 241\ttrain_loss: 1.6715\n",
      "Iteration: 100 of 241\ttrain_loss: 1.6234\n",
      "Iteration: 102 of 241\ttrain_loss: 1.5357\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4759\n",
      "Iteration: 106 of 241\ttrain_loss: 1.6297\n",
      "Iteration: 108 of 241\ttrain_loss: 1.3134\n",
      "Iteration: 110 of 241\ttrain_loss: 1.6852\n",
      "Iteration: 112 of 241\ttrain_loss: 1.7977\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4312\n",
      "Iteration: 116 of 241\ttrain_loss: 1.7107\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7912\n",
      "Iteration: 120 of 241\ttrain_loss: 1.4825\n",
      "Iteration: 122 of 241\ttrain_loss: 2.1585\n",
      "Iteration: 124 of 241\ttrain_loss: 2.0312\n",
      "Iteration: 126 of 241\ttrain_loss: 2.0829\n",
      "Iteration: 128 of 241\ttrain_loss: 1.6005\n",
      "Iteration: 130 of 241\ttrain_loss: 2.4035\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6674\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7642\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8341\n",
      "Iteration: 138 of 241\ttrain_loss: 1.4456\n",
      "Iteration: 140 of 241\ttrain_loss: 1.4139\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6133\n",
      "Iteration: 144 of 241\ttrain_loss: 1.4351\n",
      "Iteration: 146 of 241\ttrain_loss: 1.8081\n",
      "Iteration: 148 of 241\ttrain_loss: 1.9318\n",
      "Iteration: 150 of 241\ttrain_loss: 1.5424\n",
      "Iteration: 152 of 241\ttrain_loss: 1.7333\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8072\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7274\n",
      "Iteration: 158 of 241\ttrain_loss: 2.0160\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6726\n",
      "Iteration: 162 of 241\ttrain_loss: 1.6900\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6904\n",
      "Iteration: 166 of 241\ttrain_loss: 1.8029\n",
      "Iteration: 168 of 241\ttrain_loss: 1.4140\n",
      "Iteration: 170 of 241\ttrain_loss: 1.6598\n",
      "Iteration: 172 of 241\ttrain_loss: 1.8423\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0365\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6560\n",
      "Iteration: 178 of 241\ttrain_loss: 1.7906\n",
      "Iteration: 180 of 241\ttrain_loss: 2.1671\n",
      "Iteration: 182 of 241\ttrain_loss: 1.8055\n",
      "Iteration: 184 of 241\ttrain_loss: 2.0616\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6703\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8472\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7587\n",
      "Iteration: 192 of 241\ttrain_loss: 1.5490\n",
      "Iteration: 194 of 241\ttrain_loss: 1.6626\n",
      "Iteration: 196 of 241\ttrain_loss: 1.6883\n",
      "Iteration: 198 of 241\ttrain_loss: 2.0617\n",
      "Iteration: 200 of 241\ttrain_loss: 2.0183\n",
      "Iteration: 202 of 241\ttrain_loss: 1.6485\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6993\n",
      "Iteration: 206 of 241\ttrain_loss: 1.6795\n",
      "Iteration: 208 of 241\ttrain_loss: 1.9668\n",
      "Iteration: 210 of 241\ttrain_loss: 1.5538\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8268\n",
      "Iteration: 214 of 241\ttrain_loss: 2.2298\n",
      "Iteration: 216 of 241\ttrain_loss: 1.7906\n",
      "Iteration: 218 of 241\ttrain_loss: 1.9821\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1142\n",
      "Iteration: 222 of 241\ttrain_loss: 1.8067\n",
      "Iteration: 224 of 241\ttrain_loss: 2.2558\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9301\n",
      "Iteration: 228 of 241\ttrain_loss: 1.3322\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6783\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7285\n",
      "Iteration: 234 of 241\ttrain_loss: 1.5732\n",
      "Iteration: 236 of 241\ttrain_loss: 3.0443\n",
      "Iteration: 238 of 241\ttrain_loss: 1.9631\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6678\n",
      "Iteration: 241 of 241\ttrain_loss: 1.5315\n",
      "Average Score for this Epoch: 1.7571772336959839\n",
      "-------------------- Epoch 84 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.4913\n",
      "Iteration: 2 of 241\ttrain_loss: 1.8237\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8359\n",
      "Iteration: 6 of 241\ttrain_loss: 2.2528\n",
      "Iteration: 8 of 241\ttrain_loss: 1.3322\n",
      "Iteration: 10 of 241\ttrain_loss: 2.4085\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7200\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9130\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7109\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8417\n",
      "Iteration: 20 of 241\ttrain_loss: 1.4478\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8883\n",
      "Iteration: 24 of 241\ttrain_loss: 1.6727\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8093\n",
      "Iteration: 28 of 241\ttrain_loss: 1.2888\n",
      "Iteration: 30 of 241\ttrain_loss: 1.7334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 32 of 241\ttrain_loss: 1.5719\n",
      "Iteration: 34 of 241\ttrain_loss: 1.6714\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0501\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7618\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5283\n",
      "Iteration: 42 of 241\ttrain_loss: 1.4422\n",
      "Iteration: 44 of 241\ttrain_loss: 1.7097\n",
      "Iteration: 46 of 241\ttrain_loss: 1.3424\n",
      "Iteration: 48 of 241\ttrain_loss: 1.8996\n",
      "Iteration: 50 of 241\ttrain_loss: 1.4008\n",
      "Iteration: 52 of 241\ttrain_loss: 1.2720\n",
      "Iteration: 54 of 241\ttrain_loss: 1.3535\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8614\n",
      "Iteration: 58 of 241\ttrain_loss: 1.5349\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8212\n",
      "Iteration: 62 of 241\ttrain_loss: 1.8622\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7162\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6783\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7878\n",
      "Iteration: 70 of 241\ttrain_loss: 1.5809\n",
      "Iteration: 72 of 241\ttrain_loss: 2.2896\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8751\n",
      "Iteration: 76 of 241\ttrain_loss: 1.4956\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8060\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6451\n",
      "Iteration: 82 of 241\ttrain_loss: 1.7870\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3970\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6981\n",
      "Iteration: 88 of 241\ttrain_loss: 1.5574\n",
      "Iteration: 90 of 241\ttrain_loss: 1.5801\n",
      "Iteration: 92 of 241\ttrain_loss: 1.6597\n",
      "Iteration: 94 of 241\ttrain_loss: 1.5219\n",
      "Iteration: 96 of 241\ttrain_loss: 1.6525\n",
      "Iteration: 98 of 241\ttrain_loss: 1.7148\n",
      "Iteration: 100 of 241\ttrain_loss: 1.5871\n",
      "Iteration: 102 of 241\ttrain_loss: 1.4716\n",
      "Iteration: 104 of 241\ttrain_loss: 1.7193\n",
      "Iteration: 106 of 241\ttrain_loss: 1.4950\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7365\n",
      "Iteration: 110 of 241\ttrain_loss: 1.6959\n",
      "Iteration: 112 of 241\ttrain_loss: 1.6415\n",
      "Iteration: 114 of 241\ttrain_loss: 1.6622\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5466\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7755\n",
      "Iteration: 120 of 241\ttrain_loss: 2.2434\n",
      "Iteration: 122 of 241\ttrain_loss: 1.3610\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7300\n",
      "Iteration: 126 of 241\ttrain_loss: 1.7660\n",
      "Iteration: 128 of 241\ttrain_loss: 1.4543\n",
      "Iteration: 130 of 241\ttrain_loss: 1.8951\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7753\n",
      "Iteration: 134 of 241\ttrain_loss: 1.9178\n",
      "Iteration: 136 of 241\ttrain_loss: 1.5126\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6384\n",
      "Iteration: 140 of 241\ttrain_loss: 1.5990\n",
      "Iteration: 142 of 241\ttrain_loss: 1.8282\n",
      "Iteration: 144 of 241\ttrain_loss: 2.3093\n",
      "Iteration: 146 of 241\ttrain_loss: 1.5614\n",
      "Iteration: 148 of 241\ttrain_loss: 1.8745\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0444\n",
      "Iteration: 152 of 241\ttrain_loss: 1.5748\n",
      "Iteration: 154 of 241\ttrain_loss: 1.6490\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7388\n",
      "Iteration: 158 of 241\ttrain_loss: 1.4968\n",
      "Iteration: 160 of 241\ttrain_loss: 1.8631\n",
      "Iteration: 162 of 241\ttrain_loss: 1.7119\n",
      "Iteration: 164 of 241\ttrain_loss: 1.7855\n",
      "Iteration: 166 of 241\ttrain_loss: 1.6029\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7593\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7995\n",
      "Iteration: 172 of 241\ttrain_loss: 1.5766\n",
      "Iteration: 174 of 241\ttrain_loss: 1.6818\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5807\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6191\n",
      "Iteration: 180 of 241\ttrain_loss: 2.9108\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7993\n",
      "Iteration: 184 of 241\ttrain_loss: 1.8854\n",
      "Iteration: 186 of 241\ttrain_loss: 1.9456\n",
      "Iteration: 188 of 241\ttrain_loss: 2.2145\n",
      "Iteration: 190 of 241\ttrain_loss: 1.6414\n",
      "Iteration: 192 of 241\ttrain_loss: 1.7953\n",
      "Iteration: 194 of 241\ttrain_loss: 1.7129\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8312\n",
      "Iteration: 198 of 241\ttrain_loss: 1.6921\n",
      "Iteration: 200 of 241\ttrain_loss: 1.7566\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8672\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6212\n",
      "Iteration: 206 of 241\ttrain_loss: 2.0353\n",
      "Iteration: 208 of 241\ttrain_loss: 1.9456\n",
      "Iteration: 210 of 241\ttrain_loss: 2.1006\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6731\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7506\n",
      "Iteration: 216 of 241\ttrain_loss: 2.0542\n",
      "Iteration: 218 of 241\ttrain_loss: 1.9564\n",
      "Iteration: 220 of 241\ttrain_loss: 1.9551\n",
      "Iteration: 222 of 241\ttrain_loss: 1.7475\n",
      "Iteration: 224 of 241\ttrain_loss: 1.3825\n",
      "Iteration: 226 of 241\ttrain_loss: 1.7460\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8271\n",
      "Iteration: 230 of 241\ttrain_loss: 2.4964\n",
      "Iteration: 232 of 241\ttrain_loss: 2.0222\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7528\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7601\n",
      "Iteration: 238 of 241\ttrain_loss: 1.5396\n",
      "Iteration: 240 of 241\ttrain_loss: 1.8465\n",
      "Iteration: 241 of 241\ttrain_loss: 1.6858\n",
      "Average Score for this Epoch: 1.7314231395721436\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 85 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6585\n",
      "Iteration: 2 of 241\ttrain_loss: 1.6089\n",
      "Iteration: 4 of 241\ttrain_loss: 1.5267\n",
      "Iteration: 6 of 241\ttrain_loss: 1.7321\n",
      "Iteration: 8 of 241\ttrain_loss: 1.7705\n",
      "Iteration: 10 of 241\ttrain_loss: 1.3233\n",
      "Iteration: 12 of 241\ttrain_loss: 1.3374\n",
      "Iteration: 14 of 241\ttrain_loss: 1.5210\n",
      "Iteration: 16 of 241\ttrain_loss: 1.3481\n",
      "Iteration: 18 of 241\ttrain_loss: 1.4434\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5371\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5780\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8397\n",
      "Iteration: 26 of 241\ttrain_loss: 1.0801\n",
      "Iteration: 28 of 241\ttrain_loss: 1.9972\n",
      "Iteration: 30 of 241\ttrain_loss: 1.9875\n",
      "Iteration: 32 of 241\ttrain_loss: 1.6517\n",
      "Iteration: 34 of 241\ttrain_loss: 1.6072\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9322\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7047\n",
      "Iteration: 40 of 241\ttrain_loss: 1.6351\n",
      "Iteration: 42 of 241\ttrain_loss: 1.5983\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5245\n",
      "Iteration: 46 of 241\ttrain_loss: 1.5220\n",
      "Iteration: 48 of 241\ttrain_loss: 1.5440\n",
      "Iteration: 50 of 241\ttrain_loss: 1.9366\n",
      "Iteration: 52 of 241\ttrain_loss: 1.3960\n",
      "Iteration: 54 of 241\ttrain_loss: 1.5666\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8287\n",
      "Iteration: 58 of 241\ttrain_loss: 1.9408\n",
      "Iteration: 60 of 241\ttrain_loss: 1.6024\n",
      "Iteration: 62 of 241\ttrain_loss: 1.6038\n",
      "Iteration: 64 of 241\ttrain_loss: 1.9990\n",
      "Iteration: 66 of 241\ttrain_loss: 1.5412\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9096\n",
      "Iteration: 70 of 241\ttrain_loss: 1.4916\n",
      "Iteration: 72 of 241\ttrain_loss: 2.2335\n",
      "Iteration: 74 of 241\ttrain_loss: 1.3880\n",
      "Iteration: 76 of 241\ttrain_loss: 1.3567\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8502\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6163\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8230\n",
      "Iteration: 84 of 241\ttrain_loss: 1.5365\n",
      "Iteration: 86 of 241\ttrain_loss: 1.4037\n",
      "Iteration: 88 of 241\ttrain_loss: 1.3078\n",
      "Iteration: 90 of 241\ttrain_loss: 2.2060\n",
      "Iteration: 92 of 241\ttrain_loss: 1.4303\n",
      "Iteration: 94 of 241\ttrain_loss: 1.8596\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4325\n",
      "Iteration: 98 of 241\ttrain_loss: 1.6497\n",
      "Iteration: 100 of 241\ttrain_loss: 1.3792\n",
      "Iteration: 102 of 241\ttrain_loss: 1.5921\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4947\n",
      "Iteration: 106 of 241\ttrain_loss: 1.5630\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7916\n",
      "Iteration: 110 of 241\ttrain_loss: 1.6184\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5455\n",
      "Iteration: 114 of 241\ttrain_loss: 1.3981\n",
      "Iteration: 116 of 241\ttrain_loss: 1.6020\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7573\n",
      "Iteration: 120 of 241\ttrain_loss: 1.6193\n",
      "Iteration: 122 of 241\ttrain_loss: 1.9579\n",
      "Iteration: 124 of 241\ttrain_loss: 1.3886\n",
      "Iteration: 126 of 241\ttrain_loss: 2.1540\n",
      "Iteration: 128 of 241\ttrain_loss: 1.5826\n",
      "Iteration: 130 of 241\ttrain_loss: 1.3853\n",
      "Iteration: 132 of 241\ttrain_loss: 1.9053\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7906\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8856\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6819\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0279\n",
      "Iteration: 142 of 241\ttrain_loss: 1.8845\n",
      "Iteration: 144 of 241\ttrain_loss: 1.9809\n",
      "Iteration: 146 of 241\ttrain_loss: 2.0364\n",
      "Iteration: 148 of 241\ttrain_loss: 1.4656\n",
      "Iteration: 150 of 241\ttrain_loss: 1.4991\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6847\n",
      "Iteration: 154 of 241\ttrain_loss: 1.7149\n",
      "Iteration: 156 of 241\ttrain_loss: 1.5075\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5013\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6538\n",
      "Iteration: 162 of 241\ttrain_loss: 1.5768\n",
      "Iteration: 164 of 241\ttrain_loss: 1.5856\n",
      "Iteration: 166 of 241\ttrain_loss: 1.8192\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7460\n",
      "Iteration: 170 of 241\ttrain_loss: 2.0246\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7426\n",
      "Iteration: 174 of 241\ttrain_loss: 1.6403\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6288\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1008\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6196\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7241\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 186 of 241\ttrain_loss: 2.2743\n",
      "Iteration: 188 of 241\ttrain_loss: 1.4658\n",
      "Iteration: 190 of 241\ttrain_loss: 1.6252\n",
      "Iteration: 192 of 241\ttrain_loss: 2.1175\n",
      "Iteration: 194 of 241\ttrain_loss: 1.8012\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8072\n",
      "Iteration: 198 of 241\ttrain_loss: 1.8293\n",
      "Iteration: 200 of 241\ttrain_loss: 1.7129\n",
      "Iteration: 202 of 241\ttrain_loss: 1.5541\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0357\n",
      "Iteration: 206 of 241\ttrain_loss: 1.4722\n",
      "Iteration: 208 of 241\ttrain_loss: 1.8336\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8688\n",
      "Iteration: 212 of 241\ttrain_loss: 1.9110\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7869\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8399\n",
      "Iteration: 218 of 241\ttrain_loss: 1.5014\n",
      "Iteration: 220 of 241\ttrain_loss: 1.5884\n",
      "Iteration: 222 of 241\ttrain_loss: 1.7301\n",
      "Iteration: 224 of 241\ttrain_loss: 2.0929\n",
      "Iteration: 226 of 241\ttrain_loss: 2.4768\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9982\n",
      "Iteration: 230 of 241\ttrain_loss: 1.5070\n",
      "Iteration: 232 of 241\ttrain_loss: 1.4385\n",
      "Iteration: 234 of 241\ttrain_loss: 1.6396\n",
      "Iteration: 236 of 241\ttrain_loss: 1.6537\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7798\n",
      "Iteration: 240 of 241\ttrain_loss: 1.3784\n",
      "Iteration: 241 of 241\ttrain_loss: 1.4898\n",
      "Average Score for this Epoch: 1.7019513845443726\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 86 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.3362\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7089\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4029\n",
      "Iteration: 6 of 241\ttrain_loss: 1.1971\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6634\n",
      "Iteration: 10 of 241\ttrain_loss: 1.7911\n",
      "Iteration: 12 of 241\ttrain_loss: 1.5234\n",
      "Iteration: 14 of 241\ttrain_loss: 1.2699\n",
      "Iteration: 16 of 241\ttrain_loss: 1.1506\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6909\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5495\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5897\n",
      "Iteration: 24 of 241\ttrain_loss: 1.6462\n",
      "Iteration: 26 of 241\ttrain_loss: 1.4558\n",
      "Iteration: 28 of 241\ttrain_loss: 1.8678\n",
      "Iteration: 30 of 241\ttrain_loss: 1.6394\n",
      "Iteration: 32 of 241\ttrain_loss: 1.3057\n",
      "Iteration: 34 of 241\ttrain_loss: 1.8217\n",
      "Iteration: 36 of 241\ttrain_loss: 2.0024\n",
      "Iteration: 38 of 241\ttrain_loss: 1.6846\n",
      "Iteration: 40 of 241\ttrain_loss: 2.1603\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6738\n",
      "Iteration: 44 of 241\ttrain_loss: 1.4157\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8201\n",
      "Iteration: 48 of 241\ttrain_loss: 2.2633\n",
      "Iteration: 50 of 241\ttrain_loss: 1.8172\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5860\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8928\n",
      "Iteration: 56 of 241\ttrain_loss: 1.6515\n",
      "Iteration: 58 of 241\ttrain_loss: 2.0267\n",
      "Iteration: 60 of 241\ttrain_loss: 2.2033\n",
      "Iteration: 62 of 241\ttrain_loss: 1.3507\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7682\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1544\n",
      "Iteration: 68 of 241\ttrain_loss: 1.6130\n",
      "Iteration: 70 of 241\ttrain_loss: 1.9386\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8991\n",
      "Iteration: 74 of 241\ttrain_loss: 1.6858\n",
      "Iteration: 76 of 241\ttrain_loss: 2.0409\n",
      "Iteration: 78 of 241\ttrain_loss: 1.5102\n",
      "Iteration: 80 of 241\ttrain_loss: 1.8175\n",
      "Iteration: 82 of 241\ttrain_loss: 1.3430\n",
      "Iteration: 84 of 241\ttrain_loss: 1.6363\n",
      "Iteration: 86 of 241\ttrain_loss: 1.3028\n",
      "Iteration: 88 of 241\ttrain_loss: 1.6798\n",
      "Iteration: 90 of 241\ttrain_loss: 1.3930\n",
      "Iteration: 92 of 241\ttrain_loss: 1.3381\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9015\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5925\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9356\n",
      "Iteration: 100 of 241\ttrain_loss: 2.1127\n",
      "Iteration: 102 of 241\ttrain_loss: 1.8305\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9356\n",
      "Iteration: 106 of 241\ttrain_loss: 1.6476\n",
      "Iteration: 108 of 241\ttrain_loss: 1.5030\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9072\n",
      "Iteration: 112 of 241\ttrain_loss: 1.6499\n",
      "Iteration: 114 of 241\ttrain_loss: 2.0287\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5808\n",
      "Iteration: 118 of 241\ttrain_loss: 1.4552\n",
      "Iteration: 120 of 241\ttrain_loss: 1.4345\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5616\n",
      "Iteration: 124 of 241\ttrain_loss: 1.9410\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8333\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8857\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7009\n",
      "Iteration: 132 of 241\ttrain_loss: 1.1252\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7427\n",
      "Iteration: 136 of 241\ttrain_loss: 1.5308\n",
      "Iteration: 138 of 241\ttrain_loss: 1.5499\n",
      "Iteration: 140 of 241\ttrain_loss: 1.7940\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6120\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6061\n",
      "Iteration: 146 of 241\ttrain_loss: 1.6977\n",
      "Iteration: 148 of 241\ttrain_loss: 1.8296\n",
      "Iteration: 150 of 241\ttrain_loss: 1.9300\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6295\n",
      "Iteration: 154 of 241\ttrain_loss: 1.4042\n",
      "Iteration: 156 of 241\ttrain_loss: 1.6443\n",
      "Iteration: 158 of 241\ttrain_loss: 1.7211\n",
      "Iteration: 160 of 241\ttrain_loss: 2.6572\n",
      "Iteration: 162 of 241\ttrain_loss: 1.6894\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6834\n",
      "Iteration: 166 of 241\ttrain_loss: 1.6496\n",
      "Iteration: 168 of 241\ttrain_loss: 1.3591\n",
      "Iteration: 170 of 241\ttrain_loss: 2.1738\n",
      "Iteration: 172 of 241\ttrain_loss: 1.4952\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5712\n",
      "Iteration: 176 of 241\ttrain_loss: 1.7187\n",
      "Iteration: 178 of 241\ttrain_loss: 1.9299\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8141\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6249\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7195\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6841\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8504\n",
      "Iteration: 190 of 241\ttrain_loss: 2.0534\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0007\n",
      "Iteration: 194 of 241\ttrain_loss: 1.8115\n",
      "Iteration: 196 of 241\ttrain_loss: 2.3264\n",
      "Iteration: 198 of 241\ttrain_loss: 1.7973\n",
      "Iteration: 200 of 241\ttrain_loss: 1.6567\n",
      "Iteration: 202 of 241\ttrain_loss: 1.1837\n",
      "Iteration: 204 of 241\ttrain_loss: 1.8427\n",
      "Iteration: 206 of 241\ttrain_loss: 1.4442\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6707\n",
      "Iteration: 210 of 241\ttrain_loss: 1.4343\n",
      "Iteration: 212 of 241\ttrain_loss: 1.5055\n",
      "Iteration: 214 of 241\ttrain_loss: 1.2453\n",
      "Iteration: 216 of 241\ttrain_loss: 2.0194\n",
      "Iteration: 218 of 241\ttrain_loss: 1.6662\n",
      "Iteration: 220 of 241\ttrain_loss: 2.3433\n",
      "Iteration: 222 of 241\ttrain_loss: 1.3715\n",
      "Iteration: 224 of 241\ttrain_loss: 1.8767\n",
      "Iteration: 226 of 241\ttrain_loss: 1.3924\n",
      "Iteration: 228 of 241\ttrain_loss: 1.4050\n",
      "Iteration: 230 of 241\ttrain_loss: 1.7380\n",
      "Iteration: 232 of 241\ttrain_loss: 1.9598\n",
      "Iteration: 234 of 241\ttrain_loss: 1.5393\n",
      "Iteration: 236 of 241\ttrain_loss: 1.4284\n",
      "Iteration: 238 of 241\ttrain_loss: 1.8073\n",
      "Iteration: 240 of 241\ttrain_loss: 1.2611\n",
      "Iteration: 241 of 241\ttrain_loss: 2.3154\n",
      "Average Score for this Epoch: 1.69547438621521\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 87 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.1226\n",
      "Iteration: 2 of 241\ttrain_loss: 1.3969\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4082\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6378\n",
      "Iteration: 8 of 241\ttrain_loss: 1.4005\n",
      "Iteration: 10 of 241\ttrain_loss: 1.5134\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6950\n",
      "Iteration: 14 of 241\ttrain_loss: 1.9789\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4229\n",
      "Iteration: 18 of 241\ttrain_loss: 2.0118\n",
      "Iteration: 20 of 241\ttrain_loss: 1.3681\n",
      "Iteration: 22 of 241\ttrain_loss: 1.6065\n",
      "Iteration: 24 of 241\ttrain_loss: 1.5092\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6135\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7833\n",
      "Iteration: 30 of 241\ttrain_loss: 2.0842\n",
      "Iteration: 32 of 241\ttrain_loss: 1.8478\n",
      "Iteration: 34 of 241\ttrain_loss: 2.1697\n",
      "Iteration: 36 of 241\ttrain_loss: 1.3393\n",
      "Iteration: 38 of 241\ttrain_loss: 2.1313\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5261\n",
      "Iteration: 42 of 241\ttrain_loss: 2.0815\n",
      "Iteration: 44 of 241\ttrain_loss: 2.1290\n",
      "Iteration: 46 of 241\ttrain_loss: 1.2514\n",
      "Iteration: 48 of 241\ttrain_loss: 1.9918\n",
      "Iteration: 50 of 241\ttrain_loss: 1.5521\n",
      "Iteration: 52 of 241\ttrain_loss: 1.4862\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8310\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8118\n",
      "Iteration: 58 of 241\ttrain_loss: 1.4357\n",
      "Iteration: 60 of 241\ttrain_loss: 1.6304\n",
      "Iteration: 62 of 241\ttrain_loss: 1.5794\n",
      "Iteration: 64 of 241\ttrain_loss: 1.5441\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6613\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7624\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6575\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8791\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7439\n",
      "Iteration: 76 of 241\ttrain_loss: 1.7127\n",
      "Iteration: 78 of 241\ttrain_loss: 1.9922\n",
      "Iteration: 80 of 241\ttrain_loss: 2.0282\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5681\n",
      "Iteration: 84 of 241\ttrain_loss: 1.2873\n",
      "Iteration: 86 of 241\ttrain_loss: 1.4660\n",
      "Iteration: 88 of 241\ttrain_loss: 1.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 90 of 241\ttrain_loss: 1.5518\n",
      "Iteration: 92 of 241\ttrain_loss: 1.9904\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9828\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5699\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9376\n",
      "Iteration: 100 of 241\ttrain_loss: 1.7092\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9106\n",
      "Iteration: 104 of 241\ttrain_loss: 2.4791\n",
      "Iteration: 106 of 241\ttrain_loss: 1.6942\n",
      "Iteration: 108 of 241\ttrain_loss: 1.8911\n",
      "Iteration: 110 of 241\ttrain_loss: 1.9418\n",
      "Iteration: 112 of 241\ttrain_loss: 1.9361\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5944\n",
      "Iteration: 116 of 241\ttrain_loss: 1.7073\n",
      "Iteration: 118 of 241\ttrain_loss: 1.3134\n",
      "Iteration: 120 of 241\ttrain_loss: 1.7184\n",
      "Iteration: 122 of 241\ttrain_loss: 1.6722\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5390\n",
      "Iteration: 126 of 241\ttrain_loss: 1.7549\n",
      "Iteration: 128 of 241\ttrain_loss: 1.5605\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7027\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6700\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7645\n",
      "Iteration: 136 of 241\ttrain_loss: 1.7585\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6660\n",
      "Iteration: 140 of 241\ttrain_loss: 1.9875\n",
      "Iteration: 142 of 241\ttrain_loss: 1.8368\n",
      "Iteration: 144 of 241\ttrain_loss: 1.5582\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4630\n",
      "Iteration: 148 of 241\ttrain_loss: 1.6392\n",
      "Iteration: 150 of 241\ttrain_loss: 1.7777\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8850\n",
      "Iteration: 154 of 241\ttrain_loss: 1.3676\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7452\n",
      "Iteration: 158 of 241\ttrain_loss: 1.9302\n",
      "Iteration: 160 of 241\ttrain_loss: 1.8318\n",
      "Iteration: 162 of 241\ttrain_loss: 1.4860\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6404\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4403\n",
      "Iteration: 168 of 241\ttrain_loss: 1.5725\n",
      "Iteration: 170 of 241\ttrain_loss: 2.0150\n",
      "Iteration: 172 of 241\ttrain_loss: 1.6943\n",
      "Iteration: 174 of 241\ttrain_loss: 2.0059\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5961\n",
      "Iteration: 178 of 241\ttrain_loss: 1.4504\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8596\n",
      "Iteration: 182 of 241\ttrain_loss: 1.4324\n",
      "Iteration: 184 of 241\ttrain_loss: 2.4352\n",
      "Iteration: 186 of 241\ttrain_loss: 1.9896\n",
      "Iteration: 188 of 241\ttrain_loss: 1.5657\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7589\n",
      "Iteration: 192 of 241\ttrain_loss: 1.5951\n",
      "Iteration: 194 of 241\ttrain_loss: 1.7882\n",
      "Iteration: 196 of 241\ttrain_loss: 1.3411\n",
      "Iteration: 198 of 241\ttrain_loss: 1.5891\n",
      "Iteration: 200 of 241\ttrain_loss: 1.5947\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8775\n",
      "Iteration: 204 of 241\ttrain_loss: 1.7412\n",
      "Iteration: 206 of 241\ttrain_loss: 1.8319\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6873\n",
      "Iteration: 210 of 241\ttrain_loss: 1.4362\n",
      "Iteration: 212 of 241\ttrain_loss: 1.7549\n",
      "Iteration: 214 of 241\ttrain_loss: 2.4210\n",
      "Iteration: 216 of 241\ttrain_loss: 1.5628\n",
      "Iteration: 218 of 241\ttrain_loss: 1.2021\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6614\n",
      "Iteration: 222 of 241\ttrain_loss: 1.4837\n",
      "Iteration: 224 of 241\ttrain_loss: 1.6917\n",
      "Iteration: 226 of 241\ttrain_loss: 1.2787\n",
      "Iteration: 228 of 241\ttrain_loss: 1.7492\n",
      "Iteration: 230 of 241\ttrain_loss: 2.6052\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8456\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7742\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7414\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7494\n",
      "Iteration: 240 of 241\ttrain_loss: 1.4131\n",
      "Iteration: 241 of 241\ttrain_loss: 1.2157\n",
      "Average Score for this Epoch: 1.7014135122299194\n",
      "-------------------- Epoch 88 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6985\n",
      "Iteration: 2 of 241\ttrain_loss: 1.9318\n",
      "Iteration: 4 of 241\ttrain_loss: 1.3732\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9560\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6577\n",
      "Iteration: 10 of 241\ttrain_loss: 1.5181\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7366\n",
      "Iteration: 14 of 241\ttrain_loss: 1.5080\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4326\n",
      "Iteration: 18 of 241\ttrain_loss: 1.8968\n",
      "Iteration: 20 of 241\ttrain_loss: 1.7485\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8060\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8385\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8559\n",
      "Iteration: 28 of 241\ttrain_loss: 2.0595\n",
      "Iteration: 30 of 241\ttrain_loss: 2.0466\n",
      "Iteration: 32 of 241\ttrain_loss: 1.5260\n",
      "Iteration: 34 of 241\ttrain_loss: 1.7145\n",
      "Iteration: 36 of 241\ttrain_loss: 1.3366\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7341\n",
      "Iteration: 40 of 241\ttrain_loss: 1.7068\n",
      "Iteration: 42 of 241\ttrain_loss: 1.8078\n",
      "Iteration: 44 of 241\ttrain_loss: 1.6085\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8999\n",
      "Iteration: 48 of 241\ttrain_loss: 1.3837\n",
      "Iteration: 50 of 241\ttrain_loss: 1.7047\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5352\n",
      "Iteration: 54 of 241\ttrain_loss: 1.7752\n",
      "Iteration: 56 of 241\ttrain_loss: 1.8223\n",
      "Iteration: 58 of 241\ttrain_loss: 1.8321\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7847\n",
      "Iteration: 62 of 241\ttrain_loss: 1.6727\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7111\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6971\n",
      "Iteration: 68 of 241\ttrain_loss: 1.7625\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6737\n",
      "Iteration: 72 of 241\ttrain_loss: 1.5763\n",
      "Iteration: 74 of 241\ttrain_loss: 2.2884\n",
      "Iteration: 76 of 241\ttrain_loss: 1.5707\n",
      "Iteration: 78 of 241\ttrain_loss: 1.6912\n",
      "Iteration: 80 of 241\ttrain_loss: 1.7719\n",
      "Iteration: 82 of 241\ttrain_loss: 1.3814\n",
      "Iteration: 84 of 241\ttrain_loss: 1.6325\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6326\n",
      "Iteration: 88 of 241\ttrain_loss: 1.5448\n",
      "Iteration: 90 of 241\ttrain_loss: 1.6471\n",
      "Iteration: 92 of 241\ttrain_loss: 1.5744\n",
      "Iteration: 94 of 241\ttrain_loss: 1.3711\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5812\n",
      "Iteration: 98 of 241\ttrain_loss: 1.1038\n",
      "Iteration: 100 of 241\ttrain_loss: 1.7605\n",
      "Iteration: 102 of 241\ttrain_loss: 1.5535\n",
      "Iteration: 104 of 241\ttrain_loss: 1.6825\n",
      "Iteration: 106 of 241\ttrain_loss: 1.3729\n",
      "Iteration: 108 of 241\ttrain_loss: 2.0243\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8286\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5921\n",
      "Iteration: 114 of 241\ttrain_loss: 1.1540\n",
      "Iteration: 116 of 241\ttrain_loss: 2.0845\n",
      "Iteration: 118 of 241\ttrain_loss: 1.9345\n",
      "Iteration: 120 of 241\ttrain_loss: 1.3821\n",
      "Iteration: 122 of 241\ttrain_loss: 1.6394\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5948\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6467\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8256\n",
      "Iteration: 130 of 241\ttrain_loss: 1.4916\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7304\n",
      "Iteration: 134 of 241\ttrain_loss: 1.2441\n",
      "Iteration: 136 of 241\ttrain_loss: 1.6971\n",
      "Iteration: 138 of 241\ttrain_loss: 1.4201\n",
      "Iteration: 140 of 241\ttrain_loss: 1.8822\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5748\n",
      "Iteration: 144 of 241\ttrain_loss: 1.4519\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4565\n",
      "Iteration: 148 of 241\ttrain_loss: 2.1745\n",
      "Iteration: 150 of 241\ttrain_loss: 1.3251\n",
      "Iteration: 152 of 241\ttrain_loss: 1.5761\n",
      "Iteration: 154 of 241\ttrain_loss: 1.7408\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7337\n",
      "Iteration: 158 of 241\ttrain_loss: 2.1548\n",
      "Iteration: 160 of 241\ttrain_loss: 1.4599\n",
      "Iteration: 162 of 241\ttrain_loss: 1.7652\n",
      "Iteration: 164 of 241\ttrain_loss: 1.2133\n",
      "Iteration: 166 of 241\ttrain_loss: 1.5007\n",
      "Iteration: 168 of 241\ttrain_loss: 1.6216\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4853\n",
      "Iteration: 172 of 241\ttrain_loss: 1.5188\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5973\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6205\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6288\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8228\n",
      "Iteration: 182 of 241\ttrain_loss: 1.9602\n",
      "Iteration: 184 of 241\ttrain_loss: 1.6546\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4820\n",
      "Iteration: 188 of 241\ttrain_loss: 1.5367\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1549\n",
      "Iteration: 192 of 241\ttrain_loss: 1.6177\n",
      "Iteration: 194 of 241\ttrain_loss: 2.0308\n",
      "Iteration: 196 of 241\ttrain_loss: 1.6620\n",
      "Iteration: 198 of 241\ttrain_loss: 2.3584\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8906\n",
      "Iteration: 202 of 241\ttrain_loss: 1.9030\n",
      "Iteration: 204 of 241\ttrain_loss: 1.7076\n",
      "Iteration: 206 of 241\ttrain_loss: 1.3799\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6950\n",
      "Iteration: 210 of 241\ttrain_loss: 1.6592\n",
      "Iteration: 212 of 241\ttrain_loss: 1.7873\n",
      "Iteration: 214 of 241\ttrain_loss: 1.4641\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9309\n",
      "Iteration: 218 of 241\ttrain_loss: 1.9542\n",
      "Iteration: 220 of 241\ttrain_loss: 2.0631\n",
      "Iteration: 222 of 241\ttrain_loss: 1.3652\n",
      "Iteration: 224 of 241\ttrain_loss: 1.3670\n",
      "Iteration: 226 of 241\ttrain_loss: 2.3470\n",
      "Iteration: 228 of 241\ttrain_loss: 1.4149\n",
      "Iteration: 230 of 241\ttrain_loss: 1.9572\n",
      "Iteration: 232 of 241\ttrain_loss: 1.9440\n",
      "Iteration: 234 of 241\ttrain_loss: 1.9693\n",
      "Iteration: 236 of 241\ttrain_loss: 2.0085\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7133\n",
      "Iteration: 240 of 241\ttrain_loss: 1.7960\n",
      "Iteration: 241 of 241\ttrain_loss: 1.5968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score for this Epoch: 1.682234287261963\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 89 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.4116\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7288\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8931\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6660\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6942\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6010\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6136\n",
      "Iteration: 14 of 241\ttrain_loss: 1.1266\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5676\n",
      "Iteration: 18 of 241\ttrain_loss: 1.6999\n",
      "Iteration: 20 of 241\ttrain_loss: 2.1058\n",
      "Iteration: 22 of 241\ttrain_loss: 1.7738\n",
      "Iteration: 24 of 241\ttrain_loss: 1.4552\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8063\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7105\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4823\n",
      "Iteration: 32 of 241\ttrain_loss: 1.3577\n",
      "Iteration: 34 of 241\ttrain_loss: 1.4640\n",
      "Iteration: 36 of 241\ttrain_loss: 1.9439\n",
      "Iteration: 38 of 241\ttrain_loss: 1.8862\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5946\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6023\n",
      "Iteration: 44 of 241\ttrain_loss: 1.6422\n",
      "Iteration: 46 of 241\ttrain_loss: 1.2580\n",
      "Iteration: 48 of 241\ttrain_loss: 1.9069\n",
      "Iteration: 50 of 241\ttrain_loss: 1.6678\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5528\n",
      "Iteration: 54 of 241\ttrain_loss: 1.8489\n",
      "Iteration: 56 of 241\ttrain_loss: 1.6420\n",
      "Iteration: 58 of 241\ttrain_loss: 1.3261\n",
      "Iteration: 60 of 241\ttrain_loss: 1.9304\n",
      "Iteration: 62 of 241\ttrain_loss: 1.4798\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8839\n",
      "Iteration: 66 of 241\ttrain_loss: 1.4882\n",
      "Iteration: 68 of 241\ttrain_loss: 1.5265\n",
      "Iteration: 70 of 241\ttrain_loss: 1.7146\n",
      "Iteration: 72 of 241\ttrain_loss: 1.4548\n",
      "Iteration: 74 of 241\ttrain_loss: 1.5480\n",
      "Iteration: 76 of 241\ttrain_loss: 1.1040\n",
      "Iteration: 78 of 241\ttrain_loss: 1.5009\n",
      "Iteration: 80 of 241\ttrain_loss: 1.5807\n",
      "Iteration: 82 of 241\ttrain_loss: 1.7361\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0491\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6541\n",
      "Iteration: 88 of 241\ttrain_loss: 1.4342\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0592\n",
      "Iteration: 92 of 241\ttrain_loss: 1.2076\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6954\n",
      "Iteration: 96 of 241\ttrain_loss: 1.9549\n",
      "Iteration: 98 of 241\ttrain_loss: 1.1659\n",
      "Iteration: 100 of 241\ttrain_loss: 1.4957\n",
      "Iteration: 102 of 241\ttrain_loss: 1.5411\n",
      "Iteration: 104 of 241\ttrain_loss: 1.2574\n",
      "Iteration: 106 of 241\ttrain_loss: 1.7972\n",
      "Iteration: 108 of 241\ttrain_loss: 1.7659\n",
      "Iteration: 110 of 241\ttrain_loss: 1.2580\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5036\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5787\n",
      "Iteration: 116 of 241\ttrain_loss: 1.5034\n",
      "Iteration: 118 of 241\ttrain_loss: 1.4396\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8279\n",
      "Iteration: 122 of 241\ttrain_loss: 1.0426\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7208\n",
      "Iteration: 126 of 241\ttrain_loss: 1.3286\n",
      "Iteration: 128 of 241\ttrain_loss: 1.6392\n",
      "Iteration: 130 of 241\ttrain_loss: 1.6579\n",
      "Iteration: 132 of 241\ttrain_loss: 1.8541\n",
      "Iteration: 134 of 241\ttrain_loss: 1.5395\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8431\n",
      "Iteration: 138 of 241\ttrain_loss: 2.2089\n",
      "Iteration: 140 of 241\ttrain_loss: 1.9389\n",
      "Iteration: 142 of 241\ttrain_loss: 1.4927\n",
      "Iteration: 144 of 241\ttrain_loss: 1.2228\n",
      "Iteration: 146 of 241\ttrain_loss: 1.7540\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7823\n",
      "Iteration: 150 of 241\ttrain_loss: 1.2506\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6104\n",
      "Iteration: 154 of 241\ttrain_loss: 1.5000\n",
      "Iteration: 156 of 241\ttrain_loss: 1.8405\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5161\n",
      "Iteration: 160 of 241\ttrain_loss: 1.5431\n",
      "Iteration: 162 of 241\ttrain_loss: 1.3124\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9439\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4383\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7198\n",
      "Iteration: 170 of 241\ttrain_loss: 1.3285\n",
      "Iteration: 172 of 241\ttrain_loss: 1.4550\n",
      "Iteration: 174 of 241\ttrain_loss: 1.3500\n",
      "Iteration: 176 of 241\ttrain_loss: 1.7985\n",
      "Iteration: 178 of 241\ttrain_loss: 1.3926\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8486\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7732\n",
      "Iteration: 184 of 241\ttrain_loss: 1.3489\n",
      "Iteration: 186 of 241\ttrain_loss: 1.9855\n",
      "Iteration: 188 of 241\ttrain_loss: 2.0156\n",
      "Iteration: 190 of 241\ttrain_loss: 1.3554\n",
      "Iteration: 192 of 241\ttrain_loss: 1.5273\n",
      "Iteration: 194 of 241\ttrain_loss: 1.5911\n",
      "Iteration: 196 of 241\ttrain_loss: 1.7686\n",
      "Iteration: 198 of 241\ttrain_loss: 1.3077\n",
      "Iteration: 200 of 241\ttrain_loss: 1.0693\n",
      "Iteration: 202 of 241\ttrain_loss: 1.7322\n",
      "Iteration: 204 of 241\ttrain_loss: 1.8023\n",
      "Iteration: 206 of 241\ttrain_loss: 1.7064\n",
      "Iteration: 208 of 241\ttrain_loss: 1.4573\n",
      "Iteration: 210 of 241\ttrain_loss: 1.5699\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6039\n",
      "Iteration: 214 of 241\ttrain_loss: 1.1498\n",
      "Iteration: 216 of 241\ttrain_loss: 1.6704\n",
      "Iteration: 218 of 241\ttrain_loss: 1.8261\n",
      "Iteration: 220 of 241\ttrain_loss: 1.9477\n",
      "Iteration: 222 of 241\ttrain_loss: 1.3074\n",
      "Iteration: 224 of 241\ttrain_loss: 1.7742\n",
      "Iteration: 226 of 241\ttrain_loss: 1.8078\n",
      "Iteration: 228 of 241\ttrain_loss: 2.4625\n",
      "Iteration: 230 of 241\ttrain_loss: 1.7834\n",
      "Iteration: 232 of 241\ttrain_loss: 1.8759\n",
      "Iteration: 234 of 241\ttrain_loss: 1.9172\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9822\n",
      "Iteration: 238 of 241\ttrain_loss: 1.4610\n",
      "Iteration: 240 of 241\ttrain_loss: 2.1339\n",
      "Iteration: 241 of 241\ttrain_loss: 1.5516\n",
      "Average Score for this Epoch: 1.6454066038131714\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 90 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 2.1234\n",
      "Iteration: 2 of 241\ttrain_loss: 1.2279\n",
      "Iteration: 4 of 241\ttrain_loss: 1.5973\n",
      "Iteration: 6 of 241\ttrain_loss: 1.2540\n",
      "Iteration: 8 of 241\ttrain_loss: 1.5299\n",
      "Iteration: 10 of 241\ttrain_loss: 1.7781\n",
      "Iteration: 12 of 241\ttrain_loss: 1.3023\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8607\n",
      "Iteration: 16 of 241\ttrain_loss: 1.3243\n",
      "Iteration: 18 of 241\ttrain_loss: 1.7143\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5081\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5533\n",
      "Iteration: 24 of 241\ttrain_loss: 1.1609\n",
      "Iteration: 26 of 241\ttrain_loss: 1.5686\n",
      "Iteration: 28 of 241\ttrain_loss: 1.2383\n",
      "Iteration: 30 of 241\ttrain_loss: 0.9840\n",
      "Iteration: 32 of 241\ttrain_loss: 1.6614\n",
      "Iteration: 34 of 241\ttrain_loss: 2.2382\n",
      "Iteration: 36 of 241\ttrain_loss: 1.5819\n",
      "Iteration: 38 of 241\ttrain_loss: 1.2497\n",
      "Iteration: 40 of 241\ttrain_loss: 1.3863\n",
      "Iteration: 42 of 241\ttrain_loss: 1.1910\n",
      "Iteration: 44 of 241\ttrain_loss: 1.9031\n",
      "Iteration: 46 of 241\ttrain_loss: 1.0616\n",
      "Iteration: 48 of 241\ttrain_loss: 1.0162\n",
      "Iteration: 50 of 241\ttrain_loss: 1.7162\n",
      "Iteration: 52 of 241\ttrain_loss: 1.8442\n",
      "Iteration: 54 of 241\ttrain_loss: 1.7948\n",
      "Iteration: 56 of 241\ttrain_loss: 1.4123\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7117\n",
      "Iteration: 60 of 241\ttrain_loss: 1.2972\n",
      "Iteration: 62 of 241\ttrain_loss: 1.6507\n",
      "Iteration: 64 of 241\ttrain_loss: 1.6141\n",
      "Iteration: 66 of 241\ttrain_loss: 1.8270\n",
      "Iteration: 68 of 241\ttrain_loss: 1.6123\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6101\n",
      "Iteration: 72 of 241\ttrain_loss: 1.5127\n",
      "Iteration: 74 of 241\ttrain_loss: 2.1204\n",
      "Iteration: 76 of 241\ttrain_loss: 1.3737\n",
      "Iteration: 78 of 241\ttrain_loss: 1.3744\n",
      "Iteration: 80 of 241\ttrain_loss: 1.5945\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5155\n",
      "Iteration: 84 of 241\ttrain_loss: 1.1599\n",
      "Iteration: 86 of 241\ttrain_loss: 1.3564\n",
      "Iteration: 88 of 241\ttrain_loss: 1.8231\n",
      "Iteration: 90 of 241\ttrain_loss: 1.5865\n",
      "Iteration: 92 of 241\ttrain_loss: 1.8758\n",
      "Iteration: 94 of 241\ttrain_loss: 1.3437\n",
      "Iteration: 96 of 241\ttrain_loss: 1.1681\n",
      "Iteration: 98 of 241\ttrain_loss: 1.4977\n",
      "Iteration: 100 of 241\ttrain_loss: 1.9723\n",
      "Iteration: 102 of 241\ttrain_loss: 1.7722\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4162\n",
      "Iteration: 106 of 241\ttrain_loss: 1.5892\n",
      "Iteration: 108 of 241\ttrain_loss: 1.6200\n",
      "Iteration: 110 of 241\ttrain_loss: 1.3100\n",
      "Iteration: 112 of 241\ttrain_loss: 1.7093\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4052\n",
      "Iteration: 116 of 241\ttrain_loss: 1.8811\n",
      "Iteration: 118 of 241\ttrain_loss: 1.4948\n",
      "Iteration: 120 of 241\ttrain_loss: 1.8352\n",
      "Iteration: 122 of 241\ttrain_loss: 1.0676\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7774\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6294\n",
      "Iteration: 128 of 241\ttrain_loss: 1.6696\n",
      "Iteration: 130 of 241\ttrain_loss: 2.2239\n",
      "Iteration: 132 of 241\ttrain_loss: 1.5873\n",
      "Iteration: 134 of 241\ttrain_loss: 1.7091\n",
      "Iteration: 136 of 241\ttrain_loss: 1.6854\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7521\n",
      "Iteration: 140 of 241\ttrain_loss: 2.0620\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6544\n",
      "Iteration: 144 of 241\ttrain_loss: 1.5013\n",
      "Iteration: 146 of 241\ttrain_loss: 1.7994\n",
      "Iteration: 148 of 241\ttrain_loss: 1.6418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 150 of 241\ttrain_loss: 2.0969\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6610\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8146\n",
      "Iteration: 156 of 241\ttrain_loss: 2.1511\n",
      "Iteration: 158 of 241\ttrain_loss: 1.8251\n",
      "Iteration: 160 of 241\ttrain_loss: 1.5037\n",
      "Iteration: 162 of 241\ttrain_loss: 1.9180\n",
      "Iteration: 164 of 241\ttrain_loss: 1.4453\n",
      "Iteration: 166 of 241\ttrain_loss: 2.0467\n",
      "Iteration: 168 of 241\ttrain_loss: 1.6674\n",
      "Iteration: 170 of 241\ttrain_loss: 1.2033\n",
      "Iteration: 172 of 241\ttrain_loss: 1.8387\n",
      "Iteration: 174 of 241\ttrain_loss: 1.3764\n",
      "Iteration: 176 of 241\ttrain_loss: 1.4221\n",
      "Iteration: 178 of 241\ttrain_loss: 1.4748\n",
      "Iteration: 180 of 241\ttrain_loss: 1.1068\n",
      "Iteration: 182 of 241\ttrain_loss: 1.9401\n",
      "Iteration: 184 of 241\ttrain_loss: 1.5743\n",
      "Iteration: 186 of 241\ttrain_loss: 1.3119\n",
      "Iteration: 188 of 241\ttrain_loss: 1.7542\n",
      "Iteration: 190 of 241\ttrain_loss: 1.5880\n",
      "Iteration: 192 of 241\ttrain_loss: 1.4765\n",
      "Iteration: 194 of 241\ttrain_loss: 1.9809\n",
      "Iteration: 196 of 241\ttrain_loss: 1.7531\n",
      "Iteration: 198 of 241\ttrain_loss: 1.7555\n",
      "Iteration: 200 of 241\ttrain_loss: 1.6499\n",
      "Iteration: 202 of 241\ttrain_loss: 2.0711\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6653\n",
      "Iteration: 206 of 241\ttrain_loss: 1.6642\n",
      "Iteration: 208 of 241\ttrain_loss: 1.9607\n",
      "Iteration: 210 of 241\ttrain_loss: 1.6522\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8190\n",
      "Iteration: 214 of 241\ttrain_loss: 1.8336\n",
      "Iteration: 216 of 241\ttrain_loss: 1.3603\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0269\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6170\n",
      "Iteration: 222 of 241\ttrain_loss: 1.9057\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5385\n",
      "Iteration: 226 of 241\ttrain_loss: 1.0499\n",
      "Iteration: 228 of 241\ttrain_loss: 2.1407\n",
      "Iteration: 230 of 241\ttrain_loss: 2.1388\n",
      "Iteration: 232 of 241\ttrain_loss: 1.6190\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7533\n",
      "Iteration: 236 of 241\ttrain_loss: 1.2326\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7365\n",
      "Iteration: 240 of 241\ttrain_loss: 1.7911\n",
      "Iteration: 241 of 241\ttrain_loss: 1.5277\n",
      "Average Score for this Epoch: 1.6290794610977173\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 91 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.7015\n",
      "Iteration: 2 of 241\ttrain_loss: 1.3387\n",
      "Iteration: 4 of 241\ttrain_loss: 1.3620\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6997\n",
      "Iteration: 8 of 241\ttrain_loss: 2.1385\n",
      "Iteration: 10 of 241\ttrain_loss: 1.4885\n",
      "Iteration: 12 of 241\ttrain_loss: 1.5030\n",
      "Iteration: 14 of 241\ttrain_loss: 1.6489\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4850\n",
      "Iteration: 18 of 241\ttrain_loss: 1.2273\n",
      "Iteration: 20 of 241\ttrain_loss: 1.6816\n",
      "Iteration: 22 of 241\ttrain_loss: 1.4371\n",
      "Iteration: 24 of 241\ttrain_loss: 1.8844\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6928\n",
      "Iteration: 28 of 241\ttrain_loss: 1.5257\n",
      "Iteration: 30 of 241\ttrain_loss: 1.7146\n",
      "Iteration: 32 of 241\ttrain_loss: 1.4173\n",
      "Iteration: 34 of 241\ttrain_loss: 1.6450\n",
      "Iteration: 36 of 241\ttrain_loss: 1.4582\n",
      "Iteration: 38 of 241\ttrain_loss: 1.2317\n",
      "Iteration: 40 of 241\ttrain_loss: 1.6283\n",
      "Iteration: 42 of 241\ttrain_loss: 1.5677\n",
      "Iteration: 44 of 241\ttrain_loss: 1.3519\n",
      "Iteration: 46 of 241\ttrain_loss: 1.6744\n",
      "Iteration: 48 of 241\ttrain_loss: 1.4994\n",
      "Iteration: 50 of 241\ttrain_loss: 1.4693\n",
      "Iteration: 52 of 241\ttrain_loss: 1.2679\n",
      "Iteration: 54 of 241\ttrain_loss: 2.1995\n",
      "Iteration: 56 of 241\ttrain_loss: 1.3726\n",
      "Iteration: 58 of 241\ttrain_loss: 0.9479\n",
      "Iteration: 60 of 241\ttrain_loss: 2.0362\n",
      "Iteration: 62 of 241\ttrain_loss: 1.6440\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8989\n",
      "Iteration: 66 of 241\ttrain_loss: 2.1975\n",
      "Iteration: 68 of 241\ttrain_loss: 1.9288\n",
      "Iteration: 70 of 241\ttrain_loss: 1.7622\n",
      "Iteration: 72 of 241\ttrain_loss: 1.7488\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7347\n",
      "Iteration: 76 of 241\ttrain_loss: 1.2143\n",
      "Iteration: 78 of 241\ttrain_loss: 1.6861\n",
      "Iteration: 80 of 241\ttrain_loss: 1.3528\n",
      "Iteration: 82 of 241\ttrain_loss: 1.7280\n",
      "Iteration: 84 of 241\ttrain_loss: 1.7318\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6353\n",
      "Iteration: 88 of 241\ttrain_loss: 1.6648\n",
      "Iteration: 90 of 241\ttrain_loss: 1.8577\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7057\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6017\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4612\n",
      "Iteration: 98 of 241\ttrain_loss: 1.4121\n",
      "Iteration: 100 of 241\ttrain_loss: 1.5252\n",
      "Iteration: 102 of 241\ttrain_loss: 1.9068\n",
      "Iteration: 104 of 241\ttrain_loss: 1.3880\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8888\n",
      "Iteration: 108 of 241\ttrain_loss: 1.4375\n",
      "Iteration: 110 of 241\ttrain_loss: 1.7533\n",
      "Iteration: 112 of 241\ttrain_loss: 1.3166\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5446\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4772\n",
      "Iteration: 118 of 241\ttrain_loss: 1.2732\n",
      "Iteration: 120 of 241\ttrain_loss: 1.9753\n",
      "Iteration: 122 of 241\ttrain_loss: 1.4238\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7896\n",
      "Iteration: 126 of 241\ttrain_loss: 2.0670\n",
      "Iteration: 128 of 241\ttrain_loss: 1.6179\n",
      "Iteration: 130 of 241\ttrain_loss: 1.4288\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7023\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8475\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8533\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6985\n",
      "Iteration: 140 of 241\ttrain_loss: 1.9315\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5868\n",
      "Iteration: 144 of 241\ttrain_loss: 1.8672\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4325\n",
      "Iteration: 148 of 241\ttrain_loss: 1.3103\n",
      "Iteration: 150 of 241\ttrain_loss: 1.5950\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8246\n",
      "Iteration: 154 of 241\ttrain_loss: 2.1291\n",
      "Iteration: 156 of 241\ttrain_loss: 1.4539\n",
      "Iteration: 158 of 241\ttrain_loss: 1.7322\n",
      "Iteration: 160 of 241\ttrain_loss: 2.0744\n",
      "Iteration: 162 of 241\ttrain_loss: 1.5660\n",
      "Iteration: 164 of 241\ttrain_loss: 1.7102\n",
      "Iteration: 166 of 241\ttrain_loss: 1.8742\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7233\n",
      "Iteration: 170 of 241\ttrain_loss: 1.3827\n",
      "Iteration: 172 of 241\ttrain_loss: 2.3086\n",
      "Iteration: 174 of 241\ttrain_loss: 1.7942\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5802\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6750\n",
      "Iteration: 180 of 241\ttrain_loss: 1.3285\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7426\n",
      "Iteration: 184 of 241\ttrain_loss: 1.3382\n",
      "Iteration: 186 of 241\ttrain_loss: 1.7560\n",
      "Iteration: 188 of 241\ttrain_loss: 1.7705\n",
      "Iteration: 190 of 241\ttrain_loss: 1.8472\n",
      "Iteration: 192 of 241\ttrain_loss: 1.5569\n",
      "Iteration: 194 of 241\ttrain_loss: 1.4534\n",
      "Iteration: 196 of 241\ttrain_loss: 1.5615\n",
      "Iteration: 198 of 241\ttrain_loss: 1.6497\n",
      "Iteration: 200 of 241\ttrain_loss: 1.4997\n",
      "Iteration: 202 of 241\ttrain_loss: 1.4482\n",
      "Iteration: 204 of 241\ttrain_loss: 1.7098\n",
      "Iteration: 206 of 241\ttrain_loss: 1.5283\n",
      "Iteration: 208 of 241\ttrain_loss: 1.5195\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8134\n",
      "Iteration: 212 of 241\ttrain_loss: 1.5403\n",
      "Iteration: 214 of 241\ttrain_loss: 1.8899\n",
      "Iteration: 216 of 241\ttrain_loss: 1.4088\n",
      "Iteration: 218 of 241\ttrain_loss: 1.6957\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6349\n",
      "Iteration: 222 of 241\ttrain_loss: 1.5741\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5652\n",
      "Iteration: 226 of 241\ttrain_loss: 1.9785\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8274\n",
      "Iteration: 230 of 241\ttrain_loss: 1.7560\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7015\n",
      "Iteration: 234 of 241\ttrain_loss: 1.5216\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9842\n",
      "Iteration: 238 of 241\ttrain_loss: 1.5794\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6908\n",
      "Iteration: 241 of 241\ttrain_loss: 1.7465\n",
      "Average Score for this Epoch: 1.631524920463562\n",
      "-------------------- Epoch 92 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.4124\n",
      "Iteration: 2 of 241\ttrain_loss: 1.2474\n",
      "Iteration: 4 of 241\ttrain_loss: 1.8088\n",
      "Iteration: 6 of 241\ttrain_loss: 1.9168\n",
      "Iteration: 8 of 241\ttrain_loss: 1.3373\n",
      "Iteration: 10 of 241\ttrain_loss: 1.8585\n",
      "Iteration: 12 of 241\ttrain_loss: 1.8282\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7552\n",
      "Iteration: 16 of 241\ttrain_loss: 1.2893\n",
      "Iteration: 18 of 241\ttrain_loss: 1.2878\n",
      "Iteration: 20 of 241\ttrain_loss: 1.4334\n",
      "Iteration: 22 of 241\ttrain_loss: 1.3868\n",
      "Iteration: 24 of 241\ttrain_loss: 1.4115\n",
      "Iteration: 26 of 241\ttrain_loss: 1.3888\n",
      "Iteration: 28 of 241\ttrain_loss: 1.5852\n",
      "Iteration: 30 of 241\ttrain_loss: 1.5785\n",
      "Iteration: 32 of 241\ttrain_loss: 1.4548\n",
      "Iteration: 34 of 241\ttrain_loss: 1.3374\n",
      "Iteration: 36 of 241\ttrain_loss: 2.5822\n",
      "Iteration: 38 of 241\ttrain_loss: 1.7961\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9173\n",
      "Iteration: 42 of 241\ttrain_loss: 1.8077\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5350\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8605\n",
      "Iteration: 48 of 241\ttrain_loss: 1.5370\n",
      "Iteration: 50 of 241\ttrain_loss: 1.7058\n",
      "Iteration: 52 of 241\ttrain_loss: 1.5384\n",
      "Iteration: 54 of 241\ttrain_loss: 1.5350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 56 of 241\ttrain_loss: 1.4660\n",
      "Iteration: 58 of 241\ttrain_loss: 1.6824\n",
      "Iteration: 60 of 241\ttrain_loss: 1.8195\n",
      "Iteration: 62 of 241\ttrain_loss: 1.3731\n",
      "Iteration: 64 of 241\ttrain_loss: 1.6099\n",
      "Iteration: 66 of 241\ttrain_loss: 1.6419\n",
      "Iteration: 68 of 241\ttrain_loss: 1.8498\n",
      "Iteration: 70 of 241\ttrain_loss: 1.8337\n",
      "Iteration: 72 of 241\ttrain_loss: 2.1311\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7211\n",
      "Iteration: 76 of 241\ttrain_loss: 1.5642\n",
      "Iteration: 78 of 241\ttrain_loss: 1.3829\n",
      "Iteration: 80 of 241\ttrain_loss: 1.8417\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8942\n",
      "Iteration: 84 of 241\ttrain_loss: 1.7428\n",
      "Iteration: 86 of 241\ttrain_loss: 1.9573\n",
      "Iteration: 88 of 241\ttrain_loss: 1.6209\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0421\n",
      "Iteration: 92 of 241\ttrain_loss: 1.6607\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6012\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5189\n",
      "Iteration: 98 of 241\ttrain_loss: 1.5192\n",
      "Iteration: 100 of 241\ttrain_loss: 1.4481\n",
      "Iteration: 102 of 241\ttrain_loss: 1.2227\n",
      "Iteration: 104 of 241\ttrain_loss: 1.3181\n",
      "Iteration: 106 of 241\ttrain_loss: 1.5601\n",
      "Iteration: 108 of 241\ttrain_loss: 1.3189\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8193\n",
      "Iteration: 112 of 241\ttrain_loss: 1.4568\n",
      "Iteration: 114 of 241\ttrain_loss: 1.8820\n",
      "Iteration: 116 of 241\ttrain_loss: 1.7036\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7486\n",
      "Iteration: 120 of 241\ttrain_loss: 1.3471\n",
      "Iteration: 122 of 241\ttrain_loss: 1.9573\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5391\n",
      "Iteration: 126 of 241\ttrain_loss: 1.9929\n",
      "Iteration: 128 of 241\ttrain_loss: 2.0880\n",
      "Iteration: 130 of 241\ttrain_loss: 1.4069\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6384\n",
      "Iteration: 134 of 241\ttrain_loss: 2.0016\n",
      "Iteration: 136 of 241\ttrain_loss: 1.4134\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6586\n",
      "Iteration: 140 of 241\ttrain_loss: 1.4788\n",
      "Iteration: 142 of 241\ttrain_loss: 1.9032\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7100\n",
      "Iteration: 146 of 241\ttrain_loss: 1.2319\n",
      "Iteration: 148 of 241\ttrain_loss: 2.0718\n",
      "Iteration: 150 of 241\ttrain_loss: 2.0744\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6150\n",
      "Iteration: 154 of 241\ttrain_loss: 1.7382\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7406\n",
      "Iteration: 158 of 241\ttrain_loss: 1.7150\n",
      "Iteration: 160 of 241\ttrain_loss: 1.4878\n",
      "Iteration: 162 of 241\ttrain_loss: 1.1732\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6628\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4923\n",
      "Iteration: 168 of 241\ttrain_loss: 2.1909\n",
      "Iteration: 170 of 241\ttrain_loss: 2.6279\n",
      "Iteration: 172 of 241\ttrain_loss: 1.3264\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5759\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6589\n",
      "Iteration: 178 of 241\ttrain_loss: 1.2397\n",
      "Iteration: 180 of 241\ttrain_loss: 1.8757\n",
      "Iteration: 182 of 241\ttrain_loss: 1.7730\n",
      "Iteration: 184 of 241\ttrain_loss: 1.3585\n",
      "Iteration: 186 of 241\ttrain_loss: 1.5481\n",
      "Iteration: 188 of 241\ttrain_loss: 1.6397\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7625\n",
      "Iteration: 192 of 241\ttrain_loss: 2.0372\n",
      "Iteration: 194 of 241\ttrain_loss: 1.4020\n",
      "Iteration: 196 of 241\ttrain_loss: 1.4747\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1356\n",
      "Iteration: 200 of 241\ttrain_loss: 2.2299\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8967\n",
      "Iteration: 204 of 241\ttrain_loss: 1.3198\n",
      "Iteration: 206 of 241\ttrain_loss: 1.3694\n",
      "Iteration: 208 of 241\ttrain_loss: 1.5902\n",
      "Iteration: 210 of 241\ttrain_loss: 1.7649\n",
      "Iteration: 212 of 241\ttrain_loss: 1.4791\n",
      "Iteration: 214 of 241\ttrain_loss: 1.7493\n",
      "Iteration: 216 of 241\ttrain_loss: 1.6305\n",
      "Iteration: 218 of 241\ttrain_loss: 1.6108\n",
      "Iteration: 220 of 241\ttrain_loss: 1.9072\n",
      "Iteration: 222 of 241\ttrain_loss: 1.6297\n",
      "Iteration: 224 of 241\ttrain_loss: 1.8013\n",
      "Iteration: 226 of 241\ttrain_loss: 1.6784\n",
      "Iteration: 228 of 241\ttrain_loss: 1.6444\n",
      "Iteration: 230 of 241\ttrain_loss: 1.4518\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7029\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7834\n",
      "Iteration: 236 of 241\ttrain_loss: 1.4884\n",
      "Iteration: 238 of 241\ttrain_loss: 1.4855\n",
      "Iteration: 240 of 241\ttrain_loss: 1.5124\n",
      "Iteration: 241 of 241\ttrain_loss: 1.4212\n",
      "Average Score for this Epoch: 1.617617130279541\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 93 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.9661\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7560\n",
      "Iteration: 4 of 241\ttrain_loss: 2.3562\n",
      "Iteration: 6 of 241\ttrain_loss: 1.4725\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6336\n",
      "Iteration: 10 of 241\ttrain_loss: 1.4084\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7386\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7631\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5030\n",
      "Iteration: 18 of 241\ttrain_loss: 1.7643\n",
      "Iteration: 20 of 241\ttrain_loss: 1.3081\n",
      "Iteration: 22 of 241\ttrain_loss: 1.7691\n",
      "Iteration: 24 of 241\ttrain_loss: 2.0936\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6518\n",
      "Iteration: 28 of 241\ttrain_loss: 1.3262\n",
      "Iteration: 30 of 241\ttrain_loss: 1.5669\n",
      "Iteration: 32 of 241\ttrain_loss: 1.8414\n",
      "Iteration: 34 of 241\ttrain_loss: 1.1927\n",
      "Iteration: 36 of 241\ttrain_loss: 2.2657\n",
      "Iteration: 38 of 241\ttrain_loss: 1.9475\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5906\n",
      "Iteration: 42 of 241\ttrain_loss: 1.6294\n",
      "Iteration: 44 of 241\ttrain_loss: 1.2451\n",
      "Iteration: 46 of 241\ttrain_loss: 1.4492\n",
      "Iteration: 48 of 241\ttrain_loss: 1.7061\n",
      "Iteration: 50 of 241\ttrain_loss: 1.3778\n",
      "Iteration: 52 of 241\ttrain_loss: 1.8032\n",
      "Iteration: 54 of 241\ttrain_loss: 1.5816\n",
      "Iteration: 56 of 241\ttrain_loss: 1.5704\n",
      "Iteration: 58 of 241\ttrain_loss: 1.4115\n",
      "Iteration: 60 of 241\ttrain_loss: 1.3817\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7356\n",
      "Iteration: 64 of 241\ttrain_loss: 1.2511\n",
      "Iteration: 66 of 241\ttrain_loss: 1.3297\n",
      "Iteration: 68 of 241\ttrain_loss: 1.6801\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6854\n",
      "Iteration: 72 of 241\ttrain_loss: 1.3742\n",
      "Iteration: 74 of 241\ttrain_loss: 1.8805\n",
      "Iteration: 76 of 241\ttrain_loss: 1.4657\n",
      "Iteration: 78 of 241\ttrain_loss: 1.9328\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6250\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5613\n",
      "Iteration: 84 of 241\ttrain_loss: 1.1818\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6443\n",
      "Iteration: 88 of 241\ttrain_loss: 1.4791\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7429\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7043\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9271\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4537\n",
      "Iteration: 98 of 241\ttrain_loss: 1.5720\n",
      "Iteration: 100 of 241\ttrain_loss: 1.4741\n",
      "Iteration: 102 of 241\ttrain_loss: 2.1829\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4674\n",
      "Iteration: 106 of 241\ttrain_loss: 1.5799\n",
      "Iteration: 108 of 241\ttrain_loss: 1.5803\n",
      "Iteration: 110 of 241\ttrain_loss: 1.8163\n",
      "Iteration: 112 of 241\ttrain_loss: 1.6387\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5844\n",
      "Iteration: 116 of 241\ttrain_loss: 1.2338\n",
      "Iteration: 118 of 241\ttrain_loss: 1.3684\n",
      "Iteration: 120 of 241\ttrain_loss: 1.4580\n",
      "Iteration: 122 of 241\ttrain_loss: 1.3751\n",
      "Iteration: 124 of 241\ttrain_loss: 1.4016\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6399\n",
      "Iteration: 128 of 241\ttrain_loss: 1.5139\n",
      "Iteration: 130 of 241\ttrain_loss: 1.2963\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6359\n",
      "Iteration: 134 of 241\ttrain_loss: 1.5591\n",
      "Iteration: 136 of 241\ttrain_loss: 1.4571\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7204\n",
      "Iteration: 140 of 241\ttrain_loss: 2.1711\n",
      "Iteration: 142 of 241\ttrain_loss: 1.4061\n",
      "Iteration: 144 of 241\ttrain_loss: 1.5724\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4590\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7630\n",
      "Iteration: 150 of 241\ttrain_loss: 1.8036\n",
      "Iteration: 152 of 241\ttrain_loss: 1.7260\n",
      "Iteration: 154 of 241\ttrain_loss: 1.4674\n",
      "Iteration: 156 of 241\ttrain_loss: 1.6230\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5954\n",
      "Iteration: 160 of 241\ttrain_loss: 1.4720\n",
      "Iteration: 162 of 241\ttrain_loss: 1.5292\n",
      "Iteration: 164 of 241\ttrain_loss: 1.9778\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4532\n",
      "Iteration: 168 of 241\ttrain_loss: 1.9002\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4681\n",
      "Iteration: 172 of 241\ttrain_loss: 1.6516\n",
      "Iteration: 174 of 241\ttrain_loss: 1.4838\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6417\n",
      "Iteration: 178 of 241\ttrain_loss: 1.8469\n",
      "Iteration: 180 of 241\ttrain_loss: 1.1934\n",
      "Iteration: 182 of 241\ttrain_loss: 1.2663\n",
      "Iteration: 184 of 241\ttrain_loss: 1.9158\n",
      "Iteration: 186 of 241\ttrain_loss: 1.5418\n",
      "Iteration: 188 of 241\ttrain_loss: 1.3563\n",
      "Iteration: 190 of 241\ttrain_loss: 1.7320\n",
      "Iteration: 192 of 241\ttrain_loss: 1.8853\n",
      "Iteration: 194 of 241\ttrain_loss: 1.9043\n",
      "Iteration: 196 of 241\ttrain_loss: 1.4649\n",
      "Iteration: 198 of 241\ttrain_loss: 2.0940\n",
      "Iteration: 200 of 241\ttrain_loss: 1.9047\n",
      "Iteration: 202 of 241\ttrain_loss: 1.3333\n",
      "Iteration: 204 of 241\ttrain_loss: 1.5968\n",
      "Iteration: 206 of 241\ttrain_loss: 2.1894\n",
      "Iteration: 208 of 241\ttrain_loss: 1.2487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 210 of 241\ttrain_loss: 2.1898\n",
      "Iteration: 212 of 241\ttrain_loss: 2.0653\n",
      "Iteration: 214 of 241\ttrain_loss: 1.4741\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8724\n",
      "Iteration: 218 of 241\ttrain_loss: 1.5383\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6384\n",
      "Iteration: 222 of 241\ttrain_loss: 1.7493\n",
      "Iteration: 224 of 241\ttrain_loss: 1.4482\n",
      "Iteration: 226 of 241\ttrain_loss: 1.6601\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9351\n",
      "Iteration: 230 of 241\ttrain_loss: 1.5334\n",
      "Iteration: 232 of 241\ttrain_loss: 1.5704\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7886\n",
      "Iteration: 236 of 241\ttrain_loss: 2.0578\n",
      "Iteration: 238 of 241\ttrain_loss: 1.8475\n",
      "Iteration: 240 of 241\ttrain_loss: 1.1161\n",
      "Iteration: 241 of 241\ttrain_loss: 1.4374\n",
      "Average Score for this Epoch: 1.6245315074920654\n",
      "-------------------- Epoch 94 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.8476\n",
      "Iteration: 2 of 241\ttrain_loss: 1.4775\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4434\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6254\n",
      "Iteration: 8 of 241\ttrain_loss: 1.6704\n",
      "Iteration: 10 of 241\ttrain_loss: 1.2102\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6896\n",
      "Iteration: 14 of 241\ttrain_loss: 1.8682\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5704\n",
      "Iteration: 18 of 241\ttrain_loss: 1.4771\n",
      "Iteration: 20 of 241\ttrain_loss: 1.7066\n",
      "Iteration: 22 of 241\ttrain_loss: 1.7454\n",
      "Iteration: 24 of 241\ttrain_loss: 1.2418\n",
      "Iteration: 26 of 241\ttrain_loss: 1.2930\n",
      "Iteration: 28 of 241\ttrain_loss: 1.3282\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4000\n",
      "Iteration: 32 of 241\ttrain_loss: 1.6802\n",
      "Iteration: 34 of 241\ttrain_loss: 1.5964\n",
      "Iteration: 36 of 241\ttrain_loss: 1.3603\n",
      "Iteration: 38 of 241\ttrain_loss: 1.4065\n",
      "Iteration: 40 of 241\ttrain_loss: 1.8060\n",
      "Iteration: 42 of 241\ttrain_loss: 1.7960\n",
      "Iteration: 44 of 241\ttrain_loss: 1.3096\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8895\n",
      "Iteration: 48 of 241\ttrain_loss: 1.3174\n",
      "Iteration: 50 of 241\ttrain_loss: 1.6642\n",
      "Iteration: 52 of 241\ttrain_loss: 1.8850\n",
      "Iteration: 54 of 241\ttrain_loss: 1.1314\n",
      "Iteration: 56 of 241\ttrain_loss: 1.6902\n",
      "Iteration: 58 of 241\ttrain_loss: 1.4437\n",
      "Iteration: 60 of 241\ttrain_loss: 1.4868\n",
      "Iteration: 62 of 241\ttrain_loss: 1.8291\n",
      "Iteration: 64 of 241\ttrain_loss: 1.7040\n",
      "Iteration: 66 of 241\ttrain_loss: 1.5007\n",
      "Iteration: 68 of 241\ttrain_loss: 1.4955\n",
      "Iteration: 70 of 241\ttrain_loss: 1.2961\n",
      "Iteration: 72 of 241\ttrain_loss: 1.2923\n",
      "Iteration: 74 of 241\ttrain_loss: 1.6260\n",
      "Iteration: 76 of 241\ttrain_loss: 1.3247\n",
      "Iteration: 78 of 241\ttrain_loss: 1.1125\n",
      "Iteration: 80 of 241\ttrain_loss: 1.4667\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5555\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3811\n",
      "Iteration: 86 of 241\ttrain_loss: 1.3778\n",
      "Iteration: 88 of 241\ttrain_loss: 2.0608\n",
      "Iteration: 90 of 241\ttrain_loss: 1.6171\n",
      "Iteration: 92 of 241\ttrain_loss: 1.3050\n",
      "Iteration: 94 of 241\ttrain_loss: 1.3418\n",
      "Iteration: 96 of 241\ttrain_loss: 1.9563\n",
      "Iteration: 98 of 241\ttrain_loss: 1.1167\n",
      "Iteration: 100 of 241\ttrain_loss: 1.3064\n",
      "Iteration: 102 of 241\ttrain_loss: 1.6002\n",
      "Iteration: 104 of 241\ttrain_loss: 1.5661\n",
      "Iteration: 106 of 241\ttrain_loss: 1.4108\n",
      "Iteration: 108 of 241\ttrain_loss: 1.2577\n",
      "Iteration: 110 of 241\ttrain_loss: 1.2973\n",
      "Iteration: 112 of 241\ttrain_loss: 1.3805\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5575\n",
      "Iteration: 116 of 241\ttrain_loss: 1.7273\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6808\n",
      "Iteration: 120 of 241\ttrain_loss: 1.3724\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5233\n",
      "Iteration: 124 of 241\ttrain_loss: 1.5493\n",
      "Iteration: 126 of 241\ttrain_loss: 1.7614\n",
      "Iteration: 128 of 241\ttrain_loss: 1.5475\n",
      "Iteration: 130 of 241\ttrain_loss: 1.7693\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7223\n",
      "Iteration: 134 of 241\ttrain_loss: 1.6978\n",
      "Iteration: 136 of 241\ttrain_loss: 1.5201\n",
      "Iteration: 138 of 241\ttrain_loss: 1.4423\n",
      "Iteration: 140 of 241\ttrain_loss: 1.6237\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6893\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6273\n",
      "Iteration: 146 of 241\ttrain_loss: 1.2606\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7621\n",
      "Iteration: 150 of 241\ttrain_loss: 1.6815\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6569\n",
      "Iteration: 154 of 241\ttrain_loss: 1.4924\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9194\n",
      "Iteration: 158 of 241\ttrain_loss: 1.3195\n",
      "Iteration: 160 of 241\ttrain_loss: 1.9948\n",
      "Iteration: 162 of 241\ttrain_loss: 1.7863\n",
      "Iteration: 164 of 241\ttrain_loss: 1.7608\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4924\n",
      "Iteration: 168 of 241\ttrain_loss: 1.3758\n",
      "Iteration: 170 of 241\ttrain_loss: 1.6646\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7674\n",
      "Iteration: 174 of 241\ttrain_loss: 1.3809\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5408\n",
      "Iteration: 178 of 241\ttrain_loss: 1.9574\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6839\n",
      "Iteration: 182 of 241\ttrain_loss: 1.4724\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7094\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4187\n",
      "Iteration: 188 of 241\ttrain_loss: 1.8735\n",
      "Iteration: 190 of 241\ttrain_loss: 1.6751\n",
      "Iteration: 192 of 241\ttrain_loss: 1.4564\n",
      "Iteration: 194 of 241\ttrain_loss: 1.4941\n",
      "Iteration: 196 of 241\ttrain_loss: 1.4518\n",
      "Iteration: 198 of 241\ttrain_loss: 1.6752\n",
      "Iteration: 200 of 241\ttrain_loss: 1.4187\n",
      "Iteration: 202 of 241\ttrain_loss: 1.3199\n",
      "Iteration: 204 of 241\ttrain_loss: 1.5743\n",
      "Iteration: 206 of 241\ttrain_loss: 2.2758\n",
      "Iteration: 208 of 241\ttrain_loss: 1.2193\n",
      "Iteration: 210 of 241\ttrain_loss: 1.5915\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6982\n",
      "Iteration: 214 of 241\ttrain_loss: 1.8171\n",
      "Iteration: 216 of 241\ttrain_loss: 1.2903\n",
      "Iteration: 218 of 241\ttrain_loss: 1.7643\n",
      "Iteration: 220 of 241\ttrain_loss: 1.3635\n",
      "Iteration: 222 of 241\ttrain_loss: 1.4197\n",
      "Iteration: 224 of 241\ttrain_loss: 1.2518\n",
      "Iteration: 226 of 241\ttrain_loss: 1.4427\n",
      "Iteration: 228 of 241\ttrain_loss: 1.7889\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6745\n",
      "Iteration: 232 of 241\ttrain_loss: 1.6170\n",
      "Iteration: 234 of 241\ttrain_loss: 1.4998\n",
      "Iteration: 236 of 241\ttrain_loss: 1.5335\n",
      "Iteration: 238 of 241\ttrain_loss: 1.8631\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6507\n",
      "Iteration: 241 of 241\ttrain_loss: 1.6930\n",
      "Average Score for this Epoch: 1.5708894729614258\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 95 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.2524\n",
      "Iteration: 2 of 241\ttrain_loss: 1.7501\n",
      "Iteration: 4 of 241\ttrain_loss: 1.3379\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6254\n",
      "Iteration: 8 of 241\ttrain_loss: 1.5762\n",
      "Iteration: 10 of 241\ttrain_loss: 1.3326\n",
      "Iteration: 12 of 241\ttrain_loss: 1.0397\n",
      "Iteration: 14 of 241\ttrain_loss: 1.2027\n",
      "Iteration: 16 of 241\ttrain_loss: 1.5672\n",
      "Iteration: 18 of 241\ttrain_loss: 1.4643\n",
      "Iteration: 20 of 241\ttrain_loss: 1.6895\n",
      "Iteration: 22 of 241\ttrain_loss: 1.3773\n",
      "Iteration: 24 of 241\ttrain_loss: 1.3675\n",
      "Iteration: 26 of 241\ttrain_loss: 1.5121\n",
      "Iteration: 28 of 241\ttrain_loss: 1.2207\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4924\n",
      "Iteration: 32 of 241\ttrain_loss: 1.5803\n",
      "Iteration: 34 of 241\ttrain_loss: 1.3192\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6468\n",
      "Iteration: 38 of 241\ttrain_loss: 1.4869\n",
      "Iteration: 40 of 241\ttrain_loss: 1.5905\n",
      "Iteration: 42 of 241\ttrain_loss: 1.4100\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5887\n",
      "Iteration: 46 of 241\ttrain_loss: 1.6182\n",
      "Iteration: 48 of 241\ttrain_loss: 1.8403\n",
      "Iteration: 50 of 241\ttrain_loss: 1.3845\n",
      "Iteration: 52 of 241\ttrain_loss: 1.3091\n",
      "Iteration: 54 of 241\ttrain_loss: 1.9407\n",
      "Iteration: 56 of 241\ttrain_loss: 1.4011\n",
      "Iteration: 58 of 241\ttrain_loss: 1.5317\n",
      "Iteration: 60 of 241\ttrain_loss: 1.3706\n",
      "Iteration: 62 of 241\ttrain_loss: 1.3649\n",
      "Iteration: 64 of 241\ttrain_loss: 1.3301\n",
      "Iteration: 66 of 241\ttrain_loss: 1.5013\n",
      "Iteration: 68 of 241\ttrain_loss: 1.4348\n",
      "Iteration: 70 of 241\ttrain_loss: 1.6139\n",
      "Iteration: 72 of 241\ttrain_loss: 1.2679\n",
      "Iteration: 74 of 241\ttrain_loss: 1.5985\n",
      "Iteration: 76 of 241\ttrain_loss: 1.6200\n",
      "Iteration: 78 of 241\ttrain_loss: 1.2100\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6469\n",
      "Iteration: 82 of 241\ttrain_loss: 1.2074\n",
      "Iteration: 84 of 241\ttrain_loss: 1.4556\n",
      "Iteration: 86 of 241\ttrain_loss: 1.6556\n",
      "Iteration: 88 of 241\ttrain_loss: 1.3933\n",
      "Iteration: 90 of 241\ttrain_loss: 2.0598\n",
      "Iteration: 92 of 241\ttrain_loss: 1.2788\n",
      "Iteration: 94 of 241\ttrain_loss: 1.9748\n",
      "Iteration: 96 of 241\ttrain_loss: 1.6424\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9175\n",
      "Iteration: 100 of 241\ttrain_loss: 1.5110\n",
      "Iteration: 102 of 241\ttrain_loss: 2.0494\n",
      "Iteration: 104 of 241\ttrain_loss: 1.4879\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8352\n",
      "Iteration: 108 of 241\ttrain_loss: 1.3592\n",
      "Iteration: 110 of 241\ttrain_loss: 1.6037\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5169\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 116 of 241\ttrain_loss: 1.4277\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6493\n",
      "Iteration: 120 of 241\ttrain_loss: 1.5501\n",
      "Iteration: 122 of 241\ttrain_loss: 1.4860\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7398\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8187\n",
      "Iteration: 128 of 241\ttrain_loss: 1.3584\n",
      "Iteration: 130 of 241\ttrain_loss: 1.6270\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6613\n",
      "Iteration: 134 of 241\ttrain_loss: 1.4232\n",
      "Iteration: 136 of 241\ttrain_loss: 1.9151\n",
      "Iteration: 138 of 241\ttrain_loss: 1.8390\n",
      "Iteration: 140 of 241\ttrain_loss: 1.6884\n",
      "Iteration: 142 of 241\ttrain_loss: 1.7753\n",
      "Iteration: 144 of 241\ttrain_loss: 1.3854\n",
      "Iteration: 146 of 241\ttrain_loss: 1.6949\n",
      "Iteration: 148 of 241\ttrain_loss: 2.0316\n",
      "Iteration: 150 of 241\ttrain_loss: 1.4137\n",
      "Iteration: 152 of 241\ttrain_loss: 1.2846\n",
      "Iteration: 154 of 241\ttrain_loss: 1.8753\n",
      "Iteration: 156 of 241\ttrain_loss: 1.7607\n",
      "Iteration: 158 of 241\ttrain_loss: 1.4040\n",
      "Iteration: 160 of 241\ttrain_loss: 1.5431\n",
      "Iteration: 162 of 241\ttrain_loss: 1.6293\n",
      "Iteration: 164 of 241\ttrain_loss: 1.4605\n",
      "Iteration: 166 of 241\ttrain_loss: 2.1722\n",
      "Iteration: 168 of 241\ttrain_loss: 1.6301\n",
      "Iteration: 170 of 241\ttrain_loss: 1.8973\n",
      "Iteration: 172 of 241\ttrain_loss: 1.6583\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5415\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6949\n",
      "Iteration: 178 of 241\ttrain_loss: 2.1282\n",
      "Iteration: 180 of 241\ttrain_loss: 1.3542\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6918\n",
      "Iteration: 184 of 241\ttrain_loss: 2.3318\n",
      "Iteration: 186 of 241\ttrain_loss: 1.2934\n",
      "Iteration: 188 of 241\ttrain_loss: 1.5359\n",
      "Iteration: 190 of 241\ttrain_loss: 1.4028\n",
      "Iteration: 192 of 241\ttrain_loss: 1.4864\n",
      "Iteration: 194 of 241\ttrain_loss: 1.4324\n",
      "Iteration: 196 of 241\ttrain_loss: 1.5303\n",
      "Iteration: 198 of 241\ttrain_loss: 1.7138\n",
      "Iteration: 200 of 241\ttrain_loss: 1.8479\n",
      "Iteration: 202 of 241\ttrain_loss: 1.5097\n",
      "Iteration: 204 of 241\ttrain_loss: 2.0968\n",
      "Iteration: 206 of 241\ttrain_loss: 1.7133\n",
      "Iteration: 208 of 241\ttrain_loss: 1.2221\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8569\n",
      "Iteration: 212 of 241\ttrain_loss: 1.9108\n",
      "Iteration: 214 of 241\ttrain_loss: 1.5086\n",
      "Iteration: 216 of 241\ttrain_loss: 1.9107\n",
      "Iteration: 218 of 241\ttrain_loss: 1.3179\n",
      "Iteration: 220 of 241\ttrain_loss: 1.4734\n",
      "Iteration: 222 of 241\ttrain_loss: 1.5924\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5634\n",
      "Iteration: 226 of 241\ttrain_loss: 1.2399\n",
      "Iteration: 228 of 241\ttrain_loss: 1.8270\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6535\n",
      "Iteration: 232 of 241\ttrain_loss: 1.3965\n",
      "Iteration: 234 of 241\ttrain_loss: 1.9353\n",
      "Iteration: 236 of 241\ttrain_loss: 1.8389\n",
      "Iteration: 238 of 241\ttrain_loss: 1.6795\n",
      "Iteration: 240 of 241\ttrain_loss: 1.7993\n",
      "Iteration: 241 of 241\ttrain_loss: 1.4439\n",
      "Average Score for this Epoch: 1.5721112489700317\n",
      "-------------------- Epoch 96 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.4042\n",
      "Iteration: 2 of 241\ttrain_loss: 1.3028\n",
      "Iteration: 4 of 241\ttrain_loss: 1.2032\n",
      "Iteration: 6 of 241\ttrain_loss: 1.3670\n",
      "Iteration: 8 of 241\ttrain_loss: 1.4937\n",
      "Iteration: 10 of 241\ttrain_loss: 1.4797\n",
      "Iteration: 12 of 241\ttrain_loss: 1.9589\n",
      "Iteration: 14 of 241\ttrain_loss: 1.7125\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7673\n",
      "Iteration: 18 of 241\ttrain_loss: 1.4321\n",
      "Iteration: 20 of 241\ttrain_loss: 1.4584\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5675\n",
      "Iteration: 24 of 241\ttrain_loss: 1.3279\n",
      "Iteration: 26 of 241\ttrain_loss: 1.4735\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7734\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4829\n",
      "Iteration: 32 of 241\ttrain_loss: 1.3610\n",
      "Iteration: 34 of 241\ttrain_loss: 1.3108\n",
      "Iteration: 36 of 241\ttrain_loss: 1.2575\n",
      "Iteration: 38 of 241\ttrain_loss: 1.5194\n",
      "Iteration: 40 of 241\ttrain_loss: 1.9388\n",
      "Iteration: 42 of 241\ttrain_loss: 1.7260\n",
      "Iteration: 44 of 241\ttrain_loss: 1.0329\n",
      "Iteration: 46 of 241\ttrain_loss: 1.6660\n",
      "Iteration: 48 of 241\ttrain_loss: 1.4724\n",
      "Iteration: 50 of 241\ttrain_loss: 1.9517\n",
      "Iteration: 52 of 241\ttrain_loss: 1.6570\n",
      "Iteration: 54 of 241\ttrain_loss: 1.4971\n",
      "Iteration: 56 of 241\ttrain_loss: 1.2005\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7190\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7088\n",
      "Iteration: 62 of 241\ttrain_loss: 1.7839\n",
      "Iteration: 64 of 241\ttrain_loss: 1.8210\n",
      "Iteration: 66 of 241\ttrain_loss: 1.2132\n",
      "Iteration: 68 of 241\ttrain_loss: 1.2810\n",
      "Iteration: 70 of 241\ttrain_loss: 2.0910\n",
      "Iteration: 72 of 241\ttrain_loss: 1.7729\n",
      "Iteration: 74 of 241\ttrain_loss: 1.3846\n",
      "Iteration: 76 of 241\ttrain_loss: 1.8016\n",
      "Iteration: 78 of 241\ttrain_loss: 1.1790\n",
      "Iteration: 80 of 241\ttrain_loss: 1.5013\n",
      "Iteration: 82 of 241\ttrain_loss: 1.8852\n",
      "Iteration: 84 of 241\ttrain_loss: 1.4588\n",
      "Iteration: 86 of 241\ttrain_loss: 1.7463\n",
      "Iteration: 88 of 241\ttrain_loss: 1.7047\n",
      "Iteration: 90 of 241\ttrain_loss: 1.2822\n",
      "Iteration: 92 of 241\ttrain_loss: 1.2374\n",
      "Iteration: 94 of 241\ttrain_loss: 1.5393\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4753\n",
      "Iteration: 98 of 241\ttrain_loss: 1.3957\n",
      "Iteration: 100 of 241\ttrain_loss: 2.0799\n",
      "Iteration: 102 of 241\ttrain_loss: 1.1908\n",
      "Iteration: 104 of 241\ttrain_loss: 1.8494\n",
      "Iteration: 106 of 241\ttrain_loss: 1.4386\n",
      "Iteration: 108 of 241\ttrain_loss: 1.4838\n",
      "Iteration: 110 of 241\ttrain_loss: 1.0395\n",
      "Iteration: 112 of 241\ttrain_loss: 1.5354\n",
      "Iteration: 114 of 241\ttrain_loss: 1.5318\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4892\n",
      "Iteration: 118 of 241\ttrain_loss: 1.5546\n",
      "Iteration: 120 of 241\ttrain_loss: 1.7462\n",
      "Iteration: 122 of 241\ttrain_loss: 1.1409\n",
      "Iteration: 124 of 241\ttrain_loss: 1.8313\n",
      "Iteration: 126 of 241\ttrain_loss: 1.7250\n",
      "Iteration: 128 of 241\ttrain_loss: 1.2837\n",
      "Iteration: 130 of 241\ttrain_loss: 1.5789\n",
      "Iteration: 132 of 241\ttrain_loss: 1.4023\n",
      "Iteration: 134 of 241\ttrain_loss: 1.8787\n",
      "Iteration: 136 of 241\ttrain_loss: 1.4047\n",
      "Iteration: 138 of 241\ttrain_loss: 1.7260\n",
      "Iteration: 140 of 241\ttrain_loss: 1.4327\n",
      "Iteration: 142 of 241\ttrain_loss: 1.2955\n",
      "Iteration: 144 of 241\ttrain_loss: 1.3042\n",
      "Iteration: 146 of 241\ttrain_loss: 1.4476\n",
      "Iteration: 148 of 241\ttrain_loss: 1.5011\n",
      "Iteration: 150 of 241\ttrain_loss: 1.7981\n",
      "Iteration: 152 of 241\ttrain_loss: 1.1269\n",
      "Iteration: 154 of 241\ttrain_loss: 1.4949\n",
      "Iteration: 156 of 241\ttrain_loss: 1.9802\n",
      "Iteration: 158 of 241\ttrain_loss: 1.4673\n",
      "Iteration: 160 of 241\ttrain_loss: 1.7303\n",
      "Iteration: 162 of 241\ttrain_loss: 1.6906\n",
      "Iteration: 164 of 241\ttrain_loss: 1.4550\n",
      "Iteration: 166 of 241\ttrain_loss: 1.7677\n",
      "Iteration: 168 of 241\ttrain_loss: 1.5007\n",
      "Iteration: 170 of 241\ttrain_loss: 1.3863\n",
      "Iteration: 172 of 241\ttrain_loss: 1.5420\n",
      "Iteration: 174 of 241\ttrain_loss: 1.2776\n",
      "Iteration: 176 of 241\ttrain_loss: 1.6680\n",
      "Iteration: 178 of 241\ttrain_loss: 1.1942\n",
      "Iteration: 180 of 241\ttrain_loss: 1.7038\n",
      "Iteration: 182 of 241\ttrain_loss: 1.8371\n",
      "Iteration: 184 of 241\ttrain_loss: 1.5916\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4019\n",
      "Iteration: 188 of 241\ttrain_loss: 1.9556\n",
      "Iteration: 190 of 241\ttrain_loss: 1.4406\n",
      "Iteration: 192 of 241\ttrain_loss: 1.4733\n",
      "Iteration: 194 of 241\ttrain_loss: 1.2060\n",
      "Iteration: 196 of 241\ttrain_loss: 1.8052\n",
      "Iteration: 198 of 241\ttrain_loss: 2.1441\n",
      "Iteration: 200 of 241\ttrain_loss: 1.6389\n",
      "Iteration: 202 of 241\ttrain_loss: 1.3788\n",
      "Iteration: 204 of 241\ttrain_loss: 1.9184\n",
      "Iteration: 206 of 241\ttrain_loss: 1.3245\n",
      "Iteration: 208 of 241\ttrain_loss: 1.3593\n",
      "Iteration: 210 of 241\ttrain_loss: 1.1948\n",
      "Iteration: 212 of 241\ttrain_loss: 1.6975\n",
      "Iteration: 214 of 241\ttrain_loss: 1.4396\n",
      "Iteration: 216 of 241\ttrain_loss: 1.8329\n",
      "Iteration: 218 of 241\ttrain_loss: 2.0280\n",
      "Iteration: 220 of 241\ttrain_loss: 2.1307\n",
      "Iteration: 222 of 241\ttrain_loss: 2.3677\n",
      "Iteration: 224 of 241\ttrain_loss: 1.7271\n",
      "Iteration: 226 of 241\ttrain_loss: 2.2783\n",
      "Iteration: 228 of 241\ttrain_loss: 1.5384\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6188\n",
      "Iteration: 232 of 241\ttrain_loss: 1.3242\n",
      "Iteration: 234 of 241\ttrain_loss: 1.7795\n",
      "Iteration: 236 of 241\ttrain_loss: 1.3425\n",
      "Iteration: 238 of 241\ttrain_loss: 1.3604\n",
      "Iteration: 240 of 241\ttrain_loss: 1.5965\n",
      "Iteration: 241 of 241\ttrain_loss: 1.3103\n",
      "Average Score for this Epoch: 1.5594719648361206\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 97 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6363\n",
      "Iteration: 2 of 241\ttrain_loss: 1.2660\n",
      "Iteration: 4 of 241\ttrain_loss: 1.4769\n",
      "Iteration: 6 of 241\ttrain_loss: 1.3995\n",
      "Iteration: 8 of 241\ttrain_loss: 1.4336\n",
      "Iteration: 10 of 241\ttrain_loss: 1.7392\n",
      "Iteration: 12 of 241\ttrain_loss: 1.5195\n",
      "Iteration: 14 of 241\ttrain_loss: 1.4508\n",
      "Iteration: 16 of 241\ttrain_loss: 1.3210\n",
      "Iteration: 18 of 241\ttrain_loss: 1.5159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 20 of 241\ttrain_loss: 1.2624\n",
      "Iteration: 22 of 241\ttrain_loss: 1.8075\n",
      "Iteration: 24 of 241\ttrain_loss: 1.5397\n",
      "Iteration: 26 of 241\ttrain_loss: 1.8765\n",
      "Iteration: 28 of 241\ttrain_loss: 1.7527\n",
      "Iteration: 30 of 241\ttrain_loss: 1.5802\n",
      "Iteration: 32 of 241\ttrain_loss: 1.3228\n",
      "Iteration: 34 of 241\ttrain_loss: 1.4557\n",
      "Iteration: 36 of 241\ttrain_loss: 1.4489\n",
      "Iteration: 38 of 241\ttrain_loss: 1.5360\n",
      "Iteration: 40 of 241\ttrain_loss: 1.6705\n",
      "Iteration: 42 of 241\ttrain_loss: 1.7159\n",
      "Iteration: 44 of 241\ttrain_loss: 1.6299\n",
      "Iteration: 46 of 241\ttrain_loss: 1.6775\n",
      "Iteration: 48 of 241\ttrain_loss: 1.5081\n",
      "Iteration: 50 of 241\ttrain_loss: 1.4840\n",
      "Iteration: 52 of 241\ttrain_loss: 1.7503\n",
      "Iteration: 54 of 241\ttrain_loss: 1.4476\n",
      "Iteration: 56 of 241\ttrain_loss: 1.6258\n",
      "Iteration: 58 of 241\ttrain_loss: 1.6207\n",
      "Iteration: 60 of 241\ttrain_loss: 1.4720\n",
      "Iteration: 62 of 241\ttrain_loss: 1.3625\n",
      "Iteration: 64 of 241\ttrain_loss: 1.5079\n",
      "Iteration: 66 of 241\ttrain_loss: 1.2959\n",
      "Iteration: 68 of 241\ttrain_loss: 1.1712\n",
      "Iteration: 70 of 241\ttrain_loss: 1.3204\n",
      "Iteration: 72 of 241\ttrain_loss: 1.8925\n",
      "Iteration: 74 of 241\ttrain_loss: 1.5584\n",
      "Iteration: 76 of 241\ttrain_loss: 1.4389\n",
      "Iteration: 78 of 241\ttrain_loss: 1.5737\n",
      "Iteration: 80 of 241\ttrain_loss: 1.5223\n",
      "Iteration: 82 of 241\ttrain_loss: 1.2834\n",
      "Iteration: 84 of 241\ttrain_loss: 2.0629\n",
      "Iteration: 86 of 241\ttrain_loss: 1.3953\n",
      "Iteration: 88 of 241\ttrain_loss: 1.5752\n",
      "Iteration: 90 of 241\ttrain_loss: 1.3419\n",
      "Iteration: 92 of 241\ttrain_loss: 1.6812\n",
      "Iteration: 94 of 241\ttrain_loss: 1.6258\n",
      "Iteration: 96 of 241\ttrain_loss: 1.4036\n",
      "Iteration: 98 of 241\ttrain_loss: 1.5203\n",
      "Iteration: 100 of 241\ttrain_loss: 1.5654\n",
      "Iteration: 102 of 241\ttrain_loss: 1.4375\n",
      "Iteration: 104 of 241\ttrain_loss: 1.7513\n",
      "Iteration: 106 of 241\ttrain_loss: 1.7217\n",
      "Iteration: 108 of 241\ttrain_loss: 1.6129\n",
      "Iteration: 110 of 241\ttrain_loss: 1.4326\n",
      "Iteration: 112 of 241\ttrain_loss: 1.9631\n",
      "Iteration: 114 of 241\ttrain_loss: 1.7205\n",
      "Iteration: 116 of 241\ttrain_loss: 1.6825\n",
      "Iteration: 118 of 241\ttrain_loss: 1.5602\n",
      "Iteration: 120 of 241\ttrain_loss: 1.3593\n",
      "Iteration: 122 of 241\ttrain_loss: 1.4149\n",
      "Iteration: 124 of 241\ttrain_loss: 2.0298\n",
      "Iteration: 126 of 241\ttrain_loss: 1.3475\n",
      "Iteration: 128 of 241\ttrain_loss: 1.3902\n",
      "Iteration: 130 of 241\ttrain_loss: 1.6049\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6386\n",
      "Iteration: 134 of 241\ttrain_loss: 1.6134\n",
      "Iteration: 136 of 241\ttrain_loss: 1.3191\n",
      "Iteration: 138 of 241\ttrain_loss: 1.9531\n",
      "Iteration: 140 of 241\ttrain_loss: 1.8174\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5403\n",
      "Iteration: 144 of 241\ttrain_loss: 1.3108\n",
      "Iteration: 146 of 241\ttrain_loss: 1.5465\n",
      "Iteration: 148 of 241\ttrain_loss: 1.5269\n",
      "Iteration: 150 of 241\ttrain_loss: 1.2326\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8579\n",
      "Iteration: 154 of 241\ttrain_loss: 1.1219\n",
      "Iteration: 156 of 241\ttrain_loss: 1.0470\n",
      "Iteration: 158 of 241\ttrain_loss: 1.1836\n",
      "Iteration: 160 of 241\ttrain_loss: 1.4451\n",
      "Iteration: 162 of 241\ttrain_loss: 1.4498\n",
      "Iteration: 164 of 241\ttrain_loss: 1.8804\n",
      "Iteration: 166 of 241\ttrain_loss: 1.4575\n",
      "Iteration: 168 of 241\ttrain_loss: 1.4898\n",
      "Iteration: 170 of 241\ttrain_loss: 1.5879\n",
      "Iteration: 172 of 241\ttrain_loss: 1.3375\n",
      "Iteration: 174 of 241\ttrain_loss: 1.3735\n",
      "Iteration: 176 of 241\ttrain_loss: 1.3292\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6494\n",
      "Iteration: 180 of 241\ttrain_loss: 1.4586\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6988\n",
      "Iteration: 184 of 241\ttrain_loss: 1.7183\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4366\n",
      "Iteration: 188 of 241\ttrain_loss: 1.6444\n",
      "Iteration: 190 of 241\ttrain_loss: 1.8091\n",
      "Iteration: 192 of 241\ttrain_loss: 1.6693\n",
      "Iteration: 194 of 241\ttrain_loss: 0.9503\n",
      "Iteration: 196 of 241\ttrain_loss: 1.3248\n",
      "Iteration: 198 of 241\ttrain_loss: 1.5778\n",
      "Iteration: 200 of 241\ttrain_loss: 2.2385\n",
      "Iteration: 202 of 241\ttrain_loss: 1.4057\n",
      "Iteration: 204 of 241\ttrain_loss: 1.6163\n",
      "Iteration: 206 of 241\ttrain_loss: 1.8117\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6192\n",
      "Iteration: 210 of 241\ttrain_loss: 1.8123\n",
      "Iteration: 212 of 241\ttrain_loss: 1.4088\n",
      "Iteration: 214 of 241\ttrain_loss: 1.6262\n",
      "Iteration: 216 of 241\ttrain_loss: 1.6174\n",
      "Iteration: 218 of 241\ttrain_loss: 1.5076\n",
      "Iteration: 220 of 241\ttrain_loss: 1.8218\n",
      "Iteration: 222 of 241\ttrain_loss: 1.5448\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5419\n",
      "Iteration: 226 of 241\ttrain_loss: 1.8556\n",
      "Iteration: 228 of 241\ttrain_loss: 1.9325\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6391\n",
      "Iteration: 232 of 241\ttrain_loss: 1.6024\n",
      "Iteration: 234 of 241\ttrain_loss: 1.6559\n",
      "Iteration: 236 of 241\ttrain_loss: 1.9872\n",
      "Iteration: 238 of 241\ttrain_loss: 1.5211\n",
      "Iteration: 240 of 241\ttrain_loss: 2.1190\n",
      "Iteration: 241 of 241\ttrain_loss: 1.6411\n",
      "Average Score for this Epoch: 1.560162901878357\n",
      "-------------------- Epoch 98 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.1848\n",
      "Iteration: 2 of 241\ttrain_loss: 1.5080\n",
      "Iteration: 4 of 241\ttrain_loss: 1.1190\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6226\n",
      "Iteration: 8 of 241\ttrain_loss: 1.7705\n",
      "Iteration: 10 of 241\ttrain_loss: 1.5409\n",
      "Iteration: 12 of 241\ttrain_loss: 1.6470\n",
      "Iteration: 14 of 241\ttrain_loss: 1.6011\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4951\n",
      "Iteration: 18 of 241\ttrain_loss: 1.2689\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5315\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5082\n",
      "Iteration: 24 of 241\ttrain_loss: 1.4132\n",
      "Iteration: 26 of 241\ttrain_loss: 1.5760\n",
      "Iteration: 28 of 241\ttrain_loss: 1.3789\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4388\n",
      "Iteration: 32 of 241\ttrain_loss: 1.2604\n",
      "Iteration: 34 of 241\ttrain_loss: 1.4519\n",
      "Iteration: 36 of 241\ttrain_loss: 1.4960\n",
      "Iteration: 38 of 241\ttrain_loss: 1.6597\n",
      "Iteration: 40 of 241\ttrain_loss: 1.1762\n",
      "Iteration: 42 of 241\ttrain_loss: 1.1346\n",
      "Iteration: 44 of 241\ttrain_loss: 1.2449\n",
      "Iteration: 46 of 241\ttrain_loss: 1.8069\n",
      "Iteration: 48 of 241\ttrain_loss: 1.0802\n",
      "Iteration: 50 of 241\ttrain_loss: 1.7415\n",
      "Iteration: 52 of 241\ttrain_loss: 1.3277\n",
      "Iteration: 54 of 241\ttrain_loss: 1.4501\n",
      "Iteration: 56 of 241\ttrain_loss: 1.4945\n",
      "Iteration: 58 of 241\ttrain_loss: 1.3688\n",
      "Iteration: 60 of 241\ttrain_loss: 1.7901\n",
      "Iteration: 62 of 241\ttrain_loss: 2.0396\n",
      "Iteration: 64 of 241\ttrain_loss: 1.5581\n",
      "Iteration: 66 of 241\ttrain_loss: 1.1788\n",
      "Iteration: 68 of 241\ttrain_loss: 1.1962\n",
      "Iteration: 70 of 241\ttrain_loss: 1.0916\n",
      "Iteration: 72 of 241\ttrain_loss: 1.1144\n",
      "Iteration: 74 of 241\ttrain_loss: 1.9400\n",
      "Iteration: 76 of 241\ttrain_loss: 1.3057\n",
      "Iteration: 78 of 241\ttrain_loss: 1.3386\n",
      "Iteration: 80 of 241\ttrain_loss: 1.6129\n",
      "Iteration: 82 of 241\ttrain_loss: 1.6484\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3988\n",
      "Iteration: 86 of 241\ttrain_loss: 1.5840\n",
      "Iteration: 88 of 241\ttrain_loss: 2.1543\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7194\n",
      "Iteration: 92 of 241\ttrain_loss: 1.1450\n",
      "Iteration: 94 of 241\ttrain_loss: 1.2680\n",
      "Iteration: 96 of 241\ttrain_loss: 1.1281\n",
      "Iteration: 98 of 241\ttrain_loss: 1.9564\n",
      "Iteration: 100 of 241\ttrain_loss: 1.3403\n",
      "Iteration: 102 of 241\ttrain_loss: 1.8294\n",
      "Iteration: 104 of 241\ttrain_loss: 1.3531\n",
      "Iteration: 106 of 241\ttrain_loss: 1.3600\n",
      "Iteration: 108 of 241\ttrain_loss: 1.0539\n",
      "Iteration: 110 of 241\ttrain_loss: 1.7291\n",
      "Iteration: 112 of 241\ttrain_loss: 1.6799\n",
      "Iteration: 114 of 241\ttrain_loss: 1.9138\n",
      "Iteration: 116 of 241\ttrain_loss: 1.1551\n",
      "Iteration: 118 of 241\ttrain_loss: 1.4654\n",
      "Iteration: 120 of 241\ttrain_loss: 1.1166\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5670\n",
      "Iteration: 124 of 241\ttrain_loss: 0.9456\n",
      "Iteration: 126 of 241\ttrain_loss: 1.4269\n",
      "Iteration: 128 of 241\ttrain_loss: 1.5563\n",
      "Iteration: 130 of 241\ttrain_loss: 1.2612\n",
      "Iteration: 132 of 241\ttrain_loss: 1.2957\n",
      "Iteration: 134 of 241\ttrain_loss: 1.4732\n",
      "Iteration: 136 of 241\ttrain_loss: 1.2706\n",
      "Iteration: 138 of 241\ttrain_loss: 1.2551\n",
      "Iteration: 140 of 241\ttrain_loss: 1.3051\n",
      "Iteration: 142 of 241\ttrain_loss: 1.5012\n",
      "Iteration: 144 of 241\ttrain_loss: 1.5756\n",
      "Iteration: 146 of 241\ttrain_loss: 1.7182\n",
      "Iteration: 148 of 241\ttrain_loss: 2.1771\n",
      "Iteration: 150 of 241\ttrain_loss: 1.5918\n",
      "Iteration: 152 of 241\ttrain_loss: 1.8423\n",
      "Iteration: 154 of 241\ttrain_loss: 1.3272\n",
      "Iteration: 156 of 241\ttrain_loss: 1.3354\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5389\n",
      "Iteration: 160 of 241\ttrain_loss: 1.3122\n",
      "Iteration: 162 of 241\ttrain_loss: 1.7742\n",
      "Iteration: 164 of 241\ttrain_loss: 1.6658\n",
      "Iteration: 166 of 241\ttrain_loss: 1.6339\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7277\n",
      "Iteration: 170 of 241\ttrain_loss: 1.4237\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7913\n",
      "Iteration: 174 of 241\ttrain_loss: 1.5491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 176 of 241\ttrain_loss: 1.2227\n",
      "Iteration: 178 of 241\ttrain_loss: 1.6895\n",
      "Iteration: 180 of 241\ttrain_loss: 1.6725\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6393\n",
      "Iteration: 184 of 241\ttrain_loss: 1.3322\n",
      "Iteration: 186 of 241\ttrain_loss: 1.4985\n",
      "Iteration: 188 of 241\ttrain_loss: 1.3549\n",
      "Iteration: 190 of 241\ttrain_loss: 2.0376\n",
      "Iteration: 192 of 241\ttrain_loss: 1.1883\n",
      "Iteration: 194 of 241\ttrain_loss: 1.6459\n",
      "Iteration: 196 of 241\ttrain_loss: 1.5160\n",
      "Iteration: 198 of 241\ttrain_loss: 1.3397\n",
      "Iteration: 200 of 241\ttrain_loss: 1.2993\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8289\n",
      "Iteration: 204 of 241\ttrain_loss: 2.2610\n",
      "Iteration: 206 of 241\ttrain_loss: 1.5388\n",
      "Iteration: 208 of 241\ttrain_loss: 1.5555\n",
      "Iteration: 210 of 241\ttrain_loss: 1.5657\n",
      "Iteration: 212 of 241\ttrain_loss: 1.5123\n",
      "Iteration: 214 of 241\ttrain_loss: 1.3151\n",
      "Iteration: 216 of 241\ttrain_loss: 1.7473\n",
      "Iteration: 218 of 241\ttrain_loss: 1.6026\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6068\n",
      "Iteration: 222 of 241\ttrain_loss: 1.7407\n",
      "Iteration: 224 of 241\ttrain_loss: 1.7695\n",
      "Iteration: 226 of 241\ttrain_loss: 1.4038\n",
      "Iteration: 228 of 241\ttrain_loss: 1.7419\n",
      "Iteration: 230 of 241\ttrain_loss: 1.8629\n",
      "Iteration: 232 of 241\ttrain_loss: 1.7018\n",
      "Iteration: 234 of 241\ttrain_loss: 1.6311\n",
      "Iteration: 236 of 241\ttrain_loss: 1.6034\n",
      "Iteration: 238 of 241\ttrain_loss: 1.6310\n",
      "Iteration: 240 of 241\ttrain_loss: 1.4243\n",
      "Iteration: 241 of 241\ttrain_loss: 1.3741\n",
      "Average Score for this Epoch: 1.527638554573059\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 99 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.6193\n",
      "Iteration: 2 of 241\ttrain_loss: 1.6592\n",
      "Iteration: 4 of 241\ttrain_loss: 1.3461\n",
      "Iteration: 6 of 241\ttrain_loss: 1.6374\n",
      "Iteration: 8 of 241\ttrain_loss: 0.8279\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6523\n",
      "Iteration: 12 of 241\ttrain_loss: 1.4299\n",
      "Iteration: 14 of 241\ttrain_loss: 1.2949\n",
      "Iteration: 16 of 241\ttrain_loss: 1.4368\n",
      "Iteration: 18 of 241\ttrain_loss: 1.3972\n",
      "Iteration: 20 of 241\ttrain_loss: 1.6125\n",
      "Iteration: 22 of 241\ttrain_loss: 1.5490\n",
      "Iteration: 24 of 241\ttrain_loss: 1.3313\n",
      "Iteration: 26 of 241\ttrain_loss: 1.6897\n",
      "Iteration: 28 of 241\ttrain_loss: 1.5602\n",
      "Iteration: 30 of 241\ttrain_loss: 1.5277\n",
      "Iteration: 32 of 241\ttrain_loss: 1.7420\n",
      "Iteration: 34 of 241\ttrain_loss: 1.2296\n",
      "Iteration: 36 of 241\ttrain_loss: 1.6691\n",
      "Iteration: 38 of 241\ttrain_loss: 1.0862\n",
      "Iteration: 40 of 241\ttrain_loss: 1.3324\n",
      "Iteration: 42 of 241\ttrain_loss: 1.8298\n",
      "Iteration: 44 of 241\ttrain_loss: 1.5978\n",
      "Iteration: 46 of 241\ttrain_loss: 1.1762\n",
      "Iteration: 48 of 241\ttrain_loss: 2.1389\n",
      "Iteration: 50 of 241\ttrain_loss: 1.1646\n",
      "Iteration: 52 of 241\ttrain_loss: 1.2928\n",
      "Iteration: 54 of 241\ttrain_loss: 1.6867\n",
      "Iteration: 56 of 241\ttrain_loss: 1.5157\n",
      "Iteration: 58 of 241\ttrain_loss: 1.5278\n",
      "Iteration: 60 of 241\ttrain_loss: 1.4609\n",
      "Iteration: 62 of 241\ttrain_loss: 1.9734\n",
      "Iteration: 64 of 241\ttrain_loss: 1.1650\n",
      "Iteration: 66 of 241\ttrain_loss: 1.4002\n",
      "Iteration: 68 of 241\ttrain_loss: 1.4534\n",
      "Iteration: 70 of 241\ttrain_loss: 1.4626\n",
      "Iteration: 72 of 241\ttrain_loss: 1.7138\n",
      "Iteration: 74 of 241\ttrain_loss: 1.0745\n",
      "Iteration: 76 of 241\ttrain_loss: 1.4944\n",
      "Iteration: 78 of 241\ttrain_loss: 1.3442\n",
      "Iteration: 80 of 241\ttrain_loss: 1.5765\n",
      "Iteration: 82 of 241\ttrain_loss: 1.5160\n",
      "Iteration: 84 of 241\ttrain_loss: 1.3378\n",
      "Iteration: 86 of 241\ttrain_loss: 1.2102\n",
      "Iteration: 88 of 241\ttrain_loss: 1.4386\n",
      "Iteration: 90 of 241\ttrain_loss: 1.7584\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7100\n",
      "Iteration: 94 of 241\ttrain_loss: 1.4410\n",
      "Iteration: 96 of 241\ttrain_loss: 1.5870\n",
      "Iteration: 98 of 241\ttrain_loss: 1.5369\n",
      "Iteration: 100 of 241\ttrain_loss: 1.6361\n",
      "Iteration: 102 of 241\ttrain_loss: 1.3359\n",
      "Iteration: 104 of 241\ttrain_loss: 1.6131\n",
      "Iteration: 106 of 241\ttrain_loss: 1.8920\n",
      "Iteration: 108 of 241\ttrain_loss: 1.6383\n",
      "Iteration: 110 of 241\ttrain_loss: 1.7214\n",
      "Iteration: 112 of 241\ttrain_loss: 1.3535\n",
      "Iteration: 114 of 241\ttrain_loss: 1.4048\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4372\n",
      "Iteration: 118 of 241\ttrain_loss: 1.6109\n",
      "Iteration: 120 of 241\ttrain_loss: 1.4538\n",
      "Iteration: 122 of 241\ttrain_loss: 1.3350\n",
      "Iteration: 124 of 241\ttrain_loss: 1.7871\n",
      "Iteration: 126 of 241\ttrain_loss: 1.6336\n",
      "Iteration: 128 of 241\ttrain_loss: 1.6714\n",
      "Iteration: 130 of 241\ttrain_loss: 1.4269\n",
      "Iteration: 132 of 241\ttrain_loss: 1.7251\n",
      "Iteration: 134 of 241\ttrain_loss: 1.3298\n",
      "Iteration: 136 of 241\ttrain_loss: 1.8882\n",
      "Iteration: 138 of 241\ttrain_loss: 1.6876\n",
      "Iteration: 140 of 241\ttrain_loss: 1.4121\n",
      "Iteration: 142 of 241\ttrain_loss: 1.8025\n",
      "Iteration: 144 of 241\ttrain_loss: 1.7555\n",
      "Iteration: 146 of 241\ttrain_loss: 1.0646\n",
      "Iteration: 148 of 241\ttrain_loss: 1.3390\n",
      "Iteration: 150 of 241\ttrain_loss: 1.3805\n",
      "Iteration: 152 of 241\ttrain_loss: 1.6387\n",
      "Iteration: 154 of 241\ttrain_loss: 1.3653\n",
      "Iteration: 156 of 241\ttrain_loss: 1.4884\n",
      "Iteration: 158 of 241\ttrain_loss: 1.6451\n",
      "Iteration: 160 of 241\ttrain_loss: 1.6133\n",
      "Iteration: 162 of 241\ttrain_loss: 1.3410\n",
      "Iteration: 164 of 241\ttrain_loss: 1.3790\n",
      "Iteration: 166 of 241\ttrain_loss: 1.6379\n",
      "Iteration: 168 of 241\ttrain_loss: 1.7544\n",
      "Iteration: 170 of 241\ttrain_loss: 1.7225\n",
      "Iteration: 172 of 241\ttrain_loss: 1.3049\n",
      "Iteration: 174 of 241\ttrain_loss: 1.4339\n",
      "Iteration: 176 of 241\ttrain_loss: 1.5051\n",
      "Iteration: 178 of 241\ttrain_loss: 1.5550\n",
      "Iteration: 180 of 241\ttrain_loss: 1.4049\n",
      "Iteration: 182 of 241\ttrain_loss: 1.6050\n",
      "Iteration: 184 of 241\ttrain_loss: 1.6308\n",
      "Iteration: 186 of 241\ttrain_loss: 1.5798\n",
      "Iteration: 188 of 241\ttrain_loss: 1.3420\n",
      "Iteration: 190 of 241\ttrain_loss: 2.1427\n",
      "Iteration: 192 of 241\ttrain_loss: 1.6776\n",
      "Iteration: 194 of 241\ttrain_loss: 1.5764\n",
      "Iteration: 196 of 241\ttrain_loss: 1.5233\n",
      "Iteration: 198 of 241\ttrain_loss: 1.5438\n",
      "Iteration: 200 of 241\ttrain_loss: 1.5415\n",
      "Iteration: 202 of 241\ttrain_loss: 1.8685\n",
      "Iteration: 204 of 241\ttrain_loss: 1.2723\n",
      "Iteration: 206 of 241\ttrain_loss: 1.4497\n",
      "Iteration: 208 of 241\ttrain_loss: 1.6948\n",
      "Iteration: 210 of 241\ttrain_loss: 1.6826\n",
      "Iteration: 212 of 241\ttrain_loss: 1.8059\n",
      "Iteration: 214 of 241\ttrain_loss: 1.8279\n",
      "Iteration: 216 of 241\ttrain_loss: 1.6242\n",
      "Iteration: 218 of 241\ttrain_loss: 1.8499\n",
      "Iteration: 220 of 241\ttrain_loss: 1.6250\n",
      "Iteration: 222 of 241\ttrain_loss: 1.2174\n",
      "Iteration: 224 of 241\ttrain_loss: 1.4944\n",
      "Iteration: 226 of 241\ttrain_loss: 1.6097\n",
      "Iteration: 228 of 241\ttrain_loss: 1.5957\n",
      "Iteration: 230 of 241\ttrain_loss: 1.2565\n",
      "Iteration: 232 of 241\ttrain_loss: 1.5643\n",
      "Iteration: 234 of 241\ttrain_loss: 1.4128\n",
      "Iteration: 236 of 241\ttrain_loss: 1.7089\n",
      "Iteration: 238 of 241\ttrain_loss: 1.0542\n",
      "Iteration: 240 of 241\ttrain_loss: 1.6886\n",
      "Iteration: 241 of 241\ttrain_loss: 1.7875\n",
      "Average Score for this Epoch: 1.5250279903411865\n",
      "--- new best score ---\n",
      "\n",
      "\n",
      "-------------------- Epoch 100 of 100 --------------------\n",
      "Iteration: 0 of 241\ttrain_loss: 1.3743\n",
      "Iteration: 2 of 241\ttrain_loss: 1.5592\n",
      "Iteration: 4 of 241\ttrain_loss: 1.7548\n",
      "Iteration: 6 of 241\ttrain_loss: 2.0430\n",
      "Iteration: 8 of 241\ttrain_loss: 1.4493\n",
      "Iteration: 10 of 241\ttrain_loss: 1.6143\n",
      "Iteration: 12 of 241\ttrain_loss: 1.7120\n",
      "Iteration: 14 of 241\ttrain_loss: 2.2940\n",
      "Iteration: 16 of 241\ttrain_loss: 1.7105\n",
      "Iteration: 18 of 241\ttrain_loss: 1.1146\n",
      "Iteration: 20 of 241\ttrain_loss: 1.5466\n",
      "Iteration: 22 of 241\ttrain_loss: 1.3414\n",
      "Iteration: 24 of 241\ttrain_loss: 1.3344\n",
      "Iteration: 26 of 241\ttrain_loss: 0.9776\n",
      "Iteration: 28 of 241\ttrain_loss: 1.4934\n",
      "Iteration: 30 of 241\ttrain_loss: 1.4872\n",
      "Iteration: 32 of 241\ttrain_loss: 1.3074\n",
      "Iteration: 34 of 241\ttrain_loss: 1.6703\n",
      "Iteration: 36 of 241\ttrain_loss: 1.3207\n",
      "Iteration: 38 of 241\ttrain_loss: 1.3310\n",
      "Iteration: 40 of 241\ttrain_loss: 1.3977\n",
      "Iteration: 42 of 241\ttrain_loss: 1.2700\n",
      "Iteration: 44 of 241\ttrain_loss: 1.3303\n",
      "Iteration: 46 of 241\ttrain_loss: 1.1476\n",
      "Iteration: 48 of 241\ttrain_loss: 1.0348\n",
      "Iteration: 50 of 241\ttrain_loss: 1.5618\n",
      "Iteration: 52 of 241\ttrain_loss: 1.6202\n",
      "Iteration: 54 of 241\ttrain_loss: 1.3301\n",
      "Iteration: 56 of 241\ttrain_loss: 1.0131\n",
      "Iteration: 58 of 241\ttrain_loss: 1.7108\n",
      "Iteration: 60 of 241\ttrain_loss: 1.3833\n",
      "Iteration: 62 of 241\ttrain_loss: 1.4597\n",
      "Iteration: 64 of 241\ttrain_loss: 1.4623\n",
      "Iteration: 66 of 241\ttrain_loss: 1.7022\n",
      "Iteration: 68 of 241\ttrain_loss: 1.3268\n",
      "Iteration: 70 of 241\ttrain_loss: 1.7094\n",
      "Iteration: 72 of 241\ttrain_loss: 1.4568\n",
      "Iteration: 74 of 241\ttrain_loss: 1.7512\n",
      "Iteration: 76 of 241\ttrain_loss: 1.7862\n",
      "Iteration: 78 of 241\ttrain_loss: 1.8455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 80 of 241\ttrain_loss: 1.0229\n",
      "Iteration: 82 of 241\ttrain_loss: 1.4412\n",
      "Iteration: 84 of 241\ttrain_loss: 1.7134\n",
      "Iteration: 86 of 241\ttrain_loss: 1.1446\n",
      "Iteration: 88 of 241\ttrain_loss: 1.2594\n",
      "Iteration: 90 of 241\ttrain_loss: 1.3724\n",
      "Iteration: 92 of 241\ttrain_loss: 1.7085\n",
      "Iteration: 94 of 241\ttrain_loss: 1.5206\n",
      "Iteration: 96 of 241\ttrain_loss: 1.3835\n",
      "Iteration: 98 of 241\ttrain_loss: 1.3091\n",
      "Iteration: 100 of 241\ttrain_loss: 1.3919\n",
      "Iteration: 102 of 241\ttrain_loss: 1.2505\n",
      "Iteration: 104 of 241\ttrain_loss: 1.9460\n",
      "Iteration: 106 of 241\ttrain_loss: 1.7351\n",
      "Iteration: 108 of 241\ttrain_loss: 2.0143\n",
      "Iteration: 110 of 241\ttrain_loss: 1.2840\n",
      "Iteration: 112 of 241\ttrain_loss: 1.8864\n",
      "Iteration: 114 of 241\ttrain_loss: 1.7335\n",
      "Iteration: 116 of 241\ttrain_loss: 1.4207\n",
      "Iteration: 118 of 241\ttrain_loss: 1.7628\n",
      "Iteration: 120 of 241\ttrain_loss: 1.1641\n",
      "Iteration: 122 of 241\ttrain_loss: 1.5021\n",
      "Iteration: 124 of 241\ttrain_loss: 1.0817\n",
      "Iteration: 126 of 241\ttrain_loss: 1.8321\n",
      "Iteration: 128 of 241\ttrain_loss: 1.8317\n",
      "Iteration: 130 of 241\ttrain_loss: 1.5124\n",
      "Iteration: 132 of 241\ttrain_loss: 1.6124\n",
      "Iteration: 134 of 241\ttrain_loss: 1.5621\n",
      "Iteration: 136 of 241\ttrain_loss: 1.6122\n",
      "Iteration: 138 of 241\ttrain_loss: 1.5963\n",
      "Iteration: 140 of 241\ttrain_loss: 1.5407\n",
      "Iteration: 142 of 241\ttrain_loss: 1.6955\n",
      "Iteration: 144 of 241\ttrain_loss: 1.6399\n",
      "Iteration: 146 of 241\ttrain_loss: 1.7301\n",
      "Iteration: 148 of 241\ttrain_loss: 1.7644\n",
      "Iteration: 150 of 241\ttrain_loss: 1.4074\n",
      "Iteration: 152 of 241\ttrain_loss: 2.0986\n",
      "Iteration: 154 of 241\ttrain_loss: 1.6803\n",
      "Iteration: 156 of 241\ttrain_loss: 1.2149\n",
      "Iteration: 158 of 241\ttrain_loss: 1.5986\n",
      "Iteration: 160 of 241\ttrain_loss: 1.5376\n",
      "Iteration: 162 of 241\ttrain_loss: 1.6007\n",
      "Iteration: 164 of 241\ttrain_loss: 1.1987\n",
      "Iteration: 166 of 241\ttrain_loss: 1.5237\n",
      "Iteration: 168 of 241\ttrain_loss: 1.3097\n",
      "Iteration: 170 of 241\ttrain_loss: 1.5903\n",
      "Iteration: 172 of 241\ttrain_loss: 1.7896\n",
      "Iteration: 174 of 241\ttrain_loss: 1.4718\n",
      "Iteration: 176 of 241\ttrain_loss: 1.1588\n",
      "Iteration: 178 of 241\ttrain_loss: 1.4231\n",
      "Iteration: 180 of 241\ttrain_loss: 1.5901\n",
      "Iteration: 182 of 241\ttrain_loss: 1.4314\n",
      "Iteration: 184 of 241\ttrain_loss: 1.2642\n",
      "Iteration: 186 of 241\ttrain_loss: 1.6663\n",
      "Iteration: 188 of 241\ttrain_loss: 1.5431\n",
      "Iteration: 190 of 241\ttrain_loss: 1.3996\n",
      "Iteration: 192 of 241\ttrain_loss: 1.1313\n",
      "Iteration: 194 of 241\ttrain_loss: 1.7473\n",
      "Iteration: 196 of 241\ttrain_loss: 1.1463\n",
      "Iteration: 198 of 241\ttrain_loss: 1.3155\n",
      "Iteration: 200 of 241\ttrain_loss: 2.1059\n",
      "Iteration: 202 of 241\ttrain_loss: 1.4248\n",
      "Iteration: 204 of 241\ttrain_loss: 1.8314\n",
      "Iteration: 206 of 241\ttrain_loss: 1.4715\n",
      "Iteration: 208 of 241\ttrain_loss: 1.2389\n",
      "Iteration: 210 of 241\ttrain_loss: 1.3180\n",
      "Iteration: 212 of 241\ttrain_loss: 1.7568\n",
      "Iteration: 214 of 241\ttrain_loss: 1.4969\n",
      "Iteration: 216 of 241\ttrain_loss: 1.1718\n",
      "Iteration: 218 of 241\ttrain_loss: 1.7622\n",
      "Iteration: 220 of 241\ttrain_loss: 1.3529\n",
      "Iteration: 222 of 241\ttrain_loss: 1.5729\n",
      "Iteration: 224 of 241\ttrain_loss: 1.5442\n",
      "Iteration: 226 of 241\ttrain_loss: 1.8401\n",
      "Iteration: 228 of 241\ttrain_loss: 1.6208\n",
      "Iteration: 230 of 241\ttrain_loss: 1.6150\n",
      "Iteration: 232 of 241\ttrain_loss: 1.5126\n",
      "Iteration: 234 of 241\ttrain_loss: 1.6150\n",
      "Iteration: 236 of 241\ttrain_loss: 1.4777\n",
      "Iteration: 238 of 241\ttrain_loss: 1.7094\n",
      "Iteration: 240 of 241\ttrain_loss: 2.1169\n",
      "Iteration: 241 of 241\ttrain_loss: 1.7419\n",
      "Average Score for this Epoch: 1.5113734006881714\n",
      "--- new best score ---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build graph and train the model \n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   save_path='./models/headlines/my_modelA',\n",
    "                                   mode='TRAIN',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   batch_size = batch_size,\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = keep_probability,\n",
    "                                   learning_rate = learning_rate,\n",
    "                                   max_lr=max_lr,\n",
    "                                   learning_rate_decay_steps = learning_rate_decay_steps,\n",
    "                                   learning_rate_decay = learning_rate_decay,\n",
    "                                   epochs = epochs,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path,\n",
    "                                   use_cyclic_lr = use_cyclic_lr,\n",
    "                                   summary_dir = summary_dir)           \n",
    "\n",
    "summarizer.build_graph()\n",
    "summarizer.train(converted_texts, \n",
    "                 converted_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now we can use our trained model to create summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained embeddings.\n",
      "Graph built.\n",
      "INFO:tensorflow:Restoring parameters from ./models/headlines/my_modelA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0403 19:07:04.835104 140079934850880 saver.py:1270] Restoring parameters from ./models/headlines/my_modelA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summarizer_model_utils.reset_graph()\n",
    "summarizer = Summarizer.Summarizer(word2ind,\n",
    "                                   ind2word,\n",
    "                                   './models/headlines/my_modelA',\n",
    "                                   'INFER',\n",
    "                                   num_layers_encoder = num_layers_encoder,\n",
    "                                   num_layers_decoder = num_layers_decoder,\n",
    "                                   batch_size = len(converted_texts[:50]),\n",
    "                                   clip = clip,\n",
    "                                   keep_probability = 1.0,\n",
    "                                   learning_rate = 0.0,\n",
    "                                   beam_width = 5,\n",
    "                                   rnn_size_encoder = rnn_size_encoder,\n",
    "                                   rnn_size_decoder = rnn_size_decoder,\n",
    "                                   inference_targets = False,\n",
    "                                   pretrained_embeddings_path = pretrained_embeddings_path)\n",
    "\n",
    "summarizer.build_graph()\n",
    "preds = summarizer.infer(converted_texts[:50],\n",
    "                         restore_path =  './models/headlines/my_modelA',\n",
    "                         targets = converted_summaries[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi infosys ltd india s second biggest it services company said on thursday it would buy a 75 per cent stake in abn amro group nv s mortgage administration services unit for 127 5 million euros 143 53 million infosys will acquire the stake in stater nv through unit infosys consulting pvt ltd and the transaction is expected to close in the first quarter of fiscal 2020 this is in line with infosys strategy to strengthen its mortgage servicing capabilities in continental europe the software services company said in a statement abn amro will continue to hold the remaining 25 per cent stake in stater which operates in the netherlands belgium and germany infosys had gained 1 6 per cent by 12 58 pm while the broader mumbai market was up 0 58 per cent\n",
      "\n",
      "Actual Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro s mortgage services arm times of india\n",
      "\n",
      "Created Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi the tax department has said it is premature to estimate any shortfall in direct tax receipts for the current financial year which ends in march and said the actual picture would be known by the middle of april reports have suggested that the central board of direct taxes cbdt has shot off a letter to field formations to go all out to bridge the shortfall in direct taxes the reports have said that direct tax receipts may fall short by 15 of the target this is a routine letter we send out to field formations in the third quarter of the financial year said an official who did not wish to be identified how can we guess what the shortfall is taxes keep coming till april 1 and <UNK> taxes until april 15 the official said adding that the final picture will emerge early next month earlier this year cbdt had expressed confidence about meeting the target of rs 11 5 lakh crore for current fiscal year\n",
      "\n",
      "Actual Summary:\n",
      "premature to estimate direct tax shortfall times of india\n",
      "\n",
      "Created Summary:\n",
      "premature to estimate direct tax shortfall times\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "paris huawei the chinese mobile manufacturer on tuesday launched its p30 series of smartphones huawei p30 and huawei p30 pro with a camera that boasts of cinematic capabilities the company said the <UNK> feature supports high fidelity <UNK> up to 5x optical zoom <UNK> hybrid zoom and <UNK> digital zoom which it achieves through a periscope design the phones come with a <UNK> front camera for selfies richard yu ceo of huawei consumer business group said the latest huawei p30 series represents a breakthrough after decades of digital camera technology development that will rewrite the rules and reshape the perception that consumers have of mobile photography p30 pro is priced at 999 euros for 8gb ram 128 rom to 1 <UNK> euros 8gb ram 512 gb rom the p30 costs 799 euros for <UNK> ram and 128 rom the p30 series would be launched in india in april a company source said\n",
      "\n",
      "Actual Summary:\n",
      "huawei launches p30 series times of india\n",
      "\n",
      "Created Summary:\n",
      "huawei launches p30 series of of india\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi indian state run e commerce firm mstc limited got listed on the exchange today friday march 29 2019 the stock made a tepid market debut as it got listed at rs 111 on bombay stock exchange <UNK> a discount of 7 5 per cent to the issue price of rs 120 the initial public offering ipo of the company received bids for 2 58 29 100 shares against the total issue size of 1 76 70 400 shares the company is expected to raise as much as rs 2 26 billion through the ipo mstc had revised the ipo price range to rs 120 rs 128 per share according to the <UNK> website it had earlier priced the ipo between rs 121 to rs 128 per share mstc a kolkata based firm was incorporated in 1964 as a trading company to deal in the export of scrap there are three main business <UNK> in the company e commerce trading and recycling\n",
      "\n",
      "Actual Summary:\n",
      "mstc share price mstc makes market debut lists at 7 5 discount times of india\n",
      "\n",
      "Created Summary:\n",
      "mstc share price mstc makes market debut lists <UNK> <UNK>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "san francisco facebook on wednesday announced it will ban praise or support for white nationalism at the leading online social network and its image <UNK> messaging service instagram facebook said it will begin enforcing the ban next week ratcheting up its rules against hateful content to include white nationalism and separatism it s clear that these concepts are deeply linked to organized hate groups and have no place on our services the social network said\n",
      "\n",
      "Actual Summary:\n",
      "facebook bans praise of white nationalism separatism times of india\n",
      "\n",
      "Created Summary:\n",
      "the the of of the extremely political\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi british motorcycle manufacturer triumph motorcycles ltd has announced a recall of certain models affecting around 1 000 motorcycles in india affected model include headstock tidy street scrambler street twin bonneville t100 and bonneville t120 changes and upgrades in indicator main beam malfunction and a possible loss of engine power caused by the clutch cable contacting the main harness resulting in damage to the wiring within the main harness will be carried out any concerned customers are asked in the first instance to contact their local triumph dealer as soon as possible who will be able to carry out all necessary work as required at no cost to the owner the company said in a statement\n",
      "\n",
      "Actual Summary:\n",
      "triumph motorcycles ltd triumph recalls 1 000 motorcycles in india for minor upgrades times of india\n",
      "\n",
      "Created Summary:\n",
      "triumph motorcycles ltd triumph recalls 1 000 motorcycles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi infosys ltd india s second biggest it services company said on thursday it would buy a 75 per cent stake in abn amro group nv s mortgage administration services unit for 127 5 million euros 143 53 million infosys will acquire the stake in stater nv through unit infosys consulting pvt ltd and the transaction is expected to close in the first quarter of fiscal 2020 this is in line with infosys strategy to strengthen its mortgage servicing capabilities in continental europe the software services company said in a statement abn amro will continue to hold the remaining 25 per cent stake in stater which operates in the netherlands belgium and germany infosys had gained 1 6 per cent by 12 58 pm while the broader mumbai market was up 0 58 per cent\n",
      "\n",
      "Actual Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro s mortgage services arm times of india\n",
      "\n",
      "Created Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "patna rjd chief lalu prasad s elder son tej pratap who had otherwise been keeping silent since the declaration of the lok sabha election dates dropped two bombshells on thursday by resigning as chief of the party s student wing and seeking tickets for his candidates of choice for jehanabad and sheohar seats i am resigning as chief of chhatra rjd naadan hain woh log jo mujhe naadan samajhte hain those who think i am naive are mistaken he tweeted earlier in the day tej said that he had apprised his younger brother tejashwi of his two choices he refuted reports that he would form his own party tej said his picks angesh singh sheohar and chandra prakash jehanabad were dedicated to rjd rjd sources however said the party was likely to field surendra yadav from jehanabad\n",
      "\n",
      "Actual Summary:\n",
      "tej pratap yadav bihar demanding tickets for 2 aides tej pratap quits party post\n",
      "\n",
      "Created Summary:\n",
      "tej pratap yadav bihar demanding tickets for for aides tej pratap quits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi with the 2019 election campaigning underway the financial <UNK> of parties is coming into play as opposition efforts are on to oust bjp from the centre during 2017 18 six national parties earned a total of rs <UNK> 05 crore but nearly rs 700 crore of this income came from unknown sources as ec doesn t require parties to declare every source of earning here s how six national parties were funded in 2017 18\n",
      "\n",
      "Actual Summary:\n",
      "ls polls bjp earned five times more than congress in 2017 18\n",
      "\n",
      "Created Summary:\n",
      "the the the trump trump s s campus building\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "a video appeared on social media on thursday in which a woman who had her face covered claimed that she had warned karnataka minister <UNK> <UNK> about possible raids she claimed that a cabbie had tipped her about large number of suvs being booked for thursday for trips to hassan <UNK> and shivamogga and that his hunch was that the vehicles had been booked to transport i t officials jd s leaders were not available for comment on wednesday the cm had claimed that he had information about up to 300 i t personnel planning a <UNK> to intimidate <UNK> ahead of the polls\n",
      "\n",
      "Actual Summary:\n",
      "video claims minister was tipped off\n",
      "\n",
      "Created Summary:\n",
      "woman claims minister in tipped off\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi the congress late thursday night issued a list of 31 candidates for the lok sabha polls fielding rajasthan chief minister ashok gehlot s son vaibhav from jodhpur and former bjp leader jaswant singh s son manvendra from barmer the party s latest list has 19 candidate for rajasthan and six each for uttar pradesh and gujarat with this the number of total candidates announced by the party for the lok sabha polls has reached 293 in rajasthan the party has fielded forer union minister jitendra singh from alwar senior leaders ramnarayan meena from kota savita meena from dausa st jyoti khandelwal from jaipur namo narayan meena from tonk sawai madhopur and jyoti mirdha from nagaur among others in gujarat the party announced jagdish thakor from patan and lalit kagathara from rajkot among others in uttar pradesh it fielded major jp singh retd from sambhal and niaz ahmed from deoria and pankaj niranjan from phulpur\n",
      "\n",
      "Actual Summary:\n",
      "congress gives ticket to ashok gehlot s son fields jaswant singh s son from barmer\n",
      "\n",
      "Created Summary:\n",
      "congress gives ticket to ashok gehlot s s fields jaswant singh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "srinagar two terrorists were killed in an encounter with security forces in nowgam area of the summer capital of jammu and kashmir on friday morning police said security forces had launched a cordon and search operation in nowgam area following information about presence of terrorists there a police official said he said the two terrorists were killed in the ensuing encounter and their identity and affiliation were being <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "j k 2 terrorists killed in encounter\n",
      "\n",
      "Created Summary:\n",
      "the on on on on the the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "dehradun in another startling disclosure on thursday members of the <UNK> state commission for protection of child rights uscpcr investigating the brutal murder of a 12 year old boy in a private boarding school at ranipokhari area on the outskirts of dehradun revealed that another student from the school had vanished without a trace during a <UNK> excursion in 2017 <UNK> <UNK> <UNK> uscpcr who visited the school on thursday said that she would ask police to start a fresh investigation into the disappearance of the missing boy the allegations were confirmed by <UNK> singh <UNK> the then <UNK> at ranipokhari police station who told toi that a class 10 student at the school mohan <UNK> had indeed gone missing on a trek in 2017\n",
      "\n",
      "Actual Summary:\n",
      "dehradun school in 2017 another boy went missing\n",
      "\n",
      "Created Summary:\n",
      "dehradun school another 2017 another boy went\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi twitter has taken down a tweet spreading fake information about ingredients of the indelible ink used to mark voters on a request made by the election commission on march 26 ec wrote to twitter pointing to the very <UNK> and misleading message being circulated on it the ec has also written to delhi police to lodge an fir against the handle which posted the tweet in a letter to the police the ec said the tweet appears deliberately intended to cause mischief and <UNK> the voters specially belonging to a particular community meanwhile ec on thursday also directed tamil nadu police to file an fir against dmk mla from <UNK> <UNK> <UNK> and seven others for distributing cash to the public the cash was distributed in the presence of dmk mp <UNK>\n",
      "\n",
      "Actual Summary:\n",
      "on election commission request twitter takes down fake post\n",
      "\n",
      "Created Summary:\n",
      "new election commission request twitter takes down down\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "kolkata west bengal chief minister mamata banerjee on wednesday dismissed congress chief rahul gandhi s allegations against her government saying he is just a kid let him say what he wants why will i react to everything banerjee said the trinamool chief showed no intention to confront rahul and concede any advantage to modi banerjee in fact spared rahul any criticism though trinamool congress seniors such as partha chatterjee and <UNK> hakim had said the comments would only weaken the fight against the saffron party in bengal in his first election meeting in the state rahul had called bengal a one person show saying the cm takes her own decisions she doesn t consult anyone\n",
      "\n",
      "Actual Summary:\n",
      "mamata banerjee on congress chief he is just a kid\n",
      "\n",
      "Created Summary:\n",
      "mamata banerjee and congress congress bill clinton clinton won kid\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi former bsf officer k k sharma who was only recently appointed by ec as central police observer for west bengal and jharkhand has recused himself from the assignment in view of the trinamool congress and left <UNK> to his appointment citing his presence at an rss event last year sharma is believed to have conveyed to ec his reluctance to oversee security measures including central forces deployment in west bengal saying that he was <UNK> by the allegations the ec accepting his recusal has now posted him as central police observer for andhra pradesh and telangana former <UNK> officer vivek <UNK> 1981 batch andhra pradesh <UNK> will now be stepping in as the central police observer for west bengal and jharkhand on thursday evening all the central police observers met the full commission to discuss their responsibility and duties and likely security challenges in their respective states\n",
      "\n",
      "Actual Summary:\n",
      "former bsf dg removed from central police observer s job in west bengal jharkhand\n",
      "\n",
      "Created Summary:\n",
      "former bsf dg removed last central police observer her police\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "patna new delhi tension erupted between congress and rjd on thursday forcing them to cancel a press conference that had been scheduled to declare the names of 31 candidates of the anti bjp front in bihar the two partners who had declared candidates for nine seats locked horns again over darbhanga bettiah and supaul bjp rebel shatrughan sinha became the collateral victim of the flare up the former film star an ls member from patna sahib was expected to join congress on thursday a press meet announcing that had to be cancelled congress later released a picture of sinha and rahul gandhi shaking hands with the information that sinha would join the party on april 6\n",
      "\n",
      "Actual Summary:\n",
      "rjd congress news congress rjd friction delays shatrughan sinha entry\n",
      "\n",
      "Created Summary:\n",
      "rjd congress news congress rjd friction delays shatrughan sinha entry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "ranchi noted development economist jean dreze and his associates vivek kumar gupta and <UNK> kumar gupta were detained by police for about two hours on thursday just before they were about to host a public meeting in jharkhand allegedly without taking permission from the state administration the team had organised the meeting to listen to people s grievances related to social security pensions and delayed allotment of <UNK> after being released dreze said that police had threatened the organiser with legal consequences\n",
      "\n",
      "Actual Summary:\n",
      "economist dreze held for trying to host meet freed\n",
      "\n",
      "Created Summary:\n",
      "the on on on on on on on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "mumbai the bombay high court on thursday <UNK> at cm devendra <UNK> saying although he holds the home department portfolio he finds no time to take stock of the case related to the killing of <UNK> leader <UNK> pansare what is the cm doing he holds 11 portfolios including home department but do sic not find the time to take stock of the case remarked a bench of justices satyaranjan dharmadhikari and burgess <UNK> which is monitoring the probe on march 14 the court had said the attempt of the state <UNK> s sit to trace the <UNK> accused has been reduced to a laughing stock counsel for the state senior advocate ashok <UNK> said the strength of the probe team has been increased to nearly 35 we will continue with the same investigating officer his work will be reviewed every 15 days by the sit chief he added\n",
      "\n",
      "Actual Summary:\n",
      "hc pulls up maharashtra cm over pansare probe\n",
      "\n",
      "Created Summary:\n",
      "hc pulls up maharashtra cm over pansare probe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi some never tire of trying like dr k padmarajan starting 1988 the man from salem tamil nadu has contested 170 elections and the 60 year old has lost all of them the <UNK> book of records names him as india s most unsuccessful candidate dr padmarajan a <UNK> doctor turned businessman has <UNK> himself as all india election king he has contested not just local and parliamentary but also presidential elections <UNK> his deposit is obviously not a matter of serious concern for him here s a look\n",
      "\n",
      "Actual Summary:\n",
      "dr k padmarajan the man who contested 170 elections\n",
      "\n",
      "Created Summary:\n",
      "dr k padmarajan favorite dance witnessed contested 170 elections coachella\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi the supreme court on friday asked the gujarat government to complete disciplinary action against erring police officials convicted by the gujarat high court in the 2002 bilkis bano case a bench headed by chief justice <UNK> <UNK> said it will hear on april 23 bano s plea seeking enhanced compensation bano before the bench also comprising justices <UNK> gupta and <UNK> <UNK> refused to accept rs 5 lakh compensation offered by the gujarat government the high court had on may 4 2017 convicted seven people five policemen and two doctors under sections 218 not performing their duties and section 201 tampering of evidence of the indian <UNK> code ipc\n",
      "\n",
      "Actual Summary:\n",
      "bilkis bano case sc asks gujarat govt to complete disciplinary action against erring officials\n",
      "\n",
      "Created Summary:\n",
      "bilkis bano case sc asks gujarat govt of complete disciplinary action action erring officials officials\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "dhaka several people were feared trapped in a 22 storey building which caught fire in <UNK> capital s <UNK> area on thursday according to media reports the incident happened on thursday afternoon in the building that houses several garment shops and internet service providers several people are feared trapped inside the building the fire broke out at <UNK> tower around 12 52 pm on thursday <UNK> rahman an officer of the dhaka fire service central control room was quoted as saying by the dhaka tribune he said that 17 fire tenders have been sent to the spot to douse the blaze seventeen <UNK> units of the dhaka fire service and civil defence are trying to douse the fire he said last month at least 70 people were killed and over 50 injured when a fast moving fire swept through an apartment buildings and chemical warehouses in dhaka s congested <UNK> area\n",
      "\n",
      "Actual Summary:\n",
      "fire breaks out in 22 storey building in bangladesh times of india\n",
      "\n",
      "Created Summary:\n",
      "fire breaks out out 22 storey building\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "greece rosa ferrigno s new suit takes recycling to an extreme she knitted it from more than 300 plastic grocery bags and sewing since her childhood in sicily last summer ferrigno saw someone s purse made from repurposed plastic bags and started her own bag projects making two purses before the suit she says she does it just for fun\n",
      "\n",
      "Actual Summary:\n",
      "<UNK> with a repurpose woman makes suit of plastic bags times of india\n",
      "\n",
      "Created Summary:\n",
      "the s s defense photo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "facebook on wednesday banned praise support and representation of white nationalism and white separatism a move that drew qualified approval from new zealand where a massacre of 50 people in mosques was live streamed earlier this month facebook has <UNK> up its content monitoring teams and taken down event pages that were used to promote and organise rallies by white supremacist groups the policy will be enforced next week it said in a blog post and will apply to both its core facebook app and instagram facebook has long banned white supremacy under its rules on hateful content but did not previously consider white nationalist content to be explicitly racist it would also start connecting people who search for terms associated with white supremacy to an organisation called life after hate\n",
      "\n",
      "Actual Summary:\n",
      "facebook bans praise of white nationalism times of india\n",
      "\n",
      "Created Summary:\n",
      "saudi bans praise of of nationalism times times\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new york a pioneering use of drones to fly blood samples across a north carolina hospital campus was launched on tuesday in the latest move to expand their roles in business and health care the short trips between <UNK> buildings in raleigh mark the first time the federal aviation administration has allowed regular commercial flights of drones carrying products according to ups and drone company <UNK> which partnered with the hospital on the program this is a turning point and it s an historic moment because this is the first faa <UNK> use of a drone for routine revenue <UNK> flights <UNK> ganesh vice president of ups advanced technology group said in an interview before the announcement the faa confirmed in a statement on monday that it hadn t previously allowed drones to make routine commercial package deliveries known as revenue flights others have flown drone deliveries as part of smaller scale tests or demonstrations\n",
      "\n",
      "Actual Summary:\n",
      "this hospital in us is using drones to fly blood samples times of india\n",
      "\n",
      "Created Summary:\n",
      "man hospital arrested driver to using drones king fly blood samples\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "london iranian president hassan rouhani told iraq s prime minister on tuesday that regional countries must unite to fight against u s president donald trump s decision to <UNK> the golan heights as part of israel the excessive demands of the <UNK> regime and the wrong decisions of washington <UNK> closer cooperation among regional countries rouhani was quoted as saying by the state broadcaster <UNK> in a phone call with iraq s adel <UNK> <UNK> rouhani said developments in the golan heights were very dangerous for regional security\n",
      "\n",
      "Actual Summary:\n",
      "iran s president urges regional unity after us move on golan heights times of india\n",
      "\n",
      "Created Summary:\n",
      "iran food star urges regional unity to to move\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "rochester kodak says a new beer hitting the market can be used to develop its super 8 movie film dogfish head craft brewery in delaware created its supereight beer after a conversation with people at kodak the upstate new york technology company most famous for its photographic roots dogfish learned from kodak that heightened levels of acidity and vitamin c in certain beers could make them a processing agent for film that inspired the brewery to design such a beer kodak helped by testing it dogfish founder sam <UNK> says he ll document his summer travels on super 8 film that will be developed in supereight beer and turned into a short film the beer made with <UNK> mango <UNK> kiwi <UNK> and salt is set for national distribution next month\n",
      "\n",
      "Actual Summary:\n",
      "reel y new beer can double as motion picture film developer times of india\n",
      "\n",
      "Created Summary:\n",
      "reel y new beer planet double well motion picture film developer of of india india\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi infosys ltd india s second biggest it services company said on thursday it would buy a 75 per cent stake in abn amro group nv s mortgage administration services unit for 127 5 million euros 143 53 million infosys will acquire the stake in stater nv through unit infosys consulting pvt ltd and the transaction is expected to close in the first quarter of fiscal 2020 this is in line with infosys strategy to strengthen its mortgage servicing capabilities in continental europe the software services company said in a statement abn amro will continue to hold the remaining 25 per cent stake in stater which operates in the netherlands belgium and germany infosys had gained 1 6 per cent by 12 58 pm while the broader mumbai market was up 0 58 per cent\n",
      "\n",
      "Actual Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro s mortgage services arm times of india\n",
      "\n",
      "Created Summary:\n",
      "abn amro infosys to buy 75 stake in abn amro\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the us smart home market was worth over 5 billion in 2018 according to the latest data china was a distant second grossing over a billion dollars in smart home revenue here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic us leads in smart home adoption times of india\n",
      "\n",
      "Created Summary:\n",
      "trump trump\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "patna new delhi tension erupted between congress and rjd on thursday forcing them to cancel a press conference that had been scheduled to declare the names of 31 candidates of the anti bjp front in bihar the two partners who had declared candidates for nine seats locked horns again over darbhanga bettiah and supaul bjp rebel shatrughan sinha became the collateral victim of the flare up the former film star an ls member from patna sahib was expected to join congress on thursday a press meet announcing that had to be cancelled congress later released a picture of sinha and rahul gandhi shaking hands with the information that sinha would join the party on april 6\n",
      "\n",
      "Actual Summary:\n",
      "rjd congress news congress rjd friction delays shatrughan sinha entry\n",
      "\n",
      "Created Summary:\n",
      "rjd congress news congress rjd friction delays shatrughan sinha entry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "<UNK> authorities say they have arrested a russian tourist who was attempting to smuggle a drugged orangutan out of indonesia s resort island of bali <UNK> <UNK> marbawa from bali province s conservation agency says 27 year old <UNK> zhestkov was captured late friday at the airport after an x ray found the 2 year old male orangutan in a <UNK> basket inside his luggage marbawa said sunday that customs officers also found allergy pills two <UNK> and five <UNK> in the man s suitcase all animals the animals were alive he said that zhestkov told authorities he deliberately fed the orangutan allergy pills mixed with milk causing the animal to lose consciousness for up to three hours he told them he planned to re dose the animal during a transit in seoul\n",
      "\n",
      "Actual Summary:\n",
      "russian arrested smuggling drugged orangutan in indonesia times of india\n",
      "\n",
      "Created Summary:\n",
      "hospital arrested smuggling drugged orangutan in indonesia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "the us smart home market was worth over 5 billion in 2018 according to the latest data china was a distant second grossing over a billion dollars in smart home revenue here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic us leads in smart home adoption times of india\n",
      "\n",
      "Created Summary:\n",
      "trump trump\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "nike continues to dominate the global sneaker market in 2018 it registered an annual sales of 22 3 billion what s interesting is that many of the models released in the 1970s by nike are still its most popular shoes today not only are they selling equally well but they are also in many cases better than shoes featuring state of the art technology on the other hand adidas and asics rank second and third respectively here s a look at footwear sales of selected brands\n",
      "\n",
      "Actual Summary:\n",
      "infographic nike is no 1 in the global sneaker market times of india\n",
      "\n",
      "Created Summary:\n",
      "nike nike\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi from a largely congress set up in 1984 to the teeming mosaic of the 90s to the saffron spillover in 2014 the country s mandate has been reflected in the colours of the lok sabha the most diverse lok sabha was 1999 when 41 parties were elected and pm vajpayee headed a multi party coalition government the mosaic got more intricate as coalition politics and minority governments thrived coalescing over the years around alliances led by national parties here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic which was the most diverse ls in the past 3 decades times of india\n",
      "\n",
      "Created Summary:\n",
      "the on on on on on on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "jodhpur the rajasthan high court has directed twitter ceo jack dorsey to file his reply by april 12 in a petition filed by a <UNK> organisation accusing him of hurting the community s sentiments in a tweet last year he has been allowed to file the reply through his counsel a single bench of justice <UNK> kumar <UNK> has adjourned the matter and extended the interim stay on his arrest till april 24 the jodhpur police had registered a case against dorsey on the directions of the high court after <UNK> sharma from the <UNK> foundation filed a plea in november last year accusing him of using a humiliating tone on the community on twitter the tweet was later removed and twitter rendered an apology on the ceo s behalf dorsey s counsel later sought <UNK> of the fir after investigating officer informed the hc of absence of information on the deleted tweet which was posted on november 18 2018\n",
      "\n",
      "Actual Summary:\n",
      "jack dorsey twitter ceo jack dorsey told to file reply for <UNK> tweet times of india\n",
      "\n",
      "Created Summary:\n",
      "jack dorsey twitter ceo jack dorsey told to file reply corps\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "mumbai a protection of children from sexual offences pocso act court on thursday convicted and sentenced a now 75 year old man to 10 years imprisonment for sexually assaulting his six year old neighbour in 2016 the prosecution stated that on april 12 2016 he took the child to his house and sexually assaulted her when the child s mother found her crying in the house of the accused she confided in her parents an fir was registered after which the magistrate recorded the child s statement medical evidence has supported the child s testimony the accused has been in jail since his arrest and his repeated bail pleas have been rejected\n",
      "\n",
      "Actual Summary:\n",
      "75 year old gets 10 years jail for raping six year old\n",
      "\n",
      "Created Summary:\n",
      "75 year old old 10 year jail moment raping six year year\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi from a largely congress set up in 1984 to the teeming mosaic of the 90s to the saffron spillover in 2014 the country s mandate has been reflected in the colours of the lok sabha the most diverse lok sabha was 1999 when 41 parties were elected and pm vajpayee headed a multi party coalition government the mosaic got more intricate as coalition politics and minority governments thrived coalescing over the years around alliances led by national parties here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic which was the most diverse ls in the past 3 decades times of india\n",
      "\n",
      "Created Summary:\n",
      "the on on on on on on\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "nike continues to dominate the global sneaker market in 2018 it registered an annual sales of 22 3 billion what s interesting is that many of the models released in the 1970s by nike are still its most popular shoes today not only are they selling equally well but they are also in many cases better than shoes featuring state of the art technology on the other hand adidas and asics rank second and third respectively here s a look at footwear sales of selected brands\n",
      "\n",
      "Actual Summary:\n",
      "infographic nike is no 1 in the global sneaker market times of india\n",
      "\n",
      "Created Summary:\n",
      "nike nike\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "patna rjd chief lalu prasad s elder son tej pratap who had otherwise been keeping silent since the declaration of the lok sabha election dates dropped two bombshells on thursday by resigning as chief of the party s student wing and seeking tickets for his candidates of choice for jehanabad and sheohar seats i am resigning as chief of chhatra rjd naadan hain woh log jo mujhe naadan samajhte hain those who think i am naive are mistaken he tweeted earlier in the day tej said that he had apprised his younger brother tejashwi of his two choices he refuted reports that he would form his own party tej said his picks angesh singh sheohar and chandra prakash jehanabad were dedicated to rjd rjd sources however said the party was likely to field surendra yadav from jehanabad\n",
      "\n",
      "Actual Summary:\n",
      "tej pratap yadav bihar demanding tickets for 2 aides tej pratap quits party post\n",
      "\n",
      "Created Summary:\n",
      "tej pratap yadav bihar demanding tickets for for aides tej pratap quits\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "noida officials in lucknow said that uttar pradesh chief minister yogi adityanath has directed <UNK> <UNK> nagar police and administration chiefs to <UNK> assistance to the victims the bus was on its way to noida from <UNK> the incident took place under <UNK> police station limits around 5 am the bus belongs to <UNK> depot eight people were killed and 20 injured in the incident police said in a statement police personnel were present at the site of the accident and the victims were rushed to the nearby <UNK> hospital in <UNK> it said an official spokesperson in lucknow said the chief minister has taken <UNK> of the bus accident in greater noida where eight passengers have died he has directed the district administration and the police to provide all assistance the district magistrate and the senior superintendent of police are reaching the accident site the official said\n",
      "\n",
      "Actual Summary:\n",
      "<UNK> expressway accident 8 killed several injured in greater noida\n",
      "\n",
      "Created Summary:\n",
      "car expressway accident 8 8 several injured killings greater noida\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi the congress late thursday night issued a list of 31 candidates for the lok sabha polls fielding rajasthan chief minister ashok gehlot s son vaibhav from jodhpur and former bjp leader jaswant singh s son manvendra from barmer the party s latest list has 19 candidate for rajasthan and six each for uttar pradesh and gujarat with this the number of total candidates announced by the party for the lok sabha polls has reached 293 in rajasthan the party has fielded forer union minister jitendra singh from alwar senior leaders ramnarayan meena from kota savita meena from dausa st jyoti khandelwal from jaipur namo narayan meena from tonk sawai madhopur and jyoti mirdha from nagaur among others in gujarat the party announced jagdish thakor from patan and lalit kagathara from rajkot among others in uttar pradesh it fielded major jp singh retd from sambhal and niaz ahmed from deoria and pankaj niranjan from phulpur\n",
      "\n",
      "Actual Summary:\n",
      "congress gives ticket to ashok gehlot s son fields jaswant singh s son from barmer\n",
      "\n",
      "Created Summary:\n",
      "congress gives ticket to ashok gehlot s s fields jaswant singh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi indian institute of management rohtak organised its 8th convocation during which a number of students were awarded a statement said on sunday it said <UNK> students from pgp and <UNK> <UNK> were <UNK> on saturday union public service commission upsc chairman <UNK> <UNK> was the chief guest at the event addressing a gathering iim rohtak director <UNK> sharma said it was an achievement that all students were placed well in the industry as this was the 7th largest pgp batch among the 20 <UNK> there has been a four times increase in the female students inducted to the institute this year with the best gender ratio a statement quoted sharma as saying rank holders <UNK> jain and <UNK> malik were awarded silver and bronze medals respectively <UNK> <UNK> was awarded a gold medal for being the all <UNK> the upsc chairman congratulated the institute and the students\n",
      "\n",
      "Actual Summary:\n",
      "iim rohtak organises 8th convocation times of india\n",
      "\n",
      "Created Summary:\n",
      "iim rohtak organises 8th convocation times times\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi british motorcycle manufacturer triumph motorcycles ltd has announced a recall of certain models affecting around 1 000 motorcycles in india affected model include headstock tidy street scrambler street twin bonneville t100 and bonneville t120 changes and upgrades in indicator main beam malfunction and a possible loss of engine power caused by the clutch cable contacting the main harness resulting in damage to the wiring within the main harness will be carried out any concerned customers are asked in the first instance to contact their local triumph dealer as soon as possible who will be able to carry out all necessary work as required at no cost to the owner the company said in a statement\n",
      "\n",
      "Actual Summary:\n",
      "triumph motorcycles ltd triumph recalls 1 000 motorcycles in india for minor upgrades times of india\n",
      "\n",
      "Created Summary:\n",
      "triumph motorcycles ltd triumph recalls 1 000 motorcycles\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in several regions women are putting in a large number of hours at work whether it is paid or unpaid according to latest data the total time spent working paid or unpaid is higher for women than men in latin america for instance women work a total of 8 3 hours a day vis a vis 7 7 hours by men here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic from latin america to north africa women work more hours than men times of india\n",
      "\n",
      "Created Summary:\n",
      "the the sensors grills\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in several regions women are putting in a large number of hours at work whether it is paid or unpaid according to latest data the total time spent working paid or unpaid is higher for women than men in latin america for instance women work a total of 8 3 hours a day vis a vis 7 7 hours by men here s a look\n",
      "\n",
      "Actual Summary:\n",
      "infographic from latin america to north africa women work more hours than men times of india\n",
      "\n",
      "Created Summary:\n",
      "the the sensors grills\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "new delhi congress president rahul gandhi on friday said his party if voted to power will scrap the niti aayog and replace it with a lean planning commission it niti aayog has served no purpose other than making marketing presentations for the pm <UNK> data rahul said we will replace it with a lean planning commission whose members will be renowned economists experts with less than 100 staff the congress chief said on twitter if voted to power we will scrap the niti aayog it has served no purpose other than making marketing presentations https t co <UNK> rahul gandhi <UNK> <UNK> the planning commission set up in 1950 under the congress government was replaced by the niti aayog by the modi <UNK> the congress is seeking to revert to the original structure of the government think tank in video will scrap niti aayog if congress voted to power rahul gandhi\n",
      "\n",
      "Actual Summary:\n",
      "congress will scrap niti aayog if voted to power rahul gandhi times of india\n",
      "\n",
      "Created Summary:\n",
      "congress will scrap niti aayog if voted times power rahul gandhi\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "alipurduar bjp president amit shah on friday said that his party will replicate the national register of citizens exercise in west bengal if it comes to power at the centre we will bring in nrc in bengal and throw out all <UNK> we will ensure that hindu refugees are not touched shah said at bjp s first lok sabha campaign in bengal in <UNK> dominated alipurduar the ruling trinamool congress was quick to <UNK> bjp shah s claim we will never allow any nrc exercise in bengal they bjp want to divide the people on religious and communal lines trinamool secretary general partha chatterjee said in kolkata\n",
      "\n",
      "Actual Summary:\n",
      "will bring in nrc in bengal if voted to power amit shah times of india\n",
      "\n",
      "Created Summary:\n",
      "harley bring for nrc attack bengal to to to power amit amit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "a day after <UNK> that she was not <UNK> to contesting against pm narendra modi in the polls congress general secretary priyanka gandhi vadra attacked him for ignoring his constituency her remark stoked speculation that she might fight from varanasi i was told that in the past five years he modi has not visited even a single village in his own constituency she said on friday\n",
      "\n",
      "Actual Summary:\n",
      "priyanka gandhi s remark stokes speculation times of india\n",
      "\n",
      "Created Summary:\n",
      "the s s trillion harder to to\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "ayodhya congress general secretary priyanka gandhi vadra on friday skipped a visit to the disputed site in ayodhya where the idol of ram lalla is kept even as she kept her tryst with the <UNK> temple when asked why she had not visited the makeshift shrine she said ab sub judice <UNK> <UNK> because the matter is sub judice to a query on the bjp charge that her visit would be incomplete if she didn t visit the place she said the matter is in court and it won t be correct to comment on her maiden visit to ayodhya priyanka attacked pm narendra modi saying he went globetrotting but never visited a village in his constituency varanasi in his five years in power\n",
      "\n",
      "Actual Summary:\n",
      "sub judice says priyanka gandhi <UNK> ram lalla temple times of india\n",
      "\n",
      "Created Summary:\n",
      "sub judice says priyanka gandhi during ram lalla temple times times\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Actual Text:\n",
      "in a significant political <UNK> in up ahead of lok sabha elections nishad party chief <UNK> nishad on friday snapped ties with the alliance led by sp bsp declaring that he will either go alone or with any other party and hours later met chief minister yogi adityanath nishad party had created <UNK> last year after nishad s son praveen ended yogi s <UNK> 20 year reign over <UNK> in a <UNK> tnn\n",
      "\n",
      "Actual Summary:\n",
      "nishad party quits sp bsp front in up times of india\n",
      "\n",
      "Created Summary:\n",
      "the the of of the the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "summarizer_model_utils.sample_results(preds,\n",
    "                                      ind2word,\n",
    "                                      word2ind,\n",
    "                                      converted_summaries[:50],\n",
    "                                      converted_texts[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
